%This is for amsart
%\documentclass[12pt]{amsart}

%This is for article   
\documentclass[12pt, leqno]{article}
\newenvironment{proof}{\noindent {\bf Proof:}}{$\Box$ \vspace{2 ex}}

\usepackage{amsmath, amssymb, amscd, hyperref}
\usepackage{latexsym, epsfig, color, url, relsize, soul}
\usepackage[all]{xy}
\usepackage{tikz, verbatim, xcolor, skull, sgame}\renewcommand\gamestretch{2}
\usetikzlibrary{calc,matrix}

\usepackage{ifthen,environ,etoolbox}

% not actually using this one
\newcounter{raeindex}
\setcounter{raeindex}{0}
\makeatletter
\NewEnviron{rae}{%
  \addtocounter{raeindex}{1}%
  \expandafter\xdef\csname rae\romannumeral\value{raeindex}\endcsname{%
  \expandonce{\BODY}}%
  \BODY%
}
\makeatother
\newcounter{index}
\newcommand\showraes{%
  \vspace{5ex}%
  \setcounter{index}{0}%
  \whiledo{\value{index} < \value{raeindex}}{%
    \addtocounter{index}{1}%
    \arabic{index}.  \csname rae\romannumeral\value{index}\endcsname \vskip 0.01in %
  }%
}
%\usepackage{fullpage}    %HELLO!!!!

\newcounter{repeatatend}
\makeatletter
\def\repeat@at@end{} % initialize

\NewEnviron{important}{%
  \par
  \medskip
  \BODY\par
  \medskip
  \refstepcounter{repeatatend}%
  \label{repeatatend@\romannumeral\value{repeatatend}}%
  \xdef\repeat@at@end{%
    \unexpanded\expandafter{\repeat@at@end}%
    \unexpanded{\noindent(p.~\pageref}{repeatatend@\romannumeral\value{repeatatend}}) \ \ %
    \unexpanded\expandafter{\BODY\par}%
    \vskip 0.2in
  }%
}
\AtEndDocument{\section{Review of Games, Links, and Principles}\repeat@at@end}
%Game Descriptions, Video Links, and Important Principles



\setlength{\topmargin}{-0.2in}
\setlength{\oddsidemargin}{-0.1in}
\setlength{\evensidemargin}{-0.1in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.6in}

\newcommand*\mycirc[1]{%  
  \begin{tikzpicture}
      \node[draw,circle,inner sep=1pt] {#1};
   \end{tikzpicture}}
   

\newcommand{\calP}{\mathcal{P}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{{\rm{N}}}
\newcommand{\ns}{{\rm{ns}}}
\newcommand{\PGL}{{\rm{PGL}}}
\newcommand{\Tr}{{\rm{Tr}}}
\newcommand{\chr}{{\rm{char}}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\co}{\mathcal{O}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\nz}{+}
\newcommand{\QQ}{\mathcal{Q}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\GQ}{G_{\Q}}
\newcommand{\Qbar}{\bar{\Q}}
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\wP}{\widehat{\Phi}}
\newcommand{\charfn}{\mathbf{1}}
\newcommand{\sillyX}{\mathfrak{X}}
\newcommand{\sumf}{\sideset{}{{}^\flat}\sum}
\newcommand{\soln}{\medskip {\bf Solution. }}




\newcommand{\six}[6]{ #1 #2 #3 #4 #5 #6}
\newcommand{\tw}[2]{#1 #2}
%\newcommand{\tw}[2]{ \begin{matrix} #1 \\ #2 \end{matrix}}

\newcommand\Hom{\operatorname{Hom}}
\newcommand\Aut{\operatorname{Aut}}
\newcommand\Out{\operatorname{Out}}
\newcommand\Vol{\operatorname{Vol}}
\newcommand\Disc{\operatorname{Disc}}
\newcommand\disc{\operatorname{Disc}}
\newcommand\im{\operatorname{im}}
\newcommand\Stab{\operatorname{Stab}}
\newcommand\Gal{\operatorname{Gal}}
\newcommand\Res{\operatorname{Res}}
\newcommand\SL{\operatorname{SL}}
\newcommand\GL{\operatorname{GL}}
\newcommand\gl{\operatorname{GL}}
\newcommand\Dist{\operatorname{Dist}}
\newcommand\dist{\operatorname{dist}}
\newcommand\lcm{\operatorname{lcm}}
\newcommand\fc{\operatorname{fc}}
\newcommand\calI{\mathcal{I}}

\DeclareMathOperator{\aut}{Aut}
\DeclareMathOperator{\Sp}{Sp}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\pf}{Pfaff}
\DeclareMathOperator{\pgl}{PGL}
\DeclareMathOperator{\sym}{Sym}
\DeclareMathOperator{\spl}{SL}
\DeclareMathOperator{\codim}{codim}

\definecolor{dgreen}{RGB}{0, 170, 0}

\newcommand{\blue}[1]{{\leavevmode\color{blue}{#1}}}
\newcommand{\green}[1]{{\leavevmode\color{dgreen}{#1}}}
\newcommand{\red}[1]{{\leavevmode\color{red}{#1}}}
\newcommand{\purple}[1]{{\leavevmode\color{purple}{#1}}}
\newcommand{\orange}[1]{{\leavevmode\color{orange}{#1}}}
\newcommand{\ttblue}[1]{{\leavevmode\color{blue}{#1}}}
\newcommand{\ttgreen}[1]{{\leavevmode\color{dgreen}{#1}}}
\newcommand{\ttcmt}[1]{{\leavevmode\color{magenta}{#1}}}

% Comment the first and uncomment the second to make homework solutions disappear.
\newcommand{\hwsol}[1]{ \medskip \\ {\bf Solution.} {#1}}
%\newcommand{\hwsol}[1]{}

\newcommand{\pb}[6]
{		\left[
			\begin{smallmatrix}%
				#1&#2&#3\\%
				#4&#5&#6\\%
			\end{smallmatrix}%
		\right]%
}

% A newcommand \pt{}{}{}{}{}{}{}{}{}{}{}{}, for elements of sym^2(3)\otimes 2
\def\pt#1#2#3#4#5#6#7#8#9{%
	\def\ArgsTenAndFurther##1##2##3{%
		\left[
			\begin{smallmatrix}%
				#1&#2&#3&#4&#5&#6\\%
				#7&#8&#9&##1&##2&##3%
			\end{smallmatrix}%
		\right]%
	}%
	\ArgsTenAndFurther%
}



%\def\NineteenArgs#1#2#3#4#5#6#7#8#9{%
%    \def\ArgsTenAndFurther##1##2##3##4##5##6##7##8##9{%
%        \def\ArgNineteen####1{%
%            ####1##9##8##7##6##5##4##3##2##1\\#9#8#7#6#5#4#3#2#1%
%        }%
%        \ArgNineteen%
%    }%
%    \ArgsTenAndFurther%
%}

\newcommand{\twtw}[4]{[\begin{smallmatrix}#1&#2\\#3&#4\\\end{smallmatrix}]}


% Double
\newcommand{\odr}{\co_{D1^2}}		% Ramify
\newcommand{\ods}{\co_{D11}}		% Split
\newcommand{\odi}{\co_{D2}}		% Inert
\newcommand{\odg}{\co_{D\rm{ns}}}	% Generic
% Common component
\newcommand{\ocs}{\co_{C\rm{s}}}	% Singular
\newcommand{\ocg}{\co_{C\rm{ns}}}	% Generic
% essentially in Two variables
\newcommand{\ots}{\co_{T11}}		% Split
\newcommand{\oti}{\co_{T2}}		% Inert
% subscheme
\newcommand{\ssc}{\mathfrak X}
\newcommand{\urlc}[1]{\begin{center} \url{#1} \end{center}}

\newcommand{\newgame}[2]{ \begin{important} \smallskip {\bf Game Description} (#1): #2 \smallskip  \end{important} }
\newcommand{\sbs}[2]{W_{[#1,#2]}}
\newcommand{\sby}[2]{Y_{[#1,#2]}}

\newcommand{\sump}{\sideset{}{'}\sum}

\newcommand{\hs}\heartsuit
\newcommand{\cs}\clubsuit
\renewcommand{\ss}\spadesuit
\newcommand{\ds}\diamondsuit
\newcommand\ldmod{q}
\newcommand\tor{\textnormal{ or }}
\newcommand\tand{\textnormal{ and }}


%\newcommand\beq{\begin{equation}}
%\newcommand\eeq{\end{equation}}

\newcommand\cF{\mathcal{F}}
\newcommand\Sym{\operatorname{Sym}}
\newcommand\tra{\mathsmaller T}
%\newcommand{\tw}[12]{ \begin{matrix} #1 #2 #3 #4 #5 #6 \\ #7 #8 #9 #10 #11 #12}
%This numbers everything by section
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{example}[proposition]{Example}
\newtheorem{question}[proposition]{Question}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{defn}[proposition]{Definition}
\newtheorem{exm}[proposition]{Example}
\newtheorem{assmp}{Assumption}
\newtheorem{aim}{Aim}


%This will keep definitions and notations unnumbered and non-italicized.
%\newenvironment{definition}{\vspace{2 ex}{\noindent{\bf Definition. }}}{\vspace{2 ex}}
%\newenvironment{notation}{\vspace{2 ex}{\noindent{\bf Notation. }}}{\vspace{2 ex}}

%Use this to number and italicize definitions and notations, but need to cut out above environments.
\newtheorem{definition}[proposition]{Definition}
\newtheorem{notation}[proposition]{Notation}

\title{The Mathematics of Game Shows}
     
\author{Frank Thorne}
       
\begin{document}

\maketitle
{\itshape Note: the table of contents is clickable!}

\tableofcontents

\section{Introduction}

We will begin by watching a few game show clips and seeing a little bit of the math behind them.

\medskip
{\bf Example: The Price Is Right, Contestants' Row}
\medskip

\begin{important}
We begin with the following clip from The Price Is Right: \urlc{https://www.youtube.com/watch?v=TmKP1a03E2g}
\end{important}

\newgame{Contestants' Row - The Price Is Right}{Four contestants are shown an item up for bid. In order, each guesses its price (in whole dollars).
You can't use a guess that a previous contestant used. The winner is the contestant who bids the closest to the actual price without going over.}

In this clip, the contestants are shown some scuba equipment, and they bid
750, 875, 500, and 900 in that order. The actual price is \$994, and the fourth contestant wins. What can
we say about the contestants' strategy?

\begin{itemize}
\item
As a first step, it is useful to precisely describe the results of the bidding: the first contestant wins if the price is in
$[750, 874]$\footnote{Recall that $[a, b]$ is mathematical notation for all the numbers between $a$ and $b$.}; 
the second, if the price is in $[875, 899]$; the third, in $[500, 749]$; the fourth, in $[900, \infty)$.
If the price is less than \$500, then all the bids are cleared and the contestants start over.

We can see who did well before we learn how much the scuba gear costs. Clearly, the fourth contestant
did well. If the gear is worth anything more than \$900 (which is plausible), then she wins. The third contestant
also did well: he is left with a large range of winning prices -- 250 of them to be precise. The second contestant
didn't fare well at all: although his bid was close to the actual price, he is left with a very small winning range.
This is not his fault: it is a big disadvantage to go early.

\item
The next question to ask is: could any of the contestants have done better?

We begin with the fourth contestant. Here the answer is {\itshape yes}, and her strategy is {\bf dominated}
by a bid of \$876, which would win in the price range $[876, \infty)$. In other words: {\itshape a bid of \$876 would
win every time a bid of \$900 would, but not vice versa}. Therefore it is better to instead bid \$876 if she believes the
scuba gear is more than \$900.

Taking this analysis further, we see that there are exactly four bids that make sense: 876, 751, 501, or 1. Note
that each of these bids, except for the one-dollar bid, screws over one of her competitors, and this is not an accident:
Contestant's Row is a {\bf zero-sum game} -- if someone else wins, you lose. If you win, everyone else loses. 

\item
The analysis gets much more subtle if we look at the {\itshape third} contestant's options. {\bf Assume that the fourth
contestant will play optimally.} (Of course this assumption is very often not true in practice.

Suppose, for example, that the third contestant believes 
that the scuba gear costs around \$1000. The previous bids were \$750 and \$875. Should he follow the same reasoning and
bid \$876? Maybe, but this exposes him to a devastating bid of \$877. 

\end{itemize}
There is much more to say here, but we go on to a different example.

\medskip
{\bf Example: Deal or No Deal}
\medskip

\begin{important}
Here is a clip of the game show {\bf Deal or No Deal}:
\urlc{https://www.youtube.com/watch?v=I3BzYiCSTo8} The action starts around 4:00.

{\itshape Note: Unfortunately, as of Septermber 22, the link has been disabled by YouTube. These game show clips are copyrighted material
and it is the copyright holders' prerogative to ask YouTube to remove them. For the same reason I cannot copy them; I can only link to YouTube or other 
external sites.

If you encounter a disabled link, you might search for game's title on YouTube or on the Internet more generally, to find other clips of
different contestants playing the same game.}
\end{important}

\newgame{Deal or No Deal}{There are 26 briefcases, each of which contains
a variable amount of money from \$0.01 to \$1,000,000, totalling \$3,418,416.01,
and averaging \$131477.53. The highest prizes are \$500,000, \$750,000, and \$1,000,000.

The contestant chooses one briefcase and keeps it. Then, one at a time, the contestant chooses other briefcases
to open, and sees how much money is in each (and therefore establishes that these are not the prizes in his/her
own briefcase). Periodically, the `bank' offers to buy the contestant out, and give him/her a fixed amount of money
to quit playing. The contestant either accepts or says `no deal' and continues playing.
}

The {\bf expected value} of the game is the average amount of money you expect to win. (We'll have much more to say about this.)
So, at the beginning, the expected value of the game is \$131477.53, presuming the contestant rejects all the deals.
In theory, that means that the contestant should be equally happy to play the game or to receive \$131477.53. (Of course,
this might not be true in practice.)

Now let's look at the game after he chooses six briefcases. The twenty remaining contain a total of \$2936366, or an average
of \$146818. The expected value has gone up, because the contestant eliminated mostly small prizes and none of the three biggest.
If he wants to maximize his expected value (and I repeat that this won't necessarily be the case), then all he has to know is that
$$
146818 > 51000
$$
and so he keeps playing.

The show keeps going like this. After five more cases are eliminated, he again gets lucky and is left with fifteen cases containing a total
of \$2808416, so an average of \$187227. The bank's offer is
\$125,000 which he refuses. And it keeps going.

\medskip
{\bf Example: Jeopardy -- Final Jeopardy}
\medskip

\begin{important}
Here we see the Final Jeopardy round of the popular show Jeopardy:
\urlc{https://www.youtube.com/watch?v=DAsWPOuF4Fk}
\end{important}

\newgame{Jeopardy, Final Round}{
Three contestants start with a variable amoung of money (which they earned in the previous two rounds).
They are shown a category, and are asked how much they wish to wager on the final round. The contestants
make their wagers privately and independently.

After they make their wagers, the contestants are asked a trivia question. Anyone answering correctly gains
the amount of their wager; anyone answering incorrectly loses it.}

Perhaps here an English class would be more useful than a math class! This game is difficult to analyze;
unlike our two previous examples, the players play {\itshape simultaneously} rather than {\itshape sequentially}.

\medskip
In this clip, the contestants start off with \$9,400, \$23,000, and \$11,200 respectively. It transpires that nobody
knew who said that {\itshape the funeral baked meats did coldly furnish forth the marriage tables}. (Richard II?
Really? When in doubt, guess Hamlet.) The contestants big respectively \$1801, \$215, and \$7601.

We will save a thorough analysis for later, but we will make one note now:
the second contestant can obviously win. If his bid is less than \$600, he will end up with more than
\$22,400.

\section{Probability}

\subsection{Sample Spaces and Events}

At the foundation of any discussion of game show strategies is a discussion of {\itshape probability}. You have already
seen this informally, and we will work with this notion somewhat more formally.

\begin{important}
\begin{definition} \medskip
\begin{enumerate}
\item
A {\bf sample space} is the set of all possible outcomes of a some process.
\item
An {\bf event} is any subset of the sample space.
\end{enumerate}

\end{definition}
\end{important}

\begin{example}
You roll a die. The sample space consists of all numbers between one and six.

Using formal mathematical notation, we can write
\[
S = \{ 1, 2, 3, 4, 5, 6 \}.
\]
We can use the notation $\{ \dots \}$ to describe a set and we simply list the elements in it.

Let $E$ be the {\itshape event} that you roll an even number. Then we can write
\[
E = \{ 2, 4, 6 \}.
\]
Alternatively, we can write
\[
E = \{ x \in S \ : \ x \ \textnormal{is even} \}.
\]
Both of these are correct.
\end{example}

\begin{example} You choose at random a card from a poker deck. The sample space is the set of all 52 cards in the deck.
We could write it
\begin{multline*}
S = \{ A\cs, K\cs, Q\cs, J\cs, 10\cs, 9\cs, 8\cs, 7\cs, 6\cs, 5\cs, 4\cs, 3\cs, 2\cs, \\
A\ds, K\ds, Q\ds, J\ds, 10\ds, 9\ds, 8\ds, 7\ds, 6\ds, 5\ds, 4\ds, 3\ds, 2\ds, \\
A\hs, K\hs, Q\hs, J\hs, 10\hs, 9\hs, 8\hs, 7\hs, 6\hs, 5\hs, 4\hs, 3\hs, 2\hs, \\
A\ss, K\ss, Q\ss, J\ss, 10\ss, 9\ss, 8\ss, 7\ss, 6\ss, 5\ss, 4\ss, 3\ss, 2\ss \}
\end{multline*}
but writing all of that out is annoying. An English description is probably better.
\end{example}

\begin{example} You choose two cards at random from a poker deck. Then the sample space is the set of all pairs 
of cards in the deck. For example, $A\ss A\hs$ and $7\cs 2\ds$ are elements of this sample space,

This is definitely too long to write out every element, so here an English description is probably better.
(There are exactly 1,326 elements in this sample space.) Some events are easier to describe -- for example,
the event that you get a pair of aces can be written
\[
E = \{ A \ss A \hs, A \ss A \ds, A \ss A \cs, A \hs A \ds, A \hs A \cs, A \cs A \ds \}
\]
and has six elements. If you are playing Texas Hold'em, your odds of being dealt a pair of aces
is exactly $\frac{6}{1326} = \frac{1}{221}$, or a little under half a percent.

\end{example}

\begin{important}
The following TPIR clip illustrates a playing of {\bf Squeeze Play}.

\urlc{https://www.youtube.com/watch?v=TR7Smevj1AQ}

\newgame{Squeeze Play (The Price Is Right)}{
You are shown a prize, and a five- or six-digit number. The price of the prize is this number with one of the digits
removed, other than the first or the last. 

The contestant is asked to remove one digit. If the remaining number is the price, the contestant wins the prize.
}
\end{important}

In this clip the contestant is shown the number 114032. Can we describe the game in terms of a sample
space?

It is imporrtant to recognize that {\bf this question is not precisely defined. Your answer will depend on your interpretation
of the question!} This is probably very much {\itshape not} what you are used to from a math class.

Here's one possible interpretation. Either the contestant wins or loses, so we can describe the sample space as
\[
S = \{ \textnormal{you win}, \textnormal{you lose} \}.
\]
Logically there is nothing wrong with this. But it doesn't tell us very much about the structure of the game, does it?

Here is an answer I like better. We write
\[
S = \{ 14032, 11032, 11432, 11402 \},
\]
where we've written 14032 as shorthand for `the price of the prize is 14032'. 

Another correct answer is 
\[
S = \{ 2, 3, 4, 5 \},
\]
where here $2$ is shorthand for `the price of the prize has the second digit removed.'

Still another correct answer is
\[
S = \{ 1, 4, 0, 3 \},
\]
where here $1$ is shorthand for `the price of the prize has the $1$ removed.'

All of these answers make sense, and all of them require an accompanying explanation to understand what they mean.

\medskip

The contestant chooses to have the $0$ removed. So the event that the contestant wins can be described as
$E = \{ 11432 \}$, $E = \{4\}$, or $E = \{ 0 \}$, depending on which way you wrote the sample space. (Don't mix and match!
Once you choose how to write your sample space, you need to describe your events in the same way.) If all the possibilities
are equally likely, the contestant has a one in four chance of winning.

The contest guesses correctly and is on his way to Patagonia! 

\begin{notation}
If $S$ is any set (for example a sample space or an event), write $N(S)$ for the number of elements in it.
In this course we will always assume this number is {\itshape finite}.
\end{notation}

\begin{important}
{\bf Probability Rule: All Outcomes are Equally Likely}. Suppose $S$ is a sample space in which all outcomes
are equally likely, and $E$ is an event in $S$. Then the {\bf probability of $E$, denoted $P(E)$}, is 
\[
P(E) = \frac{ N(E) }{ N(S) }.
\]
\end{important}

\begin{example}
You roll a die, so $S = \{ 1, 2, 3, 4, 5, 6 \}$.
\begin{enumerate}
\item
Let $E$ be the event that you roll a $4$, i.e., $E = \{ 4 \}$. Then $P(E) = \frac{1}{6}$.
\item
Let $E$ be the event that you roll an odd number, i.e., $E = \{ 1, 3, 5 \}$. Then $P(E) = \frac{3}{6} = \frac{1}{2}$.
\end{enumerate}
\end{example}

\begin{example}
You draw one card from a deck, with $S$ as before.
\begin{enumerate}
\item
Let $E$ be the event that you draw a spade. Then $N(E) = 13$ and $P(E) = \frac{13}{52} = \frac{1}{4}$.
\item
Let $E$ be the event that you draw an ace. Then $N(E) = 4$ and $P(E) = \frac{4}{52} = \frac{1}{13}$.
\item
Let $E$ be the event that you draw an ace or a spade. What is $N(E)$? There are thirteen spades in the deck,
and there are three aces which are not spades. Don't double count the ace of spades!

So $N(E) = 16$ and $P(E) = \frac{16}{52} = \frac{4}{13}$.
\end{enumerate}
\end{example}

\begin{example}
In a game of Texas Hold'em, you are dealt two cards at random in first position. You decide to raise with
a pair of sixes or higher, ace-king, or ace-queen, and to fold otherwise.

The sample space has 1326 elements in it. The event of two-card hands which you are willing to raise
has $86$ elements in it. (If you like, write them all out. Later we will discuss how this number can be computed more
efficiently!)

Since all two card hands are equally likely, the probability that you raise is $\frac{86}{1326}$, or around one in fifteen.
\end{example}

Now, here is an important example: You roll two dice and sum the totals. What is the probability that you roll a 7?

The result can be anywhere from $2$ to $12$, so we have
\[
S = \{ 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 \}
\]
and $E = \{ 7 \}$. $\skull \skull \skull$: \red{Therefore, $P(E) = \frac{N(E)}{N(S)} = \frac{1}{11}$.}

\medskip
Here is another solution. We can roll anything from $1$ to $6$ on the first die, and the same for the second die,
so we have
\begin{multline*}
S = \{ 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 31, 32, 33, 34, 35, 36, \\ 
41, 42, 43, 44, 45, 46, 
51, 52, 53, 54, 55, 56, 61, 62, 63, 64, 65, 66 \}.
\end{multline*}

We list all the possibilities that add to $7$:
\[
E = \{ 16, 25, 34, 43, 52, 61 \}
\]
And so $P(E) = \frac{6}{36} = \frac{1}{6}$.

\medskip
$\skull \skull \skull$ \red{We solved this problem two different ways and got two different answers}. {\bf The point is that not every 
outcome in a sample space will be equally likely}. We know that a die (if it is equally weighted) is equally likely to come up $1$, $2$,
$3$, $4$, $5$, or $6$. So we can see that, according to our second interpretation, all the possibilities are still equally likely because
all combinations are explicitly listed. But there is no reason why all the sums should be equally likely.

Note that it is often true that all outcomes are {\itshape approximately} equally likely, and we {\itshape model} this scenario by
assuming that they are. {\itshape If our assumptions are close to the truth, so is our answer.}

For example, consider the trip to Patagonia. If we assume that all outcomes are equally likely, the contestant's guess has a $1$ in $4$ chance
of winning. But the contestant correctly guessed that over \$14,000 was implausibly expensive, and around \$11,000 was more reasonable.

\begin{important}
The next example comes from the TPIR game {\bf Rat Race}:

\urlc{https://www.youtube.com/watch?v=Kp8rhV5PUMw}
\end{important}

\newgame{Rat Race (The Price Is Right)}{
The game is played for three prizes: a small prize, a medium prize, and a car.

There is a track with five wind-up rats (pink, yellow, blue, orange, and green). The contestant attempts to price
three small items, and chooses one rat for each successful attempt. The rats then race. If she picked the third
place rat, she wins the small prize; if she picked the second place rat, she wins the medium prize; if he
picked the first place rat, she wins the car.

(Note that it is possible to win two or even all three prizes.)
}

Note that except for knowing the prices of the small items, there is no strategy. The rats are (we presume) equally likely to finish in any order.

In this example, the contestant correctly prices two of the items and picks the pink and orange rats.

\medskip
{\bf Problem 1.} {\itshape Compute the probability that she wins the car.}

\smallskip

Here's the painful solution: describe all possible orderings in which the rats could finish. We can describe the sample space as
\[
S = \{ POB, POR, POG, PBR, PBG, PRG, \dots, \dots \}
\]
where the letters indicate the ordering of the first three rats to finish. Any such ordering is equally likely.
The sample space has sixty elements, and twenty-four
of them start with P or G. So the probability is $\frac{24}{60} = \frac{2}{5}$. 

\smallskip
Do you see the easier solution? To answer the problem we were asked, we only care about the {\bf first}
rat. So let's ignore the second and third finishers, and write the sample space as
\[
S = \{ P, O, B, R, G \}.
\]
The event that she wins is
\[
E = \{ P, G \},
\]
and so $P(E) = \frac{N(E)}{N(S)} = \frac{2}{5}$.

\medskip

Here's a possible solution that was suggested in class. It doesn't work, and it's very instructive to think about why it doesn't work.
As the sample space, take all combinations of one rat and which order it finishes in:
\red{
\begin{align*}
S = \{ & \textnormal{Pink rat finishes first}, \\ 
& \textnormal{Pink rat finishes second}, \\
& \textnormal{Pink rat finishes third}, \\
& \textnormal{Pink rat finishes fourth}, \\
& \textnormal{Pink rat finishes fifth}, \\
& \textnormal{Yellow rat finishes first}, \\
& \textnormal{etc.} \}
\end{align*}
}
This sample space indeed lists a lot of different things that could happen. But how would you describe the event
that the contestant wins? If the pink or orange rat finishes first, certainly she wins. But what if the yellow rat finishes third?
Then maybe she wins, maybe she loses. There are several problems with this sample space:
\begin{itemize}
\item
The events are not mutually exclusive. It can happen that {\bf both} the pink rat finishes second, {\bf and} the yellow rat finishes
first. A sample space should be described so that {\bf exactly one of the outcomes will occur}.

Of course, a meteor could strike the television studio, and Drew, the contestant, the audience, and all five rats could explode
in a giant fireball. But we're building {\itshape mathematical models} here, and so we can afford to ignore remote possibilities
like this.

\item
In addition, you can't describe the event `the contestant wins' as a subset of the sample space. What if the pink rat
finishes fifth? The contestant also has the orange rat. It is ambigious whether this possibility should be part of the event or not.
\end{itemize}

Altogether, (from the learner's perspective) 
a very good wrong answer! Once you are very experienced, you will be able to skip straight to the correct answer.
When you are just learning the material, your first idea will often be incorrect. Your willingness to critically examine your ideas,
and to revise or reject them when needed, will lead you to the truth.

\medskip

{\bf Problem 2.} {\itshape Compute the probability that she wins both the car and the meal delivery.}

\smallskip

Here we care about the first {\itshape two} rats. We write
\[
S = \{ PO, PB, PR, PG, OP, OB, OR, OG, BP, BO, BR, BG, RP, RO, RB, RG, GP, GO, GB, GR \}.
\]
The sample space has twenty elements in it. ($20 = 5 \times 4$: there are $5$ possibilities for the first place
finisher, and (once we know who wins) $4$ for the second. More on this later.) The event that she wins
is 
\[
\{ PO, OP \}
\]
and $P(E) = \frac{N(E)}{N(S)} = \frac{2}{20} = \frac{1}{10}.$

\medskip
{\bf Problem 3.} {\itshape Compute the probability that she wins all three prizes.}

\smallskip

Zero. Duh. She only won two rats! Sorry.

\subsection{The Addition and Multiplication Rules}

\begin{important}
{\bf The Addition Rule (1).} Suppose $E$ and $F$ are two {\itshape disjoint} events in the {\itshape same sample space}
-- i.e., they don't overlap.
Then
\[
P(E \textnormal{ or } F) = P(E) + P(F).
\]
\end{important}

\begin{example} You roll a die. Compute the probability that you roll either a 1, or a four or higher.

\medskip
Let $E = \{ 1 \}$ be the event that you roll a $1$, and $E = \{4, 5, 6\}$ be the event that you roll a $4$ or higher.
Then
\[
P(E \tor F) = P(E) + P(F) = \frac{1}{6} + \frac{3}{6} = \frac{4}{6} = \frac{2}{3}.
\]
\end{example}

\begin{example} You draw a poker card at random. What is the probability you draw either a heart, or a black
card which is a ten or higher?

\medskip
Let $E$ be the event that you draw a heart. As before, $P(E) = \frac{13}{52}$.

Let $F$ be the event that you draw a black card ten or higher, i.e.,
\[
F = \{ A\cs, K\cs, Q\cs, J\cs, 10\cs, A\ss, K\ss, Q\ss, J\ss, 10\ss \}.
\]
Then $P(F) = \frac{10}{52}$.

So we have
\[
P(E \tor F) = \frac{13}{52} + \frac{10}{52} = \frac{23}{52}.
\]

\end{example}

\begin{example} You draw a poker card at random. What is the probability you draw either a heart, or a red
card which is a ten or higher?

\medskip
This doesn't have the same answer, because hearts are red. If we want to apply the addition rule, we have to do so
carefully.

Let $E$ be again the event that you draw a heart, with $P(E) = \frac{13}{52}$.

Now let $F$ be the event that you draw a {\itshape diamond} which is ten or higher:
\[
F = \{ A \ds, K \ds, Q\ds, J\ds, 10\ds \}.
\]
Now together $E$ and $F$ cover all the hearts and all the red cards at least ten, and there is no overlap. So we can use the addition
rule.
\[
P(E \tor F) = P(E) + P(F) = \frac{13}{52} + \frac{5}{52} = \frac{18}{52}.
\]
\end{example}

We can also use the addition rule with more than two events, as long as they don't overlap.

\begin{example} 
Consider the Rat Race contestant from earlier. What is the probability that she wins any two of the prizes?

\medskip
{\bf Solution 1.} We will give a solution using the addition rule. (Later, we will give another solution using the Multiplication Rule.)

Recall that her chances of winning the car and the meal delivery were $\frac{1}{10}$. Let us call this event $CM$
instead of $E$.

Now what are her chances of winning the car and the guitar? (Call this event $CG$.) Again $\frac{1}{10}$. If you like, you can work this question
out in the same way. But it is best to observe that there is a natural symmetry in the problem. The rats are all alike and any ordering
is equally likely. They don't know which prizes are in which lanes. So the probability has to be the same.

Finally, what is $P(MG)$, the probability that she wins the meal service and the guitar? Again $\frac{1}{10}$ for the same reason.

Finally, observe these events are all disjoint, because she can't possibly win more than two. So the probability is three times $\frac{1}{10}$,
or $\frac{3}{10}$.
\end{example}

Here is a contrasting situation. Suppose the contestant had picked all three small prices correctly, and got to choose three of the rats. In this
case, the probability she wins both the car and the meal service is $\frac{3}{10}$, rather than $\frac{1}{10}$. (You can either work out the details
yourself, or else take my word for it.)

But this time the probability that she wins two prizes is {\itshape not} $\frac{3}{10} + \frac{3}{10} + \frac{3}{10}$, because now the events
$CM$, $CG$, and $MG$ are not disjoint: it is possible for her to win all three prizes, and if she does, then all of $CM$, $CG$, and $MG$ occur!

It turns out that in this case the probability that she wins {\itshape at least} two is $\frac{7}{10}$, and the probability that she wins {\itshape exactly}
two is $\frac{3}{5}$.

\medskip

\begin{important}
{\bf The Multiplication Rule.} The multiplication rule computes the probability that two events $E$ and $F$ {\bf both} occur. Here they are events
in {\bf different} sample spaces.

The formula is the following:
\[
P(E \textnormal{ and } F) = P(E) \times P(F).
\]
It is not always valid, but it is valid in either of the following circumstances:
\begin{itemize}
\item The events $E$ and $F$ are {\itshape independent}.
\item The probability given for $F$ assumes that the event $E$ occurs (or vice versa).
\end{itemize}
\end{important}

\begin{example} You flip a coin twice. What is the probability that you flip heads both times?

\medskip
We can use the multiplication rule for this. The probability that you flip heads if you flip a coin once is $\frac{1}{2}$.
Since coin flips are independent (flipping heads the first time doesn't make it more or less likely that you will flip heads
the second time) we multiply the probabilities to get $\frac{1}{2} \times \frac{1}{2} = \frac{1}{4}.$

Alternatively, we can give a direct solution. Let
\[
S = \{ HH, HT, TH, TT \}
\]
and 
\[
E = \{ HH \}.
\]
Since all outcomes are equally likely,
\[
P(E) = \frac{ N(E) }{N(S) } = \frac{1}{4}.
\]
\end{example}

We can also use the multiplication rule for more than two events.

\begin{example} You flip a coin twenty times. What is the probability that you flip heads every time?

\medskip If we use the multiplication rule, we see at once that the probability is
\[
\frac{1}{2} \times \frac{1}{2} \times \cdots \times \frac{1}{2} = \frac{1}{2^{20}} = \frac{1}{1048576}.
\]
\end{example}

This example will illustrate the second use of the Multiplication Rule.
\begin{example}
Consider the Rat Race example again (as it happened in the video). What is the probability that the contestant
wins both the car and the meal service?
\end{example}

\soln The probability that she wins the car is $\frac{2}{5}$, as it was before. So we need to now compute the probability
that she wins the meal service, {\itshape given that she won the car}. 

This time the sample space consists of {\itshape four} rats: we leave out whichever one won the car. The event
is that her remaining one rat wins the meal service, and so the probability of this event is $\frac{1}{4}$. 

By the multiplication rule, the total probability is
\[
\frac{2}{5} \times \frac{1}{4} = \frac{1}{10}.
\]

\begin{example} Suppose a Rat Race contestant prices all three prizes correctly and has the opportunity to race three rats.
What is the probability she wins all three prizes?
\end{example}

\soln The probability she wins the car is $\frac{3}{5}$, as before: the sample space consists of the five rats, and the event that
she wins consists of the three rats she chooses. (Her probability is $\frac{3}{5}$ no matter which rats she chooses, under our assumption
that they finish in a random order.)

Now assume that she wins the first prize. Assuming this, the probability that she wins the meals is $\frac{2}{4} = \frac12$ The sample space
consists of the four rats {\itshape other than the first place finisher,} and the event that she wins the meals consists of the two rats
{\itshape other than the first place finishers}.

Now assume that she wins the first and second prizes. The probability she wins the guitar is $\frac{1}{3}$:  
the sample space
consists of the three rats {\itshape other than the first two finishers,} 
and the event that she wins the meals consists of the single rat {\itshape other than the first two finishers}.

\medskip
There is some subtlety going on here! To illustrate this, consider the following:
\begin{example} Suppose a Rat Race contestant prices all three prizes correctly and has the opportunity to race three rats.
What is the probability she wins the meal service?
\end{example}
\soln There are five rats in the sample space, she chooses three of them, and each of them is equally likely to finish second. 
So her probability is $\frac35$ (same as her probability of winning the car).

\medskip
{\bf But didn't we just compute that her odds of winning the car are $\frac12$?} What we're seeing is something we'll investigate much more later.
This probability $\frac12$ is a {\bf conditional} probability: it assumes that one of the rats finished first, and illustrates what is hopefully intuitive:
if she wins first place with one of her three rats, she is less likely to also win second place.

In particular, this reasoning illustrates the following \red{misapplication of the multiplication rule. Suppose we compute again the probability
that she wins all three prizes with three rats. She has a $\frac{3}{5}$ probability of winning first, a  $\frac{3}{5}$ probability of winning second,
and a  $\frac{3}{5}$ probability of winning third. By the multiplication rule, the probability that all of these events occur is
\[
\frac{3}{5} \times \frac{3}{5} \times \frac{3}{5} = \frac{27}{125}.
\]
}

What is wrong with this reasoning is that these events are {\itshape not independent.}

\medskip
\begin{important}
{\bf Michael Larson.} Here is a bit of game show history. The following clip 
comes from the game show Press Your Luck on May 19, 1984.

\urlc{https://www.youtube.com/watch?v=UzggoA4lLwk}
\end{important}

Here Michael Larsen smashed the all-time record by winning \$110,237.
The truly fascinating clip starts at around 17:00, where Larson continues to press his luck, to the host's
increasing disbelief. On 28 consecutive spins, Larson avoided all the whammies and each time
hit a space that afforded him an extra spin. There are eighteen squares on the board, and on
average there are approximately five spaces worth money and an extra spin.

\begin{example} Assume for simplicity that each time there are exactly five spaces (out of eighteen)
that Larson wants to hit, and that the outcome is random and that each square is equally likely to
occur. 

If Larson spins twenty-eight times, compute the probability that he hits a good spot every time.

\medskip
{\bf Solution.} This is a straightforward application of the multiplcation rule. The answer is
$\left( \frac{5}{18} \right)^{28}$, or approximately one in
\[
3,771,117,128,139,603.
\]
\end{example}

Either Larson got very, {\itshape very}, {\bf very}, {\Large {\bf VERY}} lucky....... or else 
the pattern is not random and he figured it out.

\medskip
{\bf Card Sharks.} Here is another game show from the eighties that leads to interesting probability computations.

\newgame{Card Sharks}{
Each of two contestants receives a lineup of five cards. The first is shown to each contestant, and a {\itshape marker} is placed
on the first card. The objective of each {\bf round} is to reach the last card.

\medskip
A {\bf turn} by the contestant consists of the following. She starts with the (face-up) card at the marker, and may
replace it with a random card if she chooses. She then guesses whether the next card is higher or lower, which is then revealed. 

If is the last card and her guess is correct, she wins the round. Otherwise, she may keep guessing cards for as long as she likes untill
one of three things happens: (1) she guesses the last card correctly, and wins; (2) she guesses any card incorrectly, in which case
the cards she has guessed are all discarded and replaced with new cards (face down); (3) she chooses to end the turn by moving
her marker forward to the last card guessed correctly.

\medskip
The {\bf round} begins with a trivia question (I don't describe the rules for that here), and the winner gets to take a turn. If this turn ends
with a freeze, the contestants go to another trivia question; if it ends with a loss, the other contestant takes a turn.
}

There is also a {\itshape bonus round} which we won't discuss here. (We could though; analyzing this would make an interesting term
project.)

\begin{important}
Here\footnote{{\itshape Summary of the clip}: ({\bf Please note.} The trivia questions are off-color and arguably sexist. This is unfortunately
common on this show.) 
The contestants are Royce and Cynthia. Cynthia wins the first trivia question. Her initial card is a king. She keeps it
and guesses lower; the second card is a two. She guesses higher; the third card is a nine. She freezes on position three.

Royce wins the next trivia question. His initial card is an eight; he changes it and gets a four. He guesses higher; the second card is
a six. He guesses higher; the third card is a nine. He freezes on position three.

Royce wins the next trivia question. He starts on position three and chooses to replace the nine, and gets a three. He guesses higher;
the fourth card is a five. He guesses higher; the fifth card is a king and Royce wins the round.
} is a typical clip:
\urlc{https://www.youtube.com/watch?v=bUv0CRU6t5o}
\end{important}

Here is our objective: {\itshape Assuming that the trivia questions are a 50-50 tossup, determine the optimal strategy in all situations.}
This problem is somewhat difficult (and our mental heuristics for it are fairly spot on). But at least in principle, it is possible to give a complete 
solution to this problem.

We won't try to achieve this all at once. Instead, we'll ask a number of probability questions to get started:

\begin{example}
Consider Cynthia's first turn, where she guesses `lower'. Compute the probability that she is correct.

\medskip
{\bf Answer.} The sample space consists of the $51$ cards other than the king of clubs. Of these, only seven
are not lower: the four aces, and the three remaining kings. So $51 - 7 = 44$ cards are lower, and her chances
are $\frac{44}{51}$.
\end{example}

We also compute the probabilities at the next two rounds. She guesses the third card will be higher than a $2$.
There are $50$ cards remaining, and $47$ of them are higher than a $2$, so her odds are $\frac{47}{50}$.

The next card was a $9$. Of the $49$ remaining cards, $27$ are lower than a $9$ and $19$ are higher. (And the three
remaining nines are neither higher nor lower -- so she would lose no matter what she picked). If she chose to play,
her odds of winning the next card would be $\frac{27}{49}$, or slightly better than $50$-$50$. She quite reasonably chooses to freeze and lock in her position.

Now we skip ahead to Royce's second round (when both Royce and Cynthia have frozen on the third of five cards).

Here are several questions we can ask:
\begin{itemize}
\item
Given that Royce has replaced his nine with a three, compute the probability that he can win the round (assuming he doesn't freeze).
\item
Before Royce sees the three, compute the probability that he can win the round.
\item
Given that Royce's card is a five, compute the probability that he wins if he doesn't choose to freeze.
\item
If Royce chooses to freeze, answers the next trivia question correctly, and gets to go again, compute the probability that he wins
on his next attempt.

(Note that this is not the total probability he wins: he could lose on his next attempt, but then answer another trivia question correctly and
get yet another try.)
\item
If Royce chooses to freeze and Cynthia answers the next trivia question correctly, what is the probability that she wins the next round
(if she doesn't freeze)?
\end{itemize}
These questions get us closer to the question we're {\itshape really} interested in: should Royce freeze on the five or not? As is often the case,
the question we are interested in is quite difficult and we build up to being able to answer it.

We tackle the first question.
\begin{example}
Given that Royce has replaced his nine with a three, compute the probability that he can win the round. Assume that he doesn't choose to freeze,
and that his higher/lower guess is always optimal.
\end{example}

Note that there are $48$ cards left in the deck: a three, a four, a six, and a nine are all missing.

It is easy to compute the probability that Royce's {\itshape first} guess is correct: out of $48$ remaining cards, $41$ are higher,
so the probability is $\frac{41}{48}$. Now, {\bf assuming that Royce's first guess is correct}, what is the probability
that his second guess is correct?

Well ...... we don't know. It depends on what the first card {\bf is}. Later, we will see some clever tricks for carrying out
this sort of computation more easily. But for now, we outline a `brute force' computation:
\begin{itemize}
\item Royce's first guess will be correct if the first card is a four, five, six, seven, eight, nine, ten, jack, queen, king, or ace.
\item Based on Royce's first guess, we can determine what Royce should guess for the second card and the probability
that this guess will be correct.

Let's do an example of this. Suppose the first card is a four; the probability of this occurring is $\frac{3}{48}$. (This reduces
to $\frac{1}{16}$, but the pattern will be clearer if we do not reduce our fractions to lowest terms.)

Then Royce should clearly guess that the second will be higher.
There are $47$ remaining cards, of which $38$ are higher than a four. So {\itshape assuming that the first card is a four}, the probability
that Royce wins is $\frac{38}{47}$. Therefore, the probability that {\itshape the first card is a four {\bf and} Royce wins} is
$\frac{3}{48} \times \frac{38}{47}$.

\item We will therefore use {\bf both} the addition and the multiplication rules by {\bf dividing into cases}: For each possible first card $n$ (that doesn't
lose Royce the round immediately), we compute the probability that the first card {\bf is} $n$ {\bf and that} Royce wins the round. This is the multiplication
rule.

Since all of these possibilities are mutually exclusive, but one of them has to occur if Royce is to win, we see that the probability that Royce wins
is the total of the probabilities we computed in the first step. This is the addition rule!
\end{itemize}

Let's roll up our sleeves and do it. The proof won't be pretty, but it is not as scary as it looks.
\begin{itemize}
\item
With probability $\frac{3}{48}$ the first card will be a four. Then Royce should guess higher, and with probability
$\frac{38}{47}$ the next card will be higher.
\item
With probability $\frac{4}{48}$ the first card will be a five. Then Royce should guess higher, and with probability
$\frac{34}{47}$ the next card will be higher.
\item
With probability $\frac{3}{48}$ the first card will be a six. Then Royce should guess higher, and with probability
$\frac{31}{47}$ the next card will be higher.
\item
With probability $\frac{4}{48}$ the first card will be a seven. Then Royce should guess higher, and with probability
$\frac{27}{47}$ the next card will be higher.
\item
With probability $\frac{4}{48}$ the first card will be an eight. Then Royce should guess higher, and with probability
$\frac{23}{47}$ the next card will be higher.
\item
With probability $\frac{3}{48}$ the first card will be a nine. Then Royce should guess lower, and with probability
$\frac{24}{47}$ the next card will be lower.
\item
With probability $\frac{4}{48}$ the first card will be a ten. Then Royce should guess lower, and with probability
$\frac{27}{47}$ the next card will be lower.
\item
With probability $\frac{4}{48}$ the first card will be a jack. Then Royce should guess lower, and with probability
$\frac{31}{47}$ the next card will be lower.
\item
With probability $\frac{4}{48}$ the first card will be a queen. Then Royce should guess lower, and with probability
$\frac{35}{47}$ the next card will be lower.
\item
With probability $\frac{4}{48}$ the first card will be a king. Then Royce should guess lower, and with probability
$\frac{39}{47}$ the next card will be lower.
\item
With probability $\frac{4}{48}$ the first card will be an ace. Then Royce should guess lower, and with probability
$\frac{43}{47}$ the next card will be lower.
\end{itemize}
(Note that all of the cases look more or less the same. Often, this is an indication that you can look for shortcuts -- but we won't do so here.)

The total probability that Royce wins is therefore
\[
\frac{3}{48} \cdot \frac{38}{47} +
\frac{4}{48} \cdot \frac{34}{47} +
\frac{3}{48} \cdot \frac{31}{47} +
\frac{4}{48} \cdot \frac{27}{47} +
\frac{4}{48} \cdot \frac{23}{47} +
\frac{3}{48} \cdot \frac{24}{47} +
\frac{4}{48} \cdot \frac{27}{47} +
\frac{4}{48} \cdot \frac{31}{47} +
\frac{4}{48} \cdot \frac{35}{47} +
\frac{4}{48} \cdot \frac{39}{47} +
\frac{4}{48} \cdot \frac{43}{47}.
\]

This is equal to $\frac{1315}{2256}$, which is already in lowest terms. Yeah, I know. You were hoping it would be nice and simple, and that
in retrospect you could have solved the problem in your head. You couldn't have. Neither could I. Sometimes math is like that.

This is roughly 58.2\%, which is not bad at all.

\subsection{Permutations and factorials}

\begin{important}
This video\footnote{{\itshape Summary of the clip}: She plays Ten Chances for a pasta maker, a lawnmower, and a car.
The digits in the pasta maker are $069$, and she guesses the correct price of $90$ on her second chance.
The digits in the mower are $0689$, and she guesses the correct price of $980$ on her third chance.
(Her third chance overall; she took only once to win the mower.) The digits in the car are $01568$, and she
guesses the correct price of $16,580$ on her first try (and wins).

Barker then hides beyond the prop ... and, uh, ({\bf please note}) the contestant violates his personal space.}
illustrates a playing of the Price Is Right game {\bf Ten Chances}:

\urlc{https://www.youtube.com/watch?v=iY_gmGcDKXE}
\end{important}

\newgame{Ten Chances (The Price Is Right)}{
The contestant is shown a small prize, a medium prize, and a large prize. She has ten chances
to win as many prizes as she can.

The price of small prize has two numbers in it, and the contestant is shown three different numbers.
She then guesses the price of the first prize. She takes as many chances as she needs to.

Once she wins the small prize, she attempts to win the medium prize. The price of the medium prize has three numbers in it,
and the contestant is shown four.

Finally, if she wins the medium prize, she attempts to win the car. Its price has five numbers in it, and the contestant is shown these five.
}

\begin{example}
The price of the pasta maker contains two digits from $\{ 0, 6, 9 \}$. Suppose that each possibility is equally likely to be the price of the pasta maker.

If the contestant has one chance, what
are her odds of winning?
\end{example}

\medskip {\bf Solution 1.} We can give a straightforward solution by simply enumerating the sample space of all possibilities. It is
\[
\{
06, 09, 60, 69, 90, 96
\}.
\]
The contestant's choice describes an event with one of these possibilities in it. Since we hypothesized that each was equally likely to occur,
her odds of winning are $\frac{1}{6}$.

\medskip {\bf Solution 2.} We use the multiplication rule. There are three different possibilities for the first digit, and exactly one of them is correct.
The probability that she gets the first digit correct is therefore $\frac{1}{3}$.

Now, {\bf assume she got the first digit correct.} (If she didn't, she might have used up the correct second digit already, and be doomed to botch
that one also!) Then there are two remaining digits, and the probability that she picks the correct one is $\frac{1}{2}$.

Thus the probability of getting both correct is $\frac{1}{3} \times \frac{1}{2} = \frac{1}{6}$.

\medskip
Notice, incidentally, that our assumption that the possibilities are equally likely is not realistic. Surely the pasta maker's price is not $06$ dollars?
Especially since you'd write it $6$ and not $06$? (Indeed, if you have watched the show a lot, you know that when there is a zero the price always ends with it.
Knowing this fact is a {\itshape big} advantage.)

\medskip
Now, she is going to use up at most six of her chances on the pasta maker, so she gets to move on to the mower. Here the price contains three digits
from $\{ 0, 6, 8, 9 \}$. This problem can be solved in the same way. The relevant sample space is
\[
\{
068, 069, 086, 089, 096, 098, 608, 609, 680, 689, 690, 698, 806, 809, 860, 869, 890, 896, 906, 908, 960, 968, 980, 986 \}
\]
which has $24$ elements in it, so her probability of winning is $\frac{1}{24}$. The analogue of solution $2$ gives
$\frac{1}{4} \times \frac{1}{3} \times \frac{1}{2} = \frac{1}{24}$.

\medskip
Finally, the price of the car has the digits $\{ 0, 1, 5, 6, 8\}$ and this time she uses all of them. The sample space is too long to
effectively write out. So we work out the analogue of Solution 2: Her odds of guessing the first digit are $\frac{1}{5}$. 
If she does so, her odds of guessing the second digit is $\frac{1}{4}$ (since she has used one up). If both these digits are
correct, her odds of guessing the third digit is $\frac{1}{3}$. If these three are correct, her odds of guessing the fourth digit
are $\frac{1}{2}$. Finally, {\bf if} the first four guesses are correct then the last digit is automatically correct by process of elimination.
So the probability she wins is
\[
\frac{1}{5} \times \frac{1}{4} \times \frac{1}{3} \times \frac{1}{2} \times 1 = \frac{1}{120}.
\]
Here the number $120$ is equal to $5!$, or $5$ {\bf factorial}. In math, an exclamation point is read `factorial' and it means
the product of all the numbers up to that point. We have
\begin{align*}
1! \ = \ & 1 \ & \ = 1\\
2! \ = \ & 1 \times 2 \ & = 2\\
3! \ = \ & 1 \times 2 \times 3 \ & = 6\\
4! \ = \ & 1 \times 2 \times 3 \times 4 \ & = 24\\
5! \ = \ & 1 \times 2 \times 3 \times 4 \times 5 \ & = 120\\
6! \ = \ & 1 \times 2 \times 3 \times 4 \times 5 \times 6 \ & = 720\\
7! \ = \ & 1 \times 2 \times 3 \times 4 \times 5 \times 6 \times 7 \ & = 5040\\
8! \ = \ & 1 \times 2 \times 3 \times 4 \times 5 \times 6 \times 7 \times 8 \ & = 40320\\
9! \ = \ & 1 \times 2 \times 3 \times 4 \times 5 \times 6 \times 7 \times 8 \times 9 \ & = 362880\\
10! \ = \ & 1 \times 2 \times 3 \times 4 \times 5 \times 6 \times 7 \times 8 \times 9 \times 10 \ & = 3628800,
\end{align*}
and so on. We also write $0! = 1$. Why $1$ and not zero? $0!$ means `don't multiply anything',
and we think of $1$ as the starting point for multiplication. (It is the {\itshape multiplicative identity}, satisfying
$1 \times x = x$ for all $x$.) So when we compute $0!$ it means we didn't leave the starting point.

These numbers occur {\bf very} commonly in the sorts of questions we have been considering, for reasons we will shortly see.

\begin{example}
The lucky contestant wins the first two prizes in only three chances, and has seven chances left over.
If each possibility for the price of the car is equally likely,
then what is the probability that she wins it?
\end{example}
The answer is seven divided by $N(S)$, the number of elements in the sample space. So if we could just compute $N(S)$,
we'd be done. 

Here there is a trick! She guesses $16580$, and we know that the probability that this is correct is $\frac{1}{N(S)}$:
one divided by the number of total possible guesses. But we already computed the probability: it's $\frac{1}{120}$.
Therefore, we know that $N(S)$ is $120$, without actually writing it all out!

\medskip
The mathematical discipline of {\bf combinatorics} is the art of {\itshape counting without counting}. We just solved our first
combinatorics problem: we figured out that
there were $120$ ways to rearrange the numbers $0, 1, 5, 6, 8$ without actually listing them. We now formalize this principle.

\begin{important}
\begin{defn}
Let $T$ be a {\bf string}. For example, $01568$ and $22045$ are strings of numbers, 
$ABC$ and $xyz$ are strings of letters, and $\otimes - \oplus \clubsuit \skull$ is a string of symbols. Order matters:
$01568$ is not the same string as $05186$.

A {\bf permutation} of $T$ is any reordering of $T$. 
\end{defn}
\end{important}
So, for example, if $T$ is the string $1224$, then $2124$, $4122$, $1224$, and $2142$ are all permutations of $T$.
Note we {\itshape do} consider $T$ itself to be a permutation of $T$, for the same reason that we consider $0$ a number.
It is called the {\bf trivial permutation}.

We have the following:
\begin{important}
\begin{proposition}
Let $T$ be a string with $n$ distinct symbols. Then there are exactly $n!$ distinct permutations of $T$.
\end{proposition}
\end{important}
In math, a {\itshape proposition} (or a {\itshape theorem}) is a statement of something true. We have stated lots of true
facts in these notes; here the title `Proposition' indicates that this one is particularly important and worth your attention.

{\itshape Please read the statement carefully}. In particular, the conclusion is only guaranteed to hold when the hypotheses
also hold. If the hypotheses don't hold, then the conclusion may or may not be true. For example, if $T$ is the string
$122$, then the set of all permutations of it is
\[
\{ 122, 212, 221 \}
\]
which has $3$ elements, and $3 \neq 3! = 6$.

Note also that this solves our earlier Ten Chances question. The contestant's guesses are all permutations of 
the string $01568$, of which there are $5! = 120$. The sample space $S$ consists of all $120$ permutations.
The contestant can make seven guesses, so let $E$ be the set of these $7$ permutations. Since we have assumed
that each possible guess is equally likely to be correct, her odds (probability) of winning are $\frac{7}{120}$.

\medskip
We will now offer a {\bf proof} of the proposition. Please don't be too scared by the word `proof': it just means a convincing
explanation of why it is true. This course will not focus on {\itshape writing} proofs, but it is good to gain practice reading them.

\medskip

\begin{proof}
Suppose $T$ is a string with $n$ distinct symbols, and we want to construct a permutation of $T$. 
symbol
We first choose the first symbol. Since $T$ has $n$ distinct symbols, we have $n$ choices for the first symbol.

No matter what we choose for the first symbol, there are $n - 1$ choices for the second symbol (all but the one we
picked already), so that there
are $n \times (n - 1)$ choices for the first two.

Similarly, there are $n - 2$ choices for the third symbol, and so on. This continues until the last (the $n$th) symbol, for which
there is exactly one choice.
\end{proof}

In math we often end proofs with a little square. If you like, you can end proofs with the phrase {\bf QED}, which is an abbreviation
for `quod erat demonstrandum' -- Latin for `that which was to be shown'. In practice, saying or writing `QED' serves the same
purpose as a football player spiking the ball after he has scored a touchdown.

If you are especially observant, you will notice that the proof is very similar to our explanation of the multiplication rule
for probability. There is a good reason for this: the same principle underlies both, and counting and probability are two sides
of the same coin.

\medskip
We now return to our Ten Chances contestant. Recall that she has seven chances to win the car.

\begin{example}
Suppose that the contestant has watched The Price Is Right a lot and so knows that
{\itshape the last digit is the zero.} Compute the probability that she wins the car, given seven chances.
\end{example}

\soln Here her possible guesses consist of permutations of the string $1568$, followed by a zero. There are
$4! = 24$ of them, so her winning probability is $\frac{7}{24}$.

Her winning probability went up by a factor of exactly $5$ -- corresponding to the fact that $\frac{1}{5}$ of the permutations
of $01568$ have the zero in the last digit. Equivalently, a random permutation of $01568$ has probability $\frac{1}{5}$ of
having the zero ias the last digit.

\medskip
Now, a smart contestant can do better. Suppose, for example, that she guessed $85610$. Mathematically it looks like a good guess ....
but she is playing for a Chevy Cavalier. I mean, really. We can rule out the $8$ as the first digit, as well as the $6$ and the $5$.


\begin{example}
Suppose that the contestant knows that the
{\itshape the last digit is the zero} and {\itshape the first digit is the one}. 
Compute the probability that she wins the car, given seven chances.
\end{example}

\soln Her guesses now consist of permutations of the string $568$, with a $1$ in front and followed by a zero. There are
$3! = 6$ of them. Assuming that the assumptions are correct and that she doesn't screw up, she is a sure bet to win the car.

Mathematically, her probability of winning is $1$ (which is the same as $100\%$). Please {\itshape don't} answer that her
probability is $\frac{7}{6}$. This doesn't make much sense!

\medskip
Note that it is only true of Ten Chances that car prices always end in zero -- not of The Price Is Right in general. Here is a contestant who
is very excited until she realizes the odds she is against:

\urlc{https://www.youtube.com/watch?v=AAIU6knD7BA}

\subsection{Exercises}

Most of these should be relatively straightforward, but there are a couple of quite difficult exercises mixed in here for good measure.


\begin{enumerate}
\item Card questions. In each question, you choose at random a card from an ordinary deck. What is the probability you --
\begin{enumerate}
\item Draw a spade?
\item Draw an ace?
\item Draw a face card? (a jack, queen, king, or an ace)
\item Draw a spade or a card below five?
\end{enumerate}

\item Dice questions:
\begin{enumerate}
\item
You roll two dice and sum the total. What is the probability you roll exactly a five? At least a ten?
\hwsol{The sample space consists of 36 possibilities, $11$ through $66$. The first event can be described
as $\{ 14, 23, 32, 41 \}$ and has probability $\frac{4}{36} = \frac19$. The second can be described as 
$\{ 46, 55, 64, 56, 65, 66 \}$ and has probability $\frac{6}{36} = \frac16$.}
\item
You roll three dice and sum the total. What is the probability you roll at least a 14? 
(This question is kind of annoying if you do it by brute force. 
Can you be systematic?)
\hwsol{There are several useful shortcuts. Here is a different way than presented in lecture. The sample space consists of
$6 \times 6 \times 6 = 216$ elements, $111$ through $666$. The event of rolling at least a $14$ can be described as
\[
\{ 266 (3), 356 (6), 366 (3), 446 (3), 455 (3), 456 (6), 466 (3), 555(1),
556 (3), 566 (3), 666 (1) \}.
\]
The number in parentheses counts the number of permutations of that dice roll, all of which count. For example, $266$,
$626$, and $662$ are the permutations of $266$. There are $35$ possibilities total, so the probability is
$\frac{35}{216}$.
}


\item The dice game of {\itshape craps} is (in its most basic form) played as follows.

You roll two dice. If you roll a 7 or 11 on your first roll, you win immediately, and if you roll a 2, 3, or 12 immediately, you lose immediately.
Otherwise, your total is called ``the point'' and you continue to roll again until you roll either the point (again) or a seven. If you roll the point,
you win; if you roll a seven, you lose.

In a game of craps, compute the probability that you win on your first roll and the probability that you lose on your second roll.
\hwsol{The probability of winning on your first roll is the probability of rolling a $7$ or $11$: $\frac{6}{36} + \frac{2}{36} = \frac{8}{36} = \frac{2}{9}$.

For the second question, I intended to ask the probability that you lose on your {\itshape first} roll. Oops. Let's answer the question as asked. There
are multiple possible interpretations, and here is one. Let us compute the probability that you lose on the second round, presuming that the game
goes on to a second round. This is the probability of rolling a $6$ or $\frac{1}{6}$.
}
\item
In a game of craps, compute the probability that the game goes to a second round and you win on the second round.
\hwsol{This can happen in one of six possible ways: you roll a $4$ twice in a row, a $5$ twice in a row, or similarly with a $6$, $8$, $9$, or $10$.

The probability of rolling a $4$ is $\frac{3}{36}$, so the probability of rolling a $4$ twice in a row is $\left( \frac{3}{36} \right)^2$. Similarly with
the other dice rolls; the total probability is
\begin{multline}
\left( \frac{3}{36} \right)^2 + 
\left( \frac{4}{36} \right)^2 + 
\left( \frac{5}{36} \right)^2 + 
\left( \frac{5}{36} \right)^2 + 
\left( \frac{4}{36} \right)^2 + 
\left( \frac{3}{36} \right)^2 = \\
\frac{9 + 16 + 25 + 25 + 16 + 9}{1296} = \frac{100}{1296} = \frac{25}{324}.
\end{multline}
}
\item
In a game of craps, compute the probability that the game goes to a second round and you lose on the second round.
\hwsol{Multiply the probability that the game goes onto a second round (easily checked to be $\frac{2}{3}$) by the probability $\frac{1}{6}$ computed
earlier, so $\frac{1}{9}$.}
\item
In a game of craps, compute the probability that you win.
\hwsol{With probability $\frac29$ you win on your first round. We will now compute the probability that you win later, with the point equal to $n$,
for $n$ equal to $4$, $5$, $6$, $8$, $9$, or $10$. We will then add these six results. Write the probability of rolling $n$ on one roll of two dice
as $\frac{a}{36}$, so that $a$ is $3$, $4$, or $5$ depending on $n$.
\begin{itemize}
\item
As we computed before, the probability of winning on the second round (with point $n$) is $\left( \frac{a}{36} \right)^2$.
\item
On each round after the first, there is a probability $\frac{30 - a}{36}$ of rolling something other than $7$ or the point. This is the probability that
the game goes on to another round.
\item
So, the probability of winning on the third round is the probability of: rolling the point on the first round, going another turn in the second round,
rolling the point on the third round. This is $\left( \frac{a}{36} \right)^2 \cdot \left( \frac{30 - a}{36} \right).$
\item
Similarly, the probability of winning with point $n$ on the fourth round is $\left( \frac{a}{36} \right)^2 \cdot \left( \frac{30 - a}{36} \right)^2$, and so on.
The total of all these probabilities is
\[
\left( \frac{a}{36} \right)^2 \sum_{k = 0}^{\infty} \left( \frac{30 - a}{36} \right)^k.
\]
\item
For $|r| < 1$, we have the infinite sum formula $\sum_{k = 0}^{\infty} r^k = \frac{1}{1 - r}$. Plugging this in, the above expression is
\[
\left( \frac{a}{36} \right)^2 \cdot \frac{36}{6 + a} = \frac{a^2}{36(6 + a)}.
\]
\end{itemize}
So we add this up for $a = 3$ (twice, for $n = 4$ or $5$), $a = 4$ (twice), and $a = 5$ (twice). We get
\[
2 \cdot \bigg( \frac{9}{36 \cdot 9} + \frac{16}{36 \cdot 10} + \frac{25}{36 \cdot 11} \bigg) = \frac{134}{495}.
\]
Adding the to the first round probability of $\frac29$ we get
\[
\frac29 + \frac{134}{495} = \frac{244}{495}.
\]
This is a little less than a half. As expected, the house wins.
}

\end{enumerate}

\item Consider the game Press Your Luck described above. Assume (despite rather convincing evidence to the contrary) that the show
is random, and that you are equally likely to stop on any square on the board.

\begin{enumerate}
\item On each spin, estimate the probability that you hit a Whammy. Justify your answer.

(Note: This is mostly not a math question. You have to watch the video clip for awhile to answer it.)

\item On each spin, estimate the probability that you {\itshape do not} hit a Whammy.

\item If you spin three times in a row, what is the probability you don't hit a whammy? Five? Ten? Twenty-eight?
(If your answer is a power of a fraction, please also use a calculator or a computer to give a decimal approximation.)

\end{enumerate}

\item Consider the game Rat Race described above.
\begin{enumerate}
\item Suppose that the contestant only prices one item correctly, and so gets to pick one rat. What is the probability that she wins the car?
That she wins {\itshape something}? That she wins nothing?
\item What if the customer prices all three items correctly? What is the probability that she wins the car? Something? Nothing? All three items?
\item Consider now the first part of the game, where the contestant is pricing each item. {\itshape Assume} that she has a 50-50 chance
of pricing each item correctly. What is the probability she prices no items correctly? Exactly one? Exactly two? All three?

Comment on whether you think this assumption is realistic.
\hwsol{Foobar.}
\item Suppose now that she has a 50-50 chance of pricing each item correctly, and she plays the game to the end. 
What is the probability she wins the car?
\end{enumerate}


\end{enumerate}

\section{Expectation}

\subsection{Definitions and examples}

We come now to the concept of {\bf expected value}. We will give a few simple examples and then give a formal definition.

\begin{example}
You play a simple dice game. You roll one die; if it comes up a six, you win 10 dollars; otherwise you win nothing. On average, how much do you expect to win?
\end{example}

\soln Ten dollars times the probability of winning, i.e.,
\[
10 \times \frac{1}{6} = 1.66\dots
\]
\medskip
So, for example, if you play this game a hundred times, on average you can expect to win 100 dollars.

\begin{example}
You play a variant of the dice game above. You roll one die; if it comes up a six, you still win 10 dollars. But this time, if it doesn't come up a six, you lose two dollars.
On average, how much do you expect to win?
\end{example}

\soln
We take into account both possibilities. We multiply the events that you win $10$ dollars or lose $2$ dollars and multiply them by their probabilities.
The answer is
\[
10 \times \frac{1}{6} + (-2) \times \frac{5}{6} = 0.
\]
On average you expect to break even.

\begin{important}
\begin{defn} Consider a random process whose outcome can be described as a real number. Suppose that the possible outcomes
are $a_1, a_2, \dots a_n$, which occur with respective probabilities $p_1, p_2, \dots, p_n$. Then the {\bf expected value} of this
process is
\[
\sum_{k = 1}^n a_k p_k = a_1 p_1 + a_2 p_2 + \cdots + a_k p_k.
\]
\end{defn}
\end{important}
If the outcomes represent the amount of money you win (positive) or lose (negative), then the expected value is the amount you should expect to win 
on average.

\begin{example} You roll a die and win a dollar amount equal to your die roll. Compute the expected value of this game.
\end{example}

\soln The possible outcomes are that you win $1$, $2$, $3$, $4$, $5$, or $6$ dollars, and each happens with probability $\frac{1}{6}$.
Therefore the expected value is
\[
1 \times \frac16 + 
2 \times \frac16 + 
3 \times \frac16 + 
4 \times \frac16 + 
5 \times \frac16 + 
6 \times \frac16 = \frac{21}{6} = 3.5.
\]
\medskip

As another example, consider the Deal or No Deal clip from the introduction.

\urlc{https://www.youtube.com/watch?v=I3BzYiCSTo8}

This is quite simple to analyze, and indeed we did so in the introduction.

For example, after the second round, he has eliminated $11$ briefcases and $15$ remain, which contain a total of
\$2,808,416, or an average of \$187,227. If he keeps playing all the way until the end, the expected value is equal
to the average of the remaining briefcases.
The bank offers him a flat payment of 
\$125,000 to quit. If he wants to maximize his expected value, he should refuse this, and indeed he does.

\medskip
We now consider some expected value computations arising from the popular game show {\bf Wheel of Fortune}.

\newgame{Wheel of Fortune, Simplified Version}{
The contestants play several rounds where they try to solve word puzzles and win money. (The contestant who has won the most money
then gets to play in a bonus round.)

The puzzle consists of a phrase whose letters are all hidden. In turn, each contestant either {\bf attempts to solve the puzzle} or 
{\bf spins the wheel}. If the contestant attempts to solve, he states a guess; if is correct, he wins all the money in his bank, and if it is
wrong, play passes to the next player.

The wheel contains lots of spaces with various dollar amounts or the word `bankrupt'. When the contest spins, the wheel comes to rest on one
of these spaces. If `bankrupt', the contestant loses all his money from this round and play passes
to the next contestant. Otherwise, the contestant chooses a letter. If that letter appears in the puzzle (and has not yet been guessed),
then each of these letters is revealed and the contestant wins the amount of money on his space for each time it appears. If the
letter does not appear, the contestant wins nothing and play passes to the next contestant.
}

These rules are incomplete: the contestants can `buy a vowel'; there are non-monetary prizes on the board which work
differently (you don't win more than one of them if a letter appears multiple times), other spaces like `lose a turn',
and so forth.

\begin{important}
Consider the episode of Wheel of Fortune shown in this clip:
\urlc{https://www.youtube.com/watch?v=A8bZUXi7zDE}
Robert wins the first round in short order. After guessing only two letters (and buying a vowel) he chooses to solve the puzzle.
Was his decision wise?
\end{important}

Let us make some assumptions to simplify the problem and set up an expected value computation:
\begin{itemize}
\item Robert wants to maximize the expected value of his winnings this round.

This is not completely accurate, especially in the final round; the contestants are interested in winning {\itshape more than the other two
contestants}, because the biggest winner gets to play the bonus round. But it is reasonably close to accurate, especially early in the running. 

\item Robert definitely knows the solution to the puzzle. 

So, if he chooses to spin again, it's to rack up the amount of prizes and money he wins. 

\item If Robert loses his turn, then he won't get another chance and will therefore lose everything.

In fact, there is a chance that each of the other two contestants will guess wrongly or hit the `bankrupt'
or `lose a turn' spots on the wheel. But this puzzle doesn't look hard: the first word {\itshape don't} is fairly obvious; also,
the second word looks like {\itshape bet}, {\itshape get}, or {\itshape let} and B, G, and L are all in the puzzle. Robert is
wise to assume he won't get another chance.

\item We won't worry too much about the `weird' spots on the board. 

The $\frac{1}{3}$-sized 
million dollar wedge is not what it looks like: it sits over (what I believe is) a \$500 wedge now, and offers
the contestant the opportunity to win \$1,000,000 in the bonus round {\itshape if} he goes to the bonus round {\itshape and}
doesn't hit bankrupt before then {\itshape and} solves the bonus puzzle correctly {\itshape and} chooses the million
dollars randomly as one of five prizes. It's a long shot, although three contestants have indeed won the million.
\end{itemize}

So we freeze-frame the show and we count what we see. Out of $24$ wedges on the wheel, there are:
\begin{itemize}
\item $16$ ordinary money wedges on the wheel, with dollar amounts totalling \$12,200.
\item Two `bankrupt' wedges, a `lose a turn' wedge, and an additional two thirds of a bankrupt wedge surrounding the million.
\item A one-third size wedge reading `one million'.
\item The cruise wedge. This isn't relevant to the contestant's decision, because he wins the cruise and reveals an ordinary wedge
underneath. We can't see what it is, so let's say \$500.
\item Two other positive wedges.
\end{itemize}
Let us now compute the expected value of another spin at the wheel. There are (with the cruise wedge) $17$ ordinary
wedges worth a total of \$12,700. If the contestant hits `bankrupt' or `lose a turn' he loses his winnings so far (\$10,959 including the cruise).
Let us guess that the million wedge is worth, on average, \$5,000 to the contestant and that the other two are worth \$2,000 each.
His expected value from another spin is
\[
\frac{1}{24} \cdot 12700 + \frac{2 \frac{2}{3}}{24} \cdot (-10959) + \frac{2}{24} \cdot 2000 + \frac{ \frac{1}{3}}{24} \cdot 5000 = -
\$452.39.
\]
It is clear by a large margin to solve the puzzle and lock in his winnings.

\begin{remark} You may be wondering where the $\frac{1}{24} \cdot 12700$ came from. Here is one way to see it:
the seventeen wedges have an average of $\frac{12700}{17}$ dollars each, and there is a $\frac{17}{24}$ probability of hitting one
of them. So the contribution is 
\[
\frac{12700}{17} \times \frac{17}{24} = \frac{12700}{24}.
\]
\end{remark}
Now let us suppose that there was some consonant appearing in the puzzle twice. In that case Robert would know that
he could guess it and get {\itshape double} the amount of money he spun. So, in our above computation, we double the $12700$.
(We should probably increae the $2000$ and $5000$ a little bit, but not double them. For simplicity's sake we'll leave them alone.)
In this case the expected value of spinning again is
\[
\frac{1}{24} \cdot 12700 \cdot 2 + \frac{2 \frac{2}{3}}{24} \cdot (-10959) + \frac{2}{24} \cdot 2000 + \frac{ \frac{1}{3}}{24} \cdot 5000 = -
\$76.77,
\]
so slightly positive. If Robert has the stomach to risk his winnings so far, he should consider spinning again.

\medskip
For an example where Robert arguably chooses unwisely, skip ahead to 10:45 on the video (the third puzzle) where he solves the puzzle with
only \$1,050 in the bank. In the exercises, you are asked to compute the expected value of another spin. Note that there
are now two $L$'s and two $R$'s, so he can earn double the dollar value of whatever he lands on. There is now a \$10,000
square on the wheel, and hitting `bankrupt' only risks his \$1,050. (His winnings from the first round are safe.)

There is one factor in favor of solving now: an extra prize (a trip to Bermuda) for the winner of the round. If it were me, I would 
definitely risk it. You do the math, and decide if you agree.

(But see the fourth run, where I would guess he knows the puzzle and is running up the score.)

\medskip
The game {\bf Punch a Bunch} from The Price Is Right has a similar (but much simpler) mechanic:

\newgame{Punch-a-Bunch (The Price Is Right)}{
The contestant is shown a punching board which contains 50 slots with the following dollar amounts:
100 (5), 250 (10), 500 (10), 1000 (10), 2500 (8), 5000 (4), 10,000 (2), 25,000 (1). The contestant can earn up to
four punches by pricing small items correclty. For each punch, the contestant punches out one hole in the board.

The host proceeds through the holes punched one at a time. The host shows the contestant the amount of money
he has won, and he has the option of either taking it and ending the game, or discarding and going on to the next hole.
}

So, if you just get one punch, there is no strategy: you just take whatever you get. In this case the expected value
is the total of all the prizes divided by 50, or $\frac{103000}{50} = 2060$.

\begin{important}
Here is a typical playing of Punch-a-Bunch:

\urlc{https://www.youtube.com/watch?v=25THBiZNPpo}
\end{important}

The contestant gets three punches, throws away 500 on his first punch, 1000 on his second, and gets 10,000 on his third.
Was he right to throw away the 1000?

Clearly yes, as the expected value of one punch is 2,060. Indeed, in this example it is a little bit higher:
there is \$101,500 in prizes left in 48 holes, for an average of \$2,114.58. You don't have to do the math exactly:
just remember that two of the small prizes are gone, so the average of the remaining ones goes up slightly.

So let's figure out optimal strategy for this game. The last two rounds are easy.
\begin{itemize}
\item
On your last round, there is no strategy: you take whatever you get.
\item
On your next-to-last round, throw away anything less than \$2500. You should keep the \$2500 prize if you're trying
to maximize your expected value. It's pretty close though; I wouldn't fault anyone who tried for the big prize. (If nothing else, 
it would make better TV.)
\item
What about your third-to-last round?
\end{itemize}
We are going to compute the expected value of the {\itshape next-to-last round}. 
We'll assume that this is also the contestant's {\itshape first}
round; otherwise, the contestant will have thrown away one or two small prizes and the expected value will be slightly higher.
(This is another example of where we simplify our problem by making such an assumption. In this case, the assumption 
is very nearly accurate.)
\begin{itemize}
\item
The contestant might win \$25,000 ($\frac{1}{50}$ chance), \$10,000 ($\frac{2}{50}$ chance), \$5,000 ($\frac{4}{50}$ chance),
or \$2,500 ($\frac{8}{50}$ chance). As we discussed earlier, the contestant should keep it and end the game.
\item
The contestant might draw a card less than \$2,500 ($\frac{35}{50}$ chance). 
As we discussed earlier, the contestant should throw it away. In this case, the contestant expects to win \$2,060 (in fact, slightly
more, as previously discussed) on average from the last punch.
\end{itemize}
So the expected value of the next-to-last round is 
\[
25000 \cdot \frac{1}{50} + 10000 \cdot \frac{2}{50} + 5000 \cdot \frac{4}{50} + 2500 \cdot \frac{8}{50} + 2060 \cdot \frac{35}{50} = \$3,142.
\]
So we see that on the contestant's third-to-last round, he should throw away the \$2,500 cards in addition to everything cheaper, and only
settle for \$5,000 or more. The expected value of the third-to-last round is
\[
25000 \cdot \frac{1}{50} + 10000 \cdot \frac{2}{50} + 5000 \cdot \frac{4}{50} + 3142 \cdot \frac{43}{50} = \$4,002.12.
\]
Therefore, if the contestant gets four punches, his strategy on the first round should be the same: to keep anything \$5,000 or more, and throw
everything else away. The expected value of a four-round game is
\[
25000 \cdot \frac{1}{50} + 10000 \cdot \frac{2}{50} + 5000 \cdot \frac{4}{50} + 4002 \cdot \frac{43}{50} = \$4,741.72.
\]
A contestant can win only up to four punches. But we see that if the contestant got more, he would eventually throw away the \$5,000 cards too.

\medskip
{\bf Who Wants To Be a Millionaire?}

\begin{important}
Here is a typical clip from Who Wants To Be a Millionaire:
\urlc{https://www.youtube.com/watch?v=sTGxOqp3qB8}
\end{important}

The rules in force for this episode were as follows.

\newgame{Who Wants to be a Millionaire?}{
The contestant is provided with a sequence of $15$ trivia questions, each of which is multiple choice with four possible answers.
They are worth an increasing amount of money: 100, 200, 300, 500, and then (in thousands) 1, 2, 4, 6, 16, 32, 64, 125, 250,
500, 1000. (In fact, in this epsiode, the million dollar question was worth \$2,060,000.)

At each stage he is asked a trivia question for the next higher dollar amount. He can choose to answer, or to not answer and to keep
his winnings so far. If he answers correctly, he goes to the next level. 
If he answers incorrectly, the game is over. At the \$1,000 and \$32,000 level his winnings are protected: he is guaranteed of winning
at least that much money. Beyond that, he forfeits any winnings if he ventures an incorrect answer.

He has three `lifelines', each of which may be used exactly once over the course of the game: `50-50', which eliminates
two of the possible answers; `phone a friend', allowing him to call a friend for help; and `ask the audience', allowing him to poll
the audience for their opinion.}

In general we want to ask the following question:

\medskip
{\bf Question.} The contestant is at level $x$, and (after using any applicable lifelines) estimates that he has a probability 
$\delta$ of answering correctly. Should he guess or not?

\medskip
Let us assume that $x \geq 32000$ (that's the interesting part of the show). Note that if $x = 32000$, he should always guess
since he is risking nothing.

Suppose then that $x = 64000$, and for now we'll consider {\itshape only the next question}. We will work with $\delta$ as a variable,
and so our answer will be of the form `He should guess if he believes his probability of answering correctly is greater than
[something].' His winnings will be $32000$ if he is incorrect and $125000$ if he is right; and these events have probability 
$1 - \delta$ and $\delta$ respectively. Therefore, the expected value of guessing is
\[
(1 - \delta) \cdot 32000 + \delta \cdot 125000 = 32000 + \delta \cdot 93000.
\]
When is this greater than $64000$? We solve the inequality $32000 + 93000 \delta > 64000$, which is equivalent
to $93000 \delta > 32000$, or $\delta > \frac{32000}{93000} = \frac{32}{93}$. This is a little bit bigger than
$\frac{1}{3}$. So, random guessing would hurt the contestant, but if (for example) he can eliminate two of the answers,
it makes sense for him to guess.

\medskip

At the level $x = 125000$, our computations are similar. This time we have to solve the inequality
\[
32000 + \delta \cdot (250000 - 32000) > 125000,
\]
which is equivalent to $\delta > \frac{93}{218}$. This is bigger, which makes sense: proportionally he is risking more --
he would go down two levels, rather than just one.

\medskip
Of course, working with only one question at a time is a little bit misleading. For example, consider the \$125,000 question.
Even after phoning a friend (and using the last of his lifelines), he has no idea. If he will only go one more question,
it is clearly correct to walk, but {\itshape what if the \$250,000 question is something he definitely knows?}

\medskip Let us go one step further in our analysis (and you can see how to do still better). Suppose that the contestant
estimates that there is a 40\% chance that the \$250,000 question is one he will know the answer to. If he does, he will
guess it correctly, quit the next turn, and walk away with \$500,000. If he doesn't, he won't venture a guess and will walk away
with \$500,000.

In this case, reaching the \$250,000 level is worth
\[
0.4 \times 500000 + 0.6 \times 250000 = 350000.
\]
So the contestant is risking \$93,000 to win another \$225,000. The expected value of guessing is
\[
(1 - \delta) \cdot 32000 + \delta \cdot 350000 = 32000 + \delta \cdot 318000,
\]
and our inequality is 
\[
32000 + 318000 \delta > 125000,
\]
which is equivalent to $\delta > \frac{93}{318}$. In this case it still doesn't make sense for him to randomly
guess, but if his guess is even slightly better than random it does. (Moreover, if the contestant estimates that
there is a small chance that he would know the answer to the \$500,000 question, this would mean that even a random
guess was called for.)

\subsection{Linearity of expectation}

\begin{example} You roll two dice and win a dollar amount equal to the sum of your die rolls. Compute the expected value of this game.
\end{example}

\soln ({\bf Hard Solution}). The possible outcomes and the probabilities of each are listed in the table below.
\begin{center}
\begin{tabular}{c | c | c|c|c |c|c|c|c|c|c}
2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
\hline
$\frac{1}{36}$ &
$\frac{2}{36}$ &
$\frac{3}{36}$ &
$\frac{4}{36}$ &
$\frac{5}{36}$ &
$\frac{6}{36}$ &
$\frac{5}{36}$ &
$\frac{4}{36}$ &
$\frac{3}{36}$ &
$\frac{2}{36}$ &
$\frac{1}{36}$
\end{tabular}
\end{center}
\medskip

The expected value is therefore
\begin{multline*}
2 \times \frac{1}{36} + 
3 \times \frac{2}{36} + 
4 \times \frac{3}{36} + 
5 \times \frac{4}{36} + 
6 \times \frac{5}{36} + 
7 \times \frac{6}{36} + 
8 \times \frac{5}{36} + 
9 \times \frac{4}{36} + 
10 \times \frac{3}{36} + 
11 \times \frac{2}{36} + 
12 \times \frac{1}{36} \\
= \frac{2 + 6 + 12 + 20 + 30 + 42 + 40 + 36 + 30 + 22 + 12}{36} = \frac{252}{36} = 7,
\end{multline*}
or exactly $7$ dollars.

\medskip
You should always be suspicious when you do a messy computation and get a simple result.

\soln ({\bf Easy Solution}). If you roll one die and get the dollar amount showing, we already computed that the expected value
of this game is $3.5$. 

The game discussed now is equivalent to playing this game twice. So the expected value is $3.5 \times 2 = 7$.

\medskip
Similarly, the expected value of throwing a thousand dice and winning a dollar amount equal to the number of pips showing 
is (exactly) \$3,500.

Here is another problem that illustrates the same principle.

\begin{example} Consider once again the game of Rat Race. Suppose that our contestant gets to pick two out of five rats,
that first place wins a car (worth \$16,000), that second place wins meal service (worth \$2,000) and that third place wins a guitar
(worth \$500).
\end{example}

The hard solution would be to compute the probability of every possible outcome: the contestant wins the car and the meals, the
car and the guitar, the guitar and the meals, the car only, the meals only, the guitar only, and nothing. 
What a mess!!! Instead, we'll give an easier solution.

\soln Consider only the first of the contestant's rats. Since this rat will win each of the three prizes for the contestant with probability $\frac{1}{5}$,
the expected value of this rat's winnings is
\[
16000 \times \frac15 + 2000 \times \frac15 + 500 \times \frac15 = 3700.
\]
The second rat is subject to the same rules, so the expected value of its winnings is also \$3700. Therefore, the total expected value is
$\$3,700 + \$3,700 = \$7,400.$

\medskip
Indeed, the expected value of the game is \$3,700 per rat won, so this computation gives the answer no matter how many rats she wins.

\medskip
There is a subtlety going on in this example, which is noteworthy because we {\bf didn't} worry about it. Suppose, for example, that the first rat fails to
even move from the starting line. It is a colossal zonk for the contestant, who must pin all of her hopes on her one remaining rat. Does this mean
that her expected value plummets to \$3,700? {\itshape No!} It now has a one in {\itshape four} chance of winning each of the three remaining prizes,
so its expected value is now
\[
16000 \times \frac14 + 2000 \times \frac14 + 500 \times \frac14 = 4625.
\]
Conversely, suppose that this rat races out from the starting block like Usain Bolt, and wins the car! Then the expected value of the remaining rat
goes {\itshape down}. (It has to: the car is off the table, and the most it can win is \$2,000.) Its expected value is a measly
\[
2000 \times \frac14 + 500 \times \frac14 = 625.
\]
This looks terribly complicated, because {\bf the outcomes of the two rats} are independent. If the first rat does poorly, the second rat
is more likely to do well, and vice versa.

The principle of {\bf linearity of expectation} says that our previous computation {\bf is correct, even though the outcomes are not
independent.} If the first rat wins the car, the second rat's expected value goes down; if the first rat loses or wins a small prize, 
the second rat's expected value goes up; and these possibilities average out. 

\medskip
\begin{important}
{\bf Principle of Linearity of Expectation.} Suppose that we have a random process which can be broken up into two or more separate processes.
Then, the total expected value is equal to the sum of the expected values of the smaller processes.

This is true whether or not the smaller processes are independent of each other.
\end{important}

\medskip
Often, games can be broken up in multiple ways. In the exercises you will redo the Rat Race computation a different way:
you will consider the expected value of winning just the car, just the meals, and just the guitar -- and you will verify that you again
get the same answer.

\medskip
We can now compute the expected value of Rat Race as a whole! Recall that Rat Race begins with the contestant attempting to price
three small items correctly, and winning one rat for each item that she gets right.

\begin{example}
Assume for each small item, the contestant has a 50-50 chance of pricing it correctly. Compute the expected value of playing of Rat Race.
\end{example}

\soln
Recall from your homework exercises that the probability of winning zero, one, two, or three rats is $\frac{1}{8}$, 
$\frac{3}{8}$, $\frac38$, and $\frac18$. Since the expected value of Rat Race is \$3.700 per rat won, the expected value
of the race is respectively \$0, \$3,700, \$7,400, and \$11,100. Therefore the expected value of Rat Race is
\[
0 \times \frac18 + 3700 \times \frac38 + 7400 \times \frac38 + 11000 \times \frac38 = 5550.
\]

\medskip

This solution is perfectly correct, but it misses a shortcut. We can use linearity of expectation again!

\soln
Each attempt to win a small item has probability $\frac12$ of winning a rat, which contributes \$3,700 to the expected value.
Therefore the expected value of each attempt is $3700 \times \frac12 = 1850$. By linearity of expectation,
the expected value of three attempts is 
\[1850 + 1850 + 1850 = 5550.
\]
\medskip

\subsection{A further expected value example}

{\bf The St. Petersburg Paradox.} You play a game as follows. You start with \$2, and you play the following game.
You flip a coin. If it comes up tails, then you win the \$2. If it comes up heads, then your stake is doubled and you get
to flip again. You keep flipping the coin, and doubling the stake for every flip of heads, until eventually you flip tails and the game ends.

How much should you be willing to pay to play this game?

\medskip
To say the same thing another way, your winnings depend on the number of consecutive heads you flip. If none, you win \$2; if one,
you win \$4; if two, you win \$8, and so on. More generally, if you flip $k$ consecutive heads before flipping tails, you win
$2^{k + 1}$ dollars. Unlike most game shows, you never risk anything and so you will certainly continue flipping until you flip tails.

We first compute the probability of every possible outcome:
\begin{itemize}
\item
With probability $\frac{1}{2}$, you flip tails on the first flip and win \$2.
\item
With probability $\frac{1}{4}$, you flip heads on the first flip and tails on the second flip: the probability for each is $\frac{1}{2}$
and you multiply them. If this happens, you win \$4.
\item
With probability $\frac{1}{8}$, you flip heads on the first two flips and tails on the third flip: the probability for each is $\frac{1}{2}$
so the probability is $\Big( \frac{1}{2} \Big)^3$. If this happens, you win \$8.
\item
Now, we'll handle all the remaining cases at once. Let $k$ be the number of consecutive heads you flip before flipping a tail.
Then, the probability of this outcome is $\Big( \frac{1}{2} \Big)^{k + 1}$: we've made $k + 1$ flips and specified the result for each
of them.

Your winnings will be $2^{k + 1}$ dollars: you start with \$2, and you double your winnings for each of the heads you flipped.
\end{itemize}
We now compute the expected value of this game. This time there are infinitely many possible outcomes, but we do the computation 
in the same way. We multiply the probabilities by the expected winnings above, and add:
\[
\$ 2 \cdot \frac{1}{2} + 
\$ 4 \cdot \frac{1}{4} + 
\$ 8 \cdot \frac{1}{8} + 
\$ 16 \cdot \frac{1}{16} + \cdots
= \$1 + \$1 + \$1 + \$1 + \cdots = \infty
\]
{\itshape The expected value of the game is infinite, and you should be willing to pay an infinite amount of money to play it.}
This does not seem to make sense.

\medskip
By contrast, consider the following version of the game. It has the same rules, only the game has a maximum of 100 flips.
If you flip 100 heads, then you don't get to keep playing, and you're forced to settle for $2^{101}$ dollars, that is, \$2,535,301,200,456,458,802,993,406,410,752.

The expected value of {\itshape this} game is a mere
\[
\$ 2 \cdot \frac{1}{2} + 
\$ 4 \cdot \frac{1}{4} + 
\$ 8 \cdot \frac{1}{8} + 
\$ 16 \cdot \frac{1}{16} + \cdots + \$ 2^{100} \cdot \frac{1}{2^{100}} + 
\$ 2^{101} \cdot \frac{1}{2^{100}}
= \$1 + \$1 + \$1 + \$1 + \cdots + \$1 + \$2 = \$102.
\]
Now think about it. If you won the maximum prize, and it was offered to you in \$100 bills, it would weigh\footnote{more precisely: have a mass of}
$2.5 \times 10^{25}$ kilograms, in comparison to the weight of the earth which is only $6 \times 10^{24}$ kilograms. If you stacked them, you could reach
any object which has been observed anywhere in the universe.

Conversely, suppose that it were offered to you in \$100,000,000,000,000 (100 trillion) dollar bills\footnote{Such banknotes were
actually printed in Zimbabwe. See, for example, \url{https://en.wikipedia.org/wiki/Zimbabwean_dollar}.} This is much more realistic to imagine;
it's roughly equivalent to the Himalayan mountain range being made of such banknotes, all of which belong to you. If you went out to lunch, you'd probably
leave a generous tip. You'd have to, because it's not like they can make change for you.

\medskip
{\itshape In summary:} {\bf This is obviously ridiculous.} You can read more on the Wikipedia article, but the point is that {\itshape the real-life
meaning of expected values can be distorted by extremely large, and extremely improbable, events.} 

\subsection{Exercises}

\begin{enumerate}
\item
Watch the Deal or No Deal clip from the introduction. Fast forward through all the talk and choosing briefcases if you like,
but pay attention to each time the bank offers him a buyout to quit. Compute, in each case, the expected value of playing
the game out until the end. Does the bank ever offer a payout larger than the expected value?

What would you decide at each stage? Explain.

\item
Consider again a game of Rat Race with two rats, played for prizes worth \$16,000 (car), \$2,000 (meals), and \$500 (guitar).
\begin{enumerate}
\item
Compute the expected value of the game, considering only the car and ignoring the other prizes. (This should be easy:
she has a 2 in 5 chance of winning the car.) 
\item
Compute the expected value of the game, considering only the meals.
\item
Compute the expected value of the game, considering only the guitar.
\item
By linearity of expectation, the expected value of the game is equal to the sum of the three expected values you just computed.
Verify that this sum is equal to \$7,400, as we computed before.
\end{enumerate}

\begin{important}
The next questions concern the Price is Right game {\bf Let 'em Roll}. Here is a clip:

\urlc{https://www.youtube.com/watch?v=g5qF-W9cSpo}
\end{important}

\newgame{Let 'em Roll (Price Is Right)}{

The contestant has five dice to roll. Each die has \$500 on one side, \$1,000 on another,
\$1,500 on a third, and a car symbol on the other three. The contestant rolls all five dice. If a car symbol is showing on
each of them, she wins the car. Otherwise, she wins the total amount of money showing. (Car symbols count nothing,
unless she wins the car.)

By default, the contestant gets one roll, and may earn up to two more by correctly pricing small grocery items. After each roll,
if she gets another roll, she may either keep all the money showing, or set the dice showing `car' aside and reroll only the rest.} 

\item First, consider a game of Let 'em Roll where the contestant only gets one dice roll.

\begin{enumerate}
\item Compute the probability that she wins the car.
\item Compute the expected value of the game, considering the car and ignoring the money. (The announcer
says that the car is worth \$16,570.)
\item Compute the expected value of the game, considering the money and ignoring the car.
\item Compute the total expected value of the game.
\end{enumerate}

{\bf Solution.} The probability that she wins the car is $(\frac{1}{2})^5 = \frac{1}{32}$: there are five dice,
and each must show a car.

Considering only the car, the expected value of the game is $\frac{1}{32} \times 16570 \sim \$518$.

Considering only the money, each die contributes an expected value of
\[
\frac16 \times 500 +
\frac16 \times 1000 +
\frac16 \times 1500 = 500.
\]
Since there are five dice, the total is \$2500, and the total (including both car and dice) is \$3018.

\item 
\begin{enumerate}
\item
Now watch the contestant's playing of the game, where after the second round she chooses to give up \$2,500 and reroll.
Compute the expected value of doing so. Do you agree with her decision?

\item Suppose that after two turns she had rolled no car symbols, and \$1,500 was showing on each of the five dice.
Compute the expected value of rerolling, and explain why she should {\itshape not} reroll.

\item Construct a hypothetical situation where the expected value of rerolling is within \$500 of not rerolling, so that
the decision to reroll is nearly a tossup.
\end{enumerate}

{\bf Solution.} After her second round, she has three cars (which she would keep if she rerolls) and \$2,500. If she
rerolls, she has a one in four probability of winning the car, so her expected value from the car is $\frac14 \times 16570 \sim 4142.$
She also obtains an additional expected value of \$1000 from the money, for a total of \$5142. As this is much larger than
\$2,500, rerolling is a good idea if she can stomach some risk.

In the second scenario, the expected value is the same as the one-turn version (because she will reroll everything):
\$3,018. Since this is much less than \$7,500, it is a good idea to keep the money.

Here is an intermediate scenario. Suppose two cars are showing and she rerolls the other three dice. Then the
expected value of the game is
\[
\frac18 \times 16570 + 3 \times 500 \sim 3571.
\]
So if the three money dice are showing a total of \$3,500, it is essentially a tossup decision whether or not to reroll.

As another correct solution, suppose only one car is showing and she rerolls the other four.
\[
\frac1{16} \times 16570 + 4 \times 500 \sim 3035.
\]
If the four money dice are showing \$3000 total, once again it is approximately a tossup.

Yet another correct solution has no cars showing and low amounts of money on the dice: a total of either \$2500 or
\$3000.

\item If the contestant prices the small grocery items correctly and plays optimally,
compute the expected value of a game of Let 'em Roll.

(Warning: if your solution is simple, then it's wrong.)
\end{enumerate}

\section{Counting}
We now consider a variety of clever counting methods, which will be useful in sophisticated probability computations.

\subsection{The Multiplication Rule}
Just as there was a multiplication rule for probability, there is a multiplication rule for counting as well. It is as follows.\footnote{We adopt the wording of
Epp, {\itshape Discrete Mathematics with Applications}, 4th ed., p. 527.}

\medskip
\begin{important}
{\bf The multiplication rule for counting.} Suppose that an operation consists of $k$ steps, and:
\begin{itemize}
\item
The first step can be performed in $n_1$ ways;
\item
The second step can be performed in $n_2$ ways (regardless of how the first step was performed);
\item 
and so on. Finally the $k$th step can be performed in $n_k$ ways (regardless of how the preceding steps were performed).
\end{itemize}
Then the entire operation can be performed in $n_1 n_2 \dots n_k$ ways.
\end{important}

\begin{example} In South Carolina, a license tag can consist of any three letters followed by any three numbers. 
(Example: TPQ-909) How many different license tags are possible?
\end{example}
\soln There are $26$ possibilities for the first letter, $26$ for the second, and $26$ for the third. Similarly there are $10$ possibilities
for each number. So the total number of possibilities is $26^3 \cdot 10^3 = 17576000$.

\medskip
Note that big states with more people than South Carolina have started using different license plate schemes, because they ran out of possible tags.

\begin{example} How many license tags are possible which don't repeat any letters or numbers?
\end{example}
\soln There are still $26$ possibilities for the first letter, and now $25$ for the second and $24$ for the third: we must avoid the letters
that were previously used. Similarly there are $10$, $9$, and $8$ possibilities for the three numbers. The total number of possibilities is
\[
26 \cdot 25 \cdot 24 \cdot 10 \cdot 9 \cdot 8 = 11232000.
\]

These computations may be used to solve probability questions. For example:
\begin{example} What is the probability that a random license tag doesn't repeat any letters or numbers?
\end{example}
This follows from the previous two computations. The result is
\[
\frac{11232000}{17576000 } = .639 \dots
\]

\begin{example} On a game of Ten Chances, Drew Carey feels particularly sadistic and puts all ten digits -- zero through nine -- to choose from in the price
of the car. The price of the car consists of five different digits. How many possibilities are there?
\end{example}
\soln
There are $10$ possibilities for the first digit, $9$ for the second, $8$ for the third, $7$ for the fourth, and $6$ for the fifth, for a total of
\[
10 \cdot 9 \cdot 8 \cdot 7 \cdot 6 = 30240
\]
possibilities. Good luck to the poor sucker playing this game.

\begin{example} As above, but suppose you know that the first digit is not zero. Now how many possibilities are there?
\end{example}
\soln
This time there are only $9$ possibilities for the first digit. There are still $9$ possibilities for the second, no matter what the first digit was,
and $8$, $7$, $6$ for the last three in turn. The total is
\[
9 \cdot 9 \cdot 8 \cdot 7 \cdot 6= 27216.
\]

\begin{example} As above, but suppose you know that the first digit is not zero {\bf and} that the last digit is zero. Now how many possibilities are there?
\end{example}
\soln 
There are $9$ possibilties for the first digit, $9$ for the second, $8$ for the third, $7$ for the fourth, and ... {\bf either $0$ or $1$ for the last
depending on whether we've used the zero.} No good! We can't use the multiplication rule this way!

To use the multiplication rule, we pick the numbers in a different order: the first digit first (anything other than the zero, $9$ ways),
then the last digit (must be the zero, so $1$ way), and then the second, third, and fourth digits in turn ($8$, $7$, and $6$ ways), for a total 
of 
\[
9 \cdot 8 \cdot 7 \cdot 6 = 3024
\]
ways.

Alternatively, we could have picked the last digit before the first, and we can pick the second, third, and fourth digits in any order. It is usually
best to find one order which works and stick to it.

\subsection{Permutations and combinations}
Recall that a {\bf permutation} of a {\bf string with $n$ symbols} is any reordering of the string. If the symbols are all distinct, then there are $n!$
possible permutations of it. We justified this earlier, and it is an example of the multiplication rule: there are $n$ ways to choose the first symbol,
$n - 1$ to choose the second, $n -2$ to choose the third, and so on.

Implicitly, we also discussed what are called $r$-permutations. If $r \leq n$, then an $r$-permutation of a string of length $n$ is a reordering of
$r$ of the $n$ symbols. For example, $16820$, $98561$, and $37682$ are $5$-permutations of the string $1234567890$. We discussed these
in our Ten Chances examples above, and there are $30240$.

\medskip

\begin{important}
{\bf Notation.} Write $P(n, r)$ for the number of $r$-permutations of a string with $n$ distinct symbols.

We have the following formula:
\[
P(n, r) = \frac{ n! }{ (n - r)! }
\]
\end{important}

Why is this true? It comes from the multiplication rule. There are $n$ possibilities for the first symbol, $n - 1$ possibilities for the second, and so on:
one less for each subsequent symbol. There are $n - r + 1$ possibilities for the $r$th symbol: we start at $n$ and count down by $1$ $r - 1$ times.
So we see that
\[
P(n, r) = n \cdot (n - 1) \cdot (n - 2) \cdot (n - 3) \cdots (n - r + 1).
\]
Why is this equal to $\frac{n!}{(n - r)!}$? Our expression is the same as $n!$, except that the numbers from $n - r$ down to $1$ are all absent.
So we've left out a product equalling $(n - r)!$ from the definition of $n!$, and so it equals $\frac{n!}{(n - r)!}$.

\medskip
\begin{example} Compute $P(n, r)$ for all possible $n$ and $r$ with $r \leq n \leq 6$ and notice any patterns.
\end{example}
\soln (To be written up here)

\medskip
{\bf Combinations.} Combinations are like permutations, only the order doesn't matter. 

\begin{important}
If we start with a string (or a set) with $n$ distinct elements,
then an {\bf $r$-combination} is a string or $r$ of these elements {\itshape where order doesn't matter}, or equivalently a subset of $r$ of these elements.
\end{important}

\begin{example} Write out all the $3$-combinations of $12345$.
\end{example}
\soln
They are: $123$, $124$, $125$, $134$, $135$, $145$, $234$, $235$, $245$, and $345$. There are ten of them.

Here, we could have equivalently written $321$, $213$, or $\{ 1, 2, 3\}$ (for example) in place of $123$, because when counting {\bf combinations}
it is irrelevant which order the symbols come in. When counting permutations it {\bf is} relevant, so {\bf please always be careful to pay attention
to exactly what you are counting!}

Note that a string with $n$ distinct elements, where order doesn't matter, is the same thing as a set of $n$ distinct elements. We won't worry
about distinguishing these too carefully, although in advanced mathematics and in computer programming it is important to be precise.

\begin{example} Write out all the $2$-combinations of $12345$.
\end{example}
\soln
They are: $45$, $35$, $34$, $25$, $24$, $23$, $15$, $14$, $13$, and $12$. Again, there are ten of them.

I didn't have to list them in reverse order, but in doing so we notice something interesting: they correspond exactly to the $3$-combinations!
Choosing which two elements to include is equivalent to choosing which three to leave out, so we can line up the list of $2$-combinations
with the list of $3$-combinations and see that there is a one-to-one correspondence. In mathematical parlance, we call this a {\bf bijection.}
{\itshape If you want to prove that two sets have the same size, finding a bijection between them is a great way to do it!}

\medskip
\begin{important}
{\bf Notation.} Write $C(n, r)$ or ${n \choose r}$ for the number of $r$-combinations of an $n$-element set.
\end{important}

\medskip
The latter notation is read ``$n$ choose $r$'', and is ubiquitous in mathematics. These numbers are also called
`binomial coefficients', because we have
\[
(x + 1)^n = {n \choose n} x^n + {n \choose {n - 1}} x^{n - 1} + {n \choose {n - 2}} x^{n - 2} + \cdots + 
 {n \choose 1} x
 + 
 {n \choose 0}.
 \]
 For example, we have
\[
(x + 1)^10 = x^{10} + 10 x^9 + 45 x^8 + 120 x^7 + 210 x^6 + 252 x^5 + 210 x^4 + 120 x^3 + 45 x^2 + 10 x + 1,
\]
so we can FOIL without FOILing. If you think about it carefully, you can figure out why the first equation is true. 
But we still haven't explained how to actually {\itshape compute} these things. Here's the answer.

\begin{important}
\begin{theorem}
We have
\[
C(n, r) = {n \choose r} = \frac{n!}{r! (n - r)!}.
\]
\end{theorem}
\end{important}
To explain thiis we will be very careful and work backwards. First of all, note that it is enough to show
that 
\begin{equation}\label{eqn:perm}
P(n, r) = C(n, r) \cdot r!
\end{equation}
We will first explain why \eqref{eqn:perm} implies the theorem, and then we will explain why  \eqref{eqn:perm} is true.
First, note that \eqref{eqn:perm} implies that
\[
C(n, r) = \frac{P(n, r)}{r!},
\]
but remember that we showed that $P(n, r) = \frac{n!}{(n - r)!}$. Therefore,
\[
C(n, r) = \frac{ \frac{n!}{(n - r)!}}{r!} = \frac{n!}{(n - r)! r!},
\]
as desired. 

We are left to explain why \label{eqn:perm} is true. To do this, we explain why both sides of \eqref{eqn:perm}
count the number of $r$-permutations of a string of $n$ elements:
\begin{itemize}
\item
This is true of $P(n, r)$ by definition.
\item
Instead, we could first choose which $r$ objects to make an $r$-permutation out of, without worrying about the order.
By definition, there are $C(n, r)$ ways to do this. Now, we have to put these $r$ symbols in some order -- i.e., to write
down a permutation of them. There are $r!$ ways to do this. So the total number of ways is $C(n, r) \cdot r!$
\end{itemize}

{\bf If you haven't seen this before, you probably didn't understand what just happened.} That's okay. Read through it again.

\subsection{Pascal's Triangle}

{\bf {\itshape The part of this section with the diagrams has been removed from this version of the notes. It will again be in the final version -- 
I removed it here because it is much faster to edit the file if I don't include the diagrams. Please see (for example) the September 30 version
of the file to see this part of these notes.}}

\begin{comment}
\begin{important}
Here is a video of the Price Is Right game {\bf Plinko}:
\urlc{https://www.youtube.com/watch?v=qr7oYqcgsXQ}
\end{important}

\newgame{Plinko (The Price Is Right)}{The contestant drops up to five chips down a board. (She starts off with one,
and can win up to four more by pricing small items.) She drops them down a board which has a lot of pegs and a variety of prizes
at the bottom. (The shape of the board {\bf is} relevant, and we will discuss it more in due course.) She hopes to land her chips into
a \$10,000 slot in the middle, and the other slots have prizes between zero and \$1,000.}

The question is, {\bf where should the contestant drop her pucks}?

\medskip
Here is a graphical representation of a Plinko board. 

\vskip 0.5in

\begin{tikzpicture}
  \foreach \r in {1, 2,...,7}
  \foreach \a in {1, 2,...,10}
     \draw (\a - 0.5, 2*\r) circle (0.05);    
  \foreach \r in {1, 2,...,6}
  \foreach \a in {1, 2,...,9}
     \draw (\a, 2*\r +1) circle (0.05);    
   \node at (1, 1.2) {1};
   \node at (2, 1.2) {5};
   \node at (3, 1.2) {10};
   \node at (4, 1.2) {0};
   \node at (5, 1.2) {100};
   \node at (6, 1.2) {0};
     \node at (7, 1.2) {10};
   \node at (8, 1.2) {5};
   \node at (9, 1.2) {1};

\end{tikzpicture}


The circles represent pegs. Also present, but not pictured, are walls on the left and right sides of the board,
so that the puck can't go off the board. The contestant chooses one of the seven spaces on the top and drops her puck down.
Eventually it falls into one of the nine slots at the bottom. The dollar amounts are given in hundreds in the schematic
representation above.

To model this problem we introduce a simplifying assumption: {\bf assume that when the puck hits a peg, it goes
to its immediate left or immediate right, with probability $\frac{1}{2}$ each.} As an exception, if the puck can't go towards
the left, then it always goes to its immediate right, and vice versa.

This assumption is not in fact true: if you watch enough video clips of the show, you will see that sometimes the puck
behaves erratically and skips over pegs. But we will build a simpler mathematical model by ignoring the possibility of
such behavior.

\medskip
Subject to this assumption, we now compute the probability that it lands in any given slot! We assume that the contestant
has dropped it in the third slot from the left.

\vskip 0.5in

\begin{tikzpicture}
  \foreach \r in {1, 2,...,7}
  \foreach \a in {1, 2,...,10}
     \draw (\a - 0.5, 2*\r) circle (0.05);    
  \foreach \r in {1, 2,...,6}
  \foreach \a in {1, 2,...,9}
     \draw (\a, 2*\r +1) circle (0.05);    
   \node at (4, 13.5) {1};
   \node at (3.5, 12.5) {$\frac12$};
   \node at (4.5, 12.5) {$\frac12$};
   \node at (3, 11.5) {$\frac14$};
   \node at (4, 11.5) {$\frac24$};
   \node at (5, 11.5) {$\frac14$};
   \node at (2.5, 10.5) {$\frac18$};
   \node at (3.5, 10.5) {$\frac38$};
   \node at (4.5, 10.5) {$\frac38$};
   \node at (5.5, 10.5) {$\frac18$};
   \node at (2, 9.5) {$\frac1{16}$};
   \node at (3, 9.5) {$\frac4{16}$};
   \node at (4, 9.5) {$\frac6{16}$};
   \node at (5, 9.5) {$\frac4{16}$};
   \node at (6, 9.5) {$\frac1{16}$};
   \node at (1.5, 8.5) {$\frac1{32}$};
   \node at (2.5, 8.5) {$\frac5{32}$};
   \node at (3.5, 8.5) {$\frac{10}{32}$};
   \node at (4.5, 8.5) {$\frac{10}{32}$};
   \node at (5.5, 8.5) {$\frac5{32}$};
   \node at (6.5, 8.5) {$\frac1{32}$};

   \node at (1, 7.5) {$\frac{1}{64}$};
   \node at (2, 7.5) {$\frac{6}{64}$};
   \node at (3, 7.5) {$\frac{15}{64}$};
   \node at (4, 7.5) {$\frac{20}{64}$};
   \node at (5, 7.5) {$\frac{15}{64}$};
   \node at (6, 7.5) {$\frac{6}{64}$};
   \node at (7, 7.5) {$\frac{1}{64}$};
   
   \node at (1, 1.2) {1};
   \node at (2, 1.2) {5};
   \node at (3, 1.2) {10};
   \node at (4, 1.2) {0};
   \node at (5, 1.2) {100};
   \node at (6, 1.2) {0};
     \node at (7, 1.2) {10};
   \node at (8, 1.2) {5};
   \node at (9, 1.2) {1};
\end{tikzpicture}

{\bf Each fraction represents the probability that the puck goes through that spot on the grid.}
We have only filled in approximately the top half of the board (one can keep going).
Whenever a peg is in the `middle' of the board, and the puck can go left or right, we
multiply the probability associated to that peg by $\frac{1}{2}$ and add that to each of the
left and right pegs beneath it. In particular, {\bf each fraction} will be the sum of the two
numbers above it.

Whenever a peg is on the left edge of the board, we simply add that probability to the right
peg below it, because it is forced to go there. 

Notice the second row consists of fractions over $2$, the third row of fractions over $4$, etc. To make it simpler to keep
track of everything, we redo this work but with the $(n + 1)$th row multiplied by $2^n$. In simpler terms, we multiply
all the fractions by their common denominator.

\vskip 0.5in

\begin{tikzpicture}
  \foreach \r in {1, 2,...,7}
  \foreach \a in {1, 2,...,10}
     \draw (\a - 0.5, 2*\r) circle (0.05);    
  \foreach \r in {1, 2,...,6}
  \foreach \a in {1, 2,...,9}
     \draw (\a, 2*\r +1) circle (0.05);    
   \node at (4, 13.5) {1};
   \node at (3.5, 12.5) {1};
   \node at (4.5, 12.5) {1};
   \node at (3, 11.5) {1};
   \node at (4, 11.5) {2};
   \node at (5, 11.5) {1};
   \node at (2.5, 10.5) {1};
   \node at (3.5, 10.5) {3};
   \node at (4.5, 10.5) {3};
   \node at (5.5, 10.5) {1};
   \node at (2, 9.5) {1};
   \node at (3, 9.5) {4};
   \node at (4, 9.5) {6};
   \node at (5, 9.5) {4};
   \node at (6, 9.5) {1};
   \node at (1.5, 8.5) {1};
   \node at (2.5, 8.5) {5};
   \node at (3.5, 8.5) {10};
   \node at (4.5, 8.5) {10};
   \node at (5.5, 8.5) {5};
   \node at (6.5, 8.5) {1};

   \node at (1, 7.5) {1};
   \node at (2, 7.5) {6};
   \node at (3, 7.5) {15};
   \node at (4, 7.5) {20};
   \node at (5, 7.5) {15};
   \node at (6, 7.5) {6};
   \node at (7, 7.5) {1};

   \node at (1.5, 6.5) {8};
   \node at (2.5, 6.5) {21};
   \node at (3.5, 6.5) {35};
   \node at (4.5, 6.5) {35};
   \node at (5.5, 6.5) {21};
   \node at (6.5, 6.5) {7};
   \node at (7.5, 6.5) {1};
   
   
   \node at (1, 5.5) {8};
   \node at (2, 5.5) {29};
   \node at (3, 5.5) {56};
   \node at (4, 5.5) {70};
   \node at (5, 5.5) {56};
   \node at (6, 5.5) {28};
   \node at (7, 5.5) {8};
   \node at (8, 5.5) {1};
   
   
   \node at (1.5, 4.5) {45};
   \node at (2.5, 4.5) {85};
   \node at (3.5, 4.5) {126};
   \node at (4.5, 4.5) {126};
   \node at (5.5, 4.5) {84};
   \node at (6.5, 4.5) {36};
   \node at (7.5, 4.5) {9};
   \node at (8.5, 4.5) {1};
   
      \node at (1, 3.5) {45};
      \node at (2, 3.5) {130};
   \node at (3, 3.5) {211};
   \node at (4, 3.5) {252};
   \node at (5, 3.5) {210};
   \node at (6, 3.5) {120};
   \node at (7, 3.5) {45};
   \node at (8, 3.5) {10};
   \node at (9, 3.5) {1};
   
   \node at (1.5, 2.5) {220};
   \node at (2.5, 2.5) {341};
   \node at (3.5, 2.5) {463};
   \node at (4.5, 2.5) {462};
   \node at (5.5, 2.5) {330};
   \node at (6.5, 2.5) {165};
   \node at (7.5, 2.5) {55};
   \node at (8.5, 2.5) {12};

      \node at (1, 1.5) {220};
      \node at (2, 1.5) {561};
   \node at (3, 1.5) {804};
   \node at (4, 1.5) {925};
   \node at (5, 1.5) {792};
   \node at (6, 1.5) {495};
   \node at (7, 1.5) {220};
   \node at (8, 1.5) {67};
   \node at (9, 1.5) {12};
      
   \node at (1, 0.2) {1};
   \node at (2, 0.2) {5};
   \node at (3, 0.2) {10};
   \node at (4, 0.2) {0};
   \node at (5, 0.2) {100};
   \node at (6, 0.2) {0};
     \node at (7, 0.2) {10};
   \node at (8, 0.2) {5};
   \node at (9, 0.2) {1};
\end{tikzpicture}

The total of the bottom row is $2^{12} = 4096$. (This is how I checked my arithmetic!) By the way we constructed the table,
the sum of each row is twice the sum of the rows above it.

{\itshape Notice that the most likely row is the row directly below where you dropped the puck.} You have a
$\frac{792}{4096}$ probability of landing your puck in the \$10,000 slot, and you can as an exercise figure the expected value
of this puck drop.

Notice also that this is nearly symmetric -- indeed it {\itshape is} symmetric until we run into the barriers of the game.
Since we would like to see this symmetry, we will make one further simplifying assumption: {\itshape the walls
of the game don't exist.} We assume that the gameboard keeps going off infinitely far to the left and the right.
In reality, if the puck hits the walls, it has a long way to get back to the center, and so this doesn't make a very large
difference anyway. (Of course it {\itshape does} make some difference -- the mathematical model we're developing is
not perfect.)

We now drop the pegs from the picture and just list the numbers.

\vskip 0.5in

\begin{tikzpicture}
   \node at (4, 13.5) {1};
   \node at (3.5, 12.5) {1};
   \node at (4.5, 12.5) {1};
   \node at (3, 11.5) {1};
   \node at (4, 11.5) {2};
   \node at (5, 11.5) {1};
   \node at (2.5, 10.5) {1};
   \node at (3.5, 10.5) {3};
   \node at (4.5, 10.5) {3};
   \node at (5.5, 10.5) {1};
   \node at (2, 9.5) {1};
   \node at (3, 9.5) {4};
   \node at (4, 9.5) {6};
   \node at (5, 9.5) {4};
   \node at (6, 9.5) {1};
   \node at (1.5, 8.5) {1};
   \node at (2.5, 8.5) {5};
   \node at (3.5, 8.5) {10};
   \node at (4.5, 8.5) {10};
   \node at (5.5, 8.5) {5};
   \node at (6.5, 8.5) {1};

   \node at (1, 7.5) {1};
   \node at (2, 7.5) {6};
   \node at (3, 7.5) {15};
   \node at (4, 7.5) {20};
   \node at (5, 7.5) {15};
   \node at (6, 7.5) {6};
   \node at (7, 7.5) {1};


   \node at (0.5, 6.5) {1};
   \node at (1.5, 6.5) {7};
   \node at (2.5, 6.5) {21};
   \node at (3.5, 6.5) {35};
   \node at (4.5, 6.5) {35};
   \node at (5.5, 6.5) {21};
   \node at (6.5, 6.5) {7};
   \node at (7.5, 6.5) {1};
   
   \node at (0, 5.5) {1};
   \node at (1, 5.5) {8};   
   \node at (2, 5.5) {28};
   \node at (3, 5.5) {56};
   \node at (4, 5.5) {70};
   \node at (5, 5.5) {56};
   \node at (6, 5.5) {28};
   \node at (7, 5.5) {8};
   \node at (8, 5.5) {1};
   
   \node at (-0.5, 4.5) {1};
   \node at (0.5, 4.5) {9};   
   \node at (1.5, 4.5) {36};
   \node at (2.5, 4.5) {84};
   \node at (3.5, 4.5) {126};
   \node at (4.5, 4.5) {126};
   \node at (5.5, 4.5) {84};
   \node at (6.5, 4.5) {36};
   \node at (7.5, 4.5) {9};
   \node at (8.5, 4.5) {1};
   
      \node at (-1, 3.5) {1};
   \node at (0, 3.5) {10};
   \node at (1, 3.5) {45};
      \node at (2, 3.5) {120};
   \node at (3, 3.5) {210};
   \node at (4, 3.5) {252};
   \node at (5, 3.5) {210};
   \node at (6, 3.5) {120};
   \node at (7, 3.5) {45};
   \node at (8, 3.5) {10};
   \node at (9, 3.5) {1};
   
      \node at (-1.5, 2.5) {1};
   \node at (-0.5, 2.5) {11};
   \node at (0.5, 2.5) {55};

   \node at (1.5, 2.5) {165};
   \node at (2.5, 2.5) {330};
   \node at (3.5, 2.5) {462};
   \node at (4.5, 2.5) {462};
   \node at (5.5, 2.5) {330};
   \node at (6.5, 2.5) {165};
   \node at (7.5, 2.5) {55};
   \node at (8.5, 2.5) {11};
   \node at (9.5, 2.5) {1};

      \node at (-2, 1.5) {1};
      \node at (-1, 1.5) {12};
      \node at (0, 1.5) {66};
      \node at (1, 1.5) {220};
      \node at (2, 1.5) {495};
   \node at (3, 1.5) {792};
   \node at (4, 1.5) {924};
   \node at (5, 1.5) {792};
   \node at (6, 1.5) {495};
   \node at (7, 1.5) {220};
   \node at (8, 1.5) {66};
   \node at (9, 1.5) {12};
   \node at (10, 1.5) {1};

      \node at (1, 0.2) {220};
      \node at (2, 0.2) {561};
   \node at (3, 0.2) {804};
   \node at (4, 0.2) {925};
   \node at (5, 0.2) {792};
   \node at (6, 0.2) {495};
   \node at (7, 0.2) {220};
   \node at (8, 0.2) {67};
   \node at (9, 0.2) {12};
      
   \node at (1, -0.8) {1};
   \node at (2, -0.8) {5};
   \node at (3, -0.8) {10};
   \node at (4, -0.8) {0};
   \node at (5, -0.8) {100};
   \node at (6, -0.8) {0};
     \node at (7, -0.8) {10};
   \node at (8, -0.8) {5};
   \node at (9, -0.8) {1};
\end{tikzpicture}

For comparison's sake we have included the previous numbers (where the board had walls)
below the final numbers. They are different, but not {\itshape so} different.

We have just written out the first thirteen rows of {\bf Pascal's Triangle}.

\end{comment}
\medskip
\begin{important}
{\bf Pascal's Triangle.} To write down Pascal's Triangle, proceed as follows.
\begin{itemize}
\item The top row has a solitary $1$ in it.
\item Each row has one more number than the previous, with a $1$ at each edge.
{\bf Each number in the middle of the table is equal to the sum of the two above it.}
\item Proceed for as many rows as you like.
\item By convention the rows are numbered as follows: the top row is the {\bf zeroth}
row. After that, the rows are numbered $1$, $2$, $3$, etc., and the $n$th row starts
with a $1$ and an $n$.
\end{itemize}

\end{important}

\begin{important}
Our idealized version of Plinko is illustrated nicely by the following computer demonstration:
\urlc{phet.colorado.edu/sims/plinko-probability/plinko-probability_en.html}
\end{important}
We now investigate Pascal's Triangle and observe that it has a large number of
remarkable properties:

\begin{important}
\begin{proposition}
The numbers in the $n$th row of Pascal's Triangle sum to $2^n$.
\end{proposition}
\end{important}
Why is this? It is true for the 1st row, and we see that each number contributes twice
to the row below it: once to its left, and once to its right. Hence, the sum of each row
is twice that of the row above it.

\begin{important}
\begin{proposition}
The numbers in the $n$th row of Pascal's Triangle are $C(n, 0)$, $C(n, 1)$, ...,
$C(n, n)$ in order.
\end{proposition}
\end{important}
This is remarkable! Note that this lets us compute $C(n, r)$ for all $r$ without any
factorials. 

Why is this true? Consider, as an example, the $15$ sitting left of center in the sixth row.
This $15$ counts the number of possible paths a ball could have taken from the top $1$ to this 
$15$.

How many such paths are there? We already saw that the answer is $15$, but let's see the
same thing another way. The ball took six steps down, and of these four were to the left
and two were to the right. Say you label these steps A, B, C, D, E, and F in order. 
Then, for 
each choice of two of these six letters (note: there are $C(6, 2)$ such choices), there
is exactly one path corresponding to that choice. For example, corresponding to the choice
$\{B, D\}$ is the path Left, Right, Left, Right, Left, Left.

The reasoning is the same no matter which number we started with.

\begin{important}
\begin{proposition} We have $C(n, r) = C(n, n - r)$ for all $n$ and $r$.
\end{proposition}
\end{important}
This can be seen from the symmetry in the triangle. It can also be seen as a consequence
of our explicit formula. We have
\[
C(n, r) = \frac{n!}{r! (n - r)!}
\]
and 
\[
C(n, n - r) = \frac{n!}{(n - r)! (n - (n - r))!}
\]
But $n - (n - r)$ is just the same thing as $r$.

\begin{important}
\begin{proposition}
The biggest numbers are always in the middle.
\end{proposition}
\end{important}
If it is true for one row, it has to be true for the row below it (since this row
is made of sums of the row above it). So since it is true at the top, it must always be true.

Note that this tells you how you should drop the puck in Plinko: {\itshape drop it directly
down the middle, right over the \$10,000!}

\begin{important}
\begin{proposition}
We have, for all $n$ and $r$, that
\[
C(n, r) + C(n, r + 1) = C(n + 1, r + 1).
\]
\end{proposition}
\end{important}
This is just a restatement of how Pascal's Triangle is constructed. Since the rows
consist of the quantities $C(n, r)$, we get the above identity.

Note that algebraically, this says that
\[
\frac{ n!}{r! (n - r)!} + \frac{n!}{(r + 1)! (n -(r + 1))!} = 
\frac{(n + 1)!}{(r + 1)! ((n + 1) - (r + 1))!},
\]
which after simplifying a bit is the same thing as
\begin{equation}\label{combi}
\frac{ n!}{r! (n - r)!} + \frac{n!}{(r + 1)! (n - r - 1)!} = 
\frac{(n + 1)!}{(r + 1)! (n - r)!},
\end{equation}
We can also verify this algebraically. A common denominator for the fractions in 
\eqref{combi} is $(r + 1)! (n - r)!$, so the left side of \eqref{combi} is 
\begin{equation}\label{combi}
\frac{ n! (r + 1)}{(r + 1)! (n - r)!} + \frac{n! (n - r)}{(r + 1)! (n - r)!} = 
\frac{ n! (r + 1 + n - r)}{(r + 1)! (n - r)!} = 
\frac{ n! (n + 1)}{(r + 1)! (n - r)!} = 
\frac{ (n + 1)!}{(r + 1)! (n - r)!}.
\end{equation}
But that was a lot less fun.

\begin{important}
\begin{proposition} You can read off a rule for FOILing from Pascal's Triangle.
In particular, you have
\[
(x + y)^n = C(n, 0) x^n + C(n, 1) x^{n - 1} y + C(n, 2) x^{n - 2} y^2 + \cdots + 
C(n, n) y^n.
\]
\end{proposition}
\end{important}
This is called the {\bf binomial theorem}.

For example, plug in $x = 1$ and $y = 1$. You get
\[
2^n = C(n, 0) + C(n, 1) + C(n, 2) + \cdots + C(n, n).
\]
In other words, we see again that the sum of the $n$th row is $2^n$.

\begin{important}
\begin{proposition} 
The alternating sum of each row of Pascal's Triangle (after the zeroth) is $0$.
\end{proposition}
\end{important}
For example, the alternating sum of the seventh row is
\[
1 - 7 + 21 - 35 + 35 - 21 + 7 - 1 = 0.
\]
Here, you can tell immediately that these sum to zero, because the numbers cancel in pairs. But look at the eighth row.
We also have
\[
1 - 8 + 28 - 56 + 70 - 56 + 28 - 8 + 1 = 0.
\]
This is not obvious immediately --- unless you look at the binomial theorem! Plug in $x = 1$ and $y = -1$.
We get 
\[
0 = C(n, 0) - C(n, 1) + C(n, 2) - C(n, 3) + \cdots \pm C(n, n).
\]
The last $\pm$ is a plus if $n$ is even and a minus if $n$ is odd.

\begin{important}
\begin{proposition} 
If you color all the odd numbers blue and the even numbers red, you will create
a familiar pattern called the `Sierpinski triangle' which is a {\bf fractal}.
\end{proposition}
\end{important}
Try it!!

\begin{important}
\begin{proposition}
Suppose you draw lines through Pascal's Triangle at an angle.

For example, start at any of the $1$'s on the left. Circle it.
Then, go over to the right one
and up and right one, and circle that number. Then, 
again go over to the right one
and up and right one and circle that. Keep going until you run out of numbers.

If you add up all the numbers you circled, you get .....
\end{proposition}
\end{important}
What, do you expect me to do it for you? And spoil the surprise? No way. Try it!!
(Try multiple such lines, and see if you can find the pattern.)

\begin{important}
\begin{proposition}
The distribution of Pascal's triangle approaches a nice limit as $n \rightarrow \infty$.
\end{proposition}
\end{important}

This is subtle, and so our explanation here will be a bit vague. Consider the following question.
You flip a fair coin a million times. What is the probability that you get at least $501,000$ heads?

We can give the answer immediately: it is
\[
2^{-1000000} \bigg( C(1000000, 501000) + C(1000000, 501001) + C(1000000, 501002) + \cdots C(1000000, 1000000) \bigg).
\]
But in some sense this is a useless answer. If I asked you whether this was nearer to $20\%$ or $0.0000000000000001\%$, could you
answer just by looking at it? (No.)

\begin{important}
Here is a website which allows you to conduct experiments like this: 
\urlc{http://www.math.uah.edu/stat/apps/BinomialTimelineExperiment.html}
\end{important}
The variable $n$ is the number of coin flips and you do that many flips, many times over. So you can see graphically
the answers to questions like this. What it is important to observe is that {\itshape the shape of the graph is, in some sense
independent of $n$.}

The limiting distribution is called a {\itshape normal} or {\itshape Gaussian} distribution, or more informally {\itshape the bell curve}.
It has {\itshape mean} (i.e., average) $\frac{n}{2}$ and {\itshape standard deviation} $\frac{\sqrt{n}}{2}$. Roughly speaking,
the {\itshape standard deviation} is a measure of how much you might reasonably expect a trial to be off from the mean.

So, for example, if you flip $1,000,000$ coins, the mean outcome is $500,000$ heads, the standard deviation is
$500$, and $501,000$ is two standard deviations away. The probability of this outcome is roughly $2.27\%$ -- unlikely,
but you wouldn't be shocked to see it happen. Conversely, the probability of getting at least $503,000$ heads
is less than a billion.

\subsection{Exercises}

{\bf Incomplete. To be added to.}

\begin{enumerate}
\item
Compute tables of $P(n, r)$ and $C(n, r)$ for all $n$ and $r$ with $0 \leq r \leq n \leq 8$.

\item
Explain why $C(n, 0) = 1$ and $C(n, 1) = n$ for all $n$. Can you explain this using the definition instead of the formula?

\item
The following clip is from the game show {\bf Scrabble}:
\urlc{https://www.youtube.com/watch?v=iliCKnHxJiQ}
\begin{enumerate}
\item
At 6:25 in the video, Michael chooses two from eleven numbered tiles. The order in which he chooses them doesn't matter.
Eight of the tiles are `good', and reveal
letters which are actually in the word. Three of them are `stoppers'.

How many different choices can he make?

{\bf Solution.} $C(11, 2) = 55.$

In this example, it turns out that there are two R tiles, and two D tiles (one of which is a stopper). The easy solution
presumes that these are different from each other -- that one of the numbered tiles is the R appearing first in the word,
and another one is the R appearing second in the word.

However, if you watch the show a lot you will observe this is {\itshape not actually true} -- the {\itshape first} $D$ picked
will always be the good one, and the second will always be the stopper. Our solution is the `easy solution' -- extra
credit to anyone who observed that this is not quite accurate, and took account of it!

\item
Of these choices, how many choices don't contain a stopper? If he places both letters, what is the probability that
both will actually appear in the word?

{\bf Solution.} $C(8, 2) = 28$, and so $\frac{28}{55}$.
\item
Michael can't guess the word and chooses two more of the remaining tiles. Now what is the probability that
both of them will actually appear in the word?

{\bf Solution.} Now there are nine remaining tiles and six of them are good. It's $\frac{C(6, 2)}{C(9, 2)} = \frac{15}{36} = \frac{5}{18}$.
Not very good.

\item
At 8:15 (and for a different word), the contestants have used up two of the stoppers. Now what is the probability that both of Michael's
letters will appear in the word?

{\bf Solution.} There are six letters, and he chooses two. The probability that neither is the bad one is $\frac{4}{6} = \frac23$.

\item
(Challenge!) Suppose that Michael knows the first (6:25) word from the beginning, but rather than guessing it immediately chooses to 
draw tiles until one of the following happens: (1) he draws and places the first $R$ on the blue spot, and thereby can earn \$500 for his
guess; (2) he draws two stoppers, and must play one of them (and so forfeits his turn); (3) he places all letters but the first $R$, and is obliged
to guess without earning the \$500.

Compute the probabilities of each of these outcomes.

{\bf Solution.} We look at this turn by turn.
\begin{itemize}
\item (First turn.) With probability $\frac{3}{55}$ he draws two stoppers and loses.
With probability $\frac{10}{55} = \frac{2}{11}$ he draws the first $R$ and can place it and win \$500.

The number of ways in which he can draw one stopper and one good tile other than the first $R$ 
is $3 \cdot 7 = 21$, so there is probability $\frac{21}{55}$ that this happens and he wins the turn but
not \$500. Finally, there are $C(7, 2) = 21$ in which he can draw good two tiles other than the first $R$,
so there is probability $\frac{21}{55}$ that this will happen and he goes to a second round.

Note that $3 + 10 + 21 + 21 = 55$ -- a good way to check our work! We've enumerated all possibilities
and the probabilities end up to $1$.

\item (Second turn.) There is probability $\frac{21}{55}$ that the game goes on to a second turn.
The following probabilities assume that it does, and should all be multiplied by $\frac{21}{55}$.

There are $C(9, 2) = 36$ ways to draw two tiles. As above, there are $3$ ways to draw two
stoppers, $8$ ways in which he can draw the first $R$ and something else, $3 \cdot 5 = 15$ ways
in which he can draw a stopper and a tile other than the first $R$, and $C(5, 2) = 10$ ways in which
he can draw two more good tiles other than the first $R$. So there is probability
$\frac{10}{36}$, or $\frac{21}{55} \times \frac{10}{36}$ total, of the game going onto a third round.

\item (Third turn.) Similar to above. There are $C(7, 2) = 21$ ways to draw two tiles, $3$ to draw
two stoppers, $6$ to draw the first $R$, $9$ to draw a stopper and a tile other than the first $R$,
and $3$ ways in which he can draw two more good tiles other than the first $R$. The probability
of the game going on to a fourth turn (total) is  $\frac{21}{55} \times \frac{10}{36} \times \frac{3}{21}$.

\item (Fourth turn.) There are $C(5, 2) = 10$ ways to draw two tiles, $3$ to draw two stoppers,
$4$ to draw the first $R$, and $3$ ways in which he can draw a stopper and a title other than the first $R$.
\end{itemize}
So we can compute all the probabilities:
\begin{itemize}
\item
Places the first R:
\[
\frac{10}{55} + 
\frac{21}{55} \cdot \frac{8}{36} +  
\frac{21}{55} \cdot \frac{10}{36} \cdot \frac{6}{21} +  
\frac{21}{55} \cdot \frac{10}{36} \cdot \frac{3}{21} \cdot \frac{4}{10} = \frac{10}{33}
\]
\item
Draws two stoppers:
\[
\frac{3}{55} + 
\frac{21}{55} \cdot \frac{3}{36} +  
\frac{21}{55} \cdot \frac{10}{36} \cdot \frac{3}{21} +  
\frac{21}{55} \cdot \frac{10}{36} \cdot \frac{3}{21} \cdot \frac{3}{10} = \frac{7}{66}
\]
\item
Must guess without winning \$500:
\[
\frac{21}{55} + 
\frac{21}{55} \cdot \frac{15}{36} +  
\frac{21}{55} \cdot \frac{10}{36} \cdot \frac{9}{21} +  
\frac{21}{55} \cdot \frac{10}{36} \cdot \frac{3}{21} \cdot \frac{3}{10} = \frac{13}{22}.
\]
\end{itemize}

\end{enumerate}

\item
Consider our first model of Plinko, where we assumed that the puck would always go one space to the left or one space to the right,
but did not ignore the walls of the board.
\begin{enumerate}
\item
If the contestant drops the puck one slot to the left of center, we already computed the probability that the puck lands in each of the nine
slots. Compute the expected value of this drop. (Use a calculator or computer, and round to the nearest dollar.)
\item
Carry out all these computations (1) if the contestant drops the puck down the center, and (2) if the contestant drops the puck down
the far left slot. If you have the patience, you might also do it if the contestant drops the puck two left of center -- in this case, by symmetry,
you will have considered all the possibilities.

What can you conclude about where the contestant should drop the puck?
\end{enumerate}

\item
Watch one or more playings of Plinko, and discuss the shortcomings in our model. Does the puck ever go more than one space to the left or right?

Briefly discuss how you would revise the model to be more accurate, and summarize how you would redo the problem above to correspond
to your revised model. (The details are likely to be messy, so you're welcome to not carry them out.)

\end{enumerate}

\section{Example: Poker}

We digress from our discussion of `traditional' game shows to discuss the game of {\itshape poker}. Poker is frequently televised -- for example
you can find the final table of the 2014 World Series of Poker on YouTube, all fourteen hours of it -- so you might call it a game show. 
Poker is a very mathematical game, and we can
very much use the mathematics we have already developed to analyze it.

We start off by describing the poker hands from best to worst and solving the combinatorial problems which naturally arise. For example,
if you are dealt five cards at random, what is the probability that you get dealt a straight or better?

We will then discuss different variations of Poker and the betting rules. This is where the really interesting decisions come into play:
do you fold, call, or raise? These are {\itshape essentially} expected value computations, although you must make informed guesses about
what your competitors hold.

\medskip
{\itshape Online play.} There are several websites where you can play free poker on the Internet. One I have used myself is 
\urlc{http://www.replaypoker.com} -- there is no gambling. (There is {\itshape betting}, but you are playing for `chips' which do not
represent real money.)

\medskip
{\itshape Further reading.} There are a great many excellent books on poker. I especially recommend the {\itshape Harrington on Hold'em}
series by Dan Harrington. These books are quite sophisticated and walk you through a number of expected value and probability computations.
If you've ever wanted to learn to play, you will find that this course provides excellent background!

\subsection{Poker Hands}
A {\bf poker hand} consists of five playing cards. From best to worst, they are ranked as follows:
\begin{itemize}
\item
{\bf A straight flush}, five consecutive cards of the same suit, e.g. $5 \ss 6 \ss 7 \ss 8 \ss 9 \ss$. An ace may be counted high
or low but straights may not `wrap around' (e.g. KA234 is not a straight).

In case of a tie, the high card in the straight flush settles ties. An ace-high straight flush is sometimes called a {\bf royal flush}, and is 
the highest hand in poker.

\item 
{\bf Four of a kind}, for example $K \ss K \cs K \ds K \hs$ and any other card. (If two players have four of a kind, the highest
set of four cards win.)

\item 
{\bf A full house}, i.e. three of a kind and a pair, $K \ss K \cs K \ds 7 \hs 7 \ds$. (If two players have a full house, the highest
set of three cards wins.)

\item
{\bf A flush}, any five cards of the same suit, e.g. $Q \cs 10 \cs 7 \cs 6 \cs 3 \cs$. The high card breaks ties (followed by the second
highest, etc.)

\item
{\bf A straight}, any five consecutive cards, e.g. $8 \cs 7 \ds 6 \ds 5 \hs 4 \ss$. The high card breaks ties.

\item
{\bf Three of a kind}, e.g. $8 \cs 8 \ds 8 \ss A \hs 4 \ss$. 

\item
{\bf Two pair}, e.g. $8 \cs 8 \ds 6 \ss 6 \hs A \ss$. 

\item
{\bf One pair}, e.g. $8 \cs 8 \ds 6 \ss 5 \hs A \ss$. 

\item
{\bf High card}, e.g. none of the above. The value of your hand is determined by the highest card in it; then, ties are settled by the
second highest card, and so on.

\end{itemize}

We now compute {\itshape the probability of each possible hand occurring.} Our computations will make heavy use of the multiplication rule.
(Note that each card is determined uniquely by its {\itshape rank} (e.g. king, six) and {\itshape suit} (e.g., spades, clubs).)

\begin{itemize}
\item {\bf All hands}. The total number of possible hands is $C(52, 5) = 2598960$.

\item {\bf Straight flush} (including royal flush). There are four possible suits, and nine possible top cards of that suit:
ace down through five. These determine the rest of the straight flush, so the total number of possibilities is $4 \times 10 = 40$.

\item {\bf Four of a kind}. There are thirteen possible ranks. You must hold all four cards of that suit, and then one of the other $48$
cards in the deck, so the total number of possibilities is $13 \times 48 = 624$.

\item {\bf Full house}. First, choose the rank in which you have three of a kind. There are 13 possible ranks, and 
$C(4, 3) = 4$ choices of three of that rank. Then, choose another rank (12 choices) and two cards ($C(4, 2) = 6)$ of that
rank. The total number of possibilities is the product of all these numbers: $13 \times 4 \times 12 \times 6 = 3744$.

\item {\bf Flush}. Choose one of four suits (in $4$ ways), and five cards of that suit (in $C(13, 5)$ ways), for a total 
of $4 \times C(13, 5) = 5148$ possibilities.

Except, we don't want to count the straight flushes again! So subtract $40$ to get $5108$.

\item {\bf Straight}. Choose the highest card (ace through five, so ten possibilities). For each of five ranks in the straight, there
are 4 cards of that rank, so the number of possibilities is $10 \times 4^5 = 10240$. Again subtracting off the straight flushes,
we get $10200$.

\item {\bf Three of a kind.} Choose a rank and three cards of that rank in $13 \times C(4, 3) = 52$ ways. Then, choose two other ranks (distinct from
each other) in $C(12, 2)$ ways. For each of these ranks there are four possibilities, so the total is $52 \times C(12, 2) \times 4^2 = 54912$.

Note that hands with four of a kind or a full house `include three of a kind', but we counted so as to exclude these possibilities, so we don't need
to subtract them now.

\item {\bf Two pair.} Choose two different ranks in $C(13, 2)$ ways; for each, choose two cards of that rank in $C(4, 2)$ ways. Finally, choose one
of the $44$ cards not of the two ranks you chose. The total number of possibilities is 
$C(13, 2) \times C(4, 2)^2 \times 44 = 123552$.

\item {\bf One pair.} Choose the rank in $13$ ways and choose two cards of that rank in $C(4, 2)$ ways. Then, choose three other ranks in
$C(12, 3)$ ways and for each choose a card of that rank in $4$ ways.

The total number of possibilities is $13 \times C(4, 2) \times C(12, 3) \times 4^3 = 1098240$ ways.

\item {\bf None of the above.} There are several ways we could count this. Here is one way: we can choose five different ranks in $C(13, 5)$ ways --
but we must subtract the ten choices that are straights. So the number of choices for ranks is $(C(13, 5) - 10)$.

Now, for each rank, we choose a suit, and the total number of choices is $4^5 - 4$. We subtract $4$ because we want to exclude the flushes!
So the total number of possibilities is $(C(13, 5) - 10) \times (4^5 - 4) = 1302540$.

\medskip
Here is a second way to get the same result. We know that the total number of possibilities is $2598960$. So we add all the previous possibilities,
and subtract from $2598960$.
\end{itemize}

This involved some subtleties, and for other variations the computations are still harder! For example, in {\bf seven card stud} you are dealt a seven-card
hand, and you choose your best five cards and make the best possible poker hand from these. You can redo all the above computations, but now some
new possibilities emerge. For example, you can be simultaneously dealt a straight and three of a kind -- and you want to count this only as a straight
(since that is better than three of a kind). But it is not {\itshape so} hard. The following Wikipedia page works out all the probabilities in detail:

\urlc{https://en.wikipedia.org/wiki/Poker_probability}

\medskip
{\bf Poker variations.} There are many variants of poker. The rules for betting (and blinds and antes) are described in the next section; for now we simply indicate when a
round of betting occurs.

\medskip
{\bf `Ordinary' poker}. (No one actually plays this.) Each player is dealt five cards face down. There is a round of betting. The best hand (among those who have not folded)
wins.

\medskip
{\bf Five-card draw}. Each player is dealt five cards face down. There is a round of betting. Then, each player who has not folded may choose
to trade in up to three cards, which are replaced with new cards (again dealt face down). There is another round of betting, and the best hand wins.

\medskip
{\bf Texas Hold'em}. Typically played using blinds (and sometimes also antes), applied to the first round of betting only. 
Each player is dealt two cards, dealt face down. There is a round of betting. Three community cards are dealt face up (the `flop'), which
every player can use as part of their hand. There is a round of betting. A fourth community card is dealt (the `turn'), followed by
another round of betting. Finally, a fifth community card is dealt (the `river'), again 
followed by another round of betting. 

Each player (who has not folded) chooses their best possible five-card hand from their two face-down cards and the five face-up cards (the latter of which
are shared by all players). The best hand wins.

Texas Hold'em is extremely popular and plenty of video can be found on the internet. For example,
this (six hour!) video is of the first part of the final table of the 2014 World Series of Poker:
\urlc{https://www.youtube.com/watch?v=5wlVFMNVJZQ}
The top prize was a cool \$10 million.

This is the most interesting poker video I have ever seen. Most telecasts of poker heavily edit their coverage, only showing the hands
where something exciting or out of the ordinary happens. This video is unedited, and so gives a much more realistic viewpoint
of what tournament poker is like.

\medskip

In the opening round of Texas Hold'em, you are dealt only your two-card hand and you have to bet before any of the community cards are dealt. This offers
some probability questions which are quite interesting, and easier than those above. For example, in {\itshape Harrington on Hold'em, Volume I: Strategic Play},
Harrington gives the following advice for you should raise, assuming you are playing at a full table of nine or ten players and are the first player to act.

\begin{itemize}
\item Early (first or second) position: Raise with any pair from aces down to tens, ace-king (suited or unsuited), or
ace-queen (suited).
\item Middle (third through sixth) position: Raise with the above hands, nines, eights, ace-queen, ace-jack, or king-queen
(suited or unsuited).
\item Late (seventh or eighth) position: Raise with all the above hands, sevens, ace-x, or high suited connectors like
queen-jack or jack-ten.
\end{itemize}

\medskip

Harrington also points out that your strategy should depend on your stack size, the other players' stack sizes, 
your table image, the other players' playing styles, any physical tells you have on the other players, the tournament status,
and the phase of the moon. But this is his starting point. Let us work out a few examples (you will be asked to work out more in the exercises).

\begin{example} In a game of Texas Hold'em, compute the probability that you are dealt a pair of aces (`pocket aces').
\end{example}
\soln{There are $C(52, 2) = 1326$ possible two-card hands. Of these, $C(4, 2) = 6$ are a pair of aces, so the answer
is $\frac{6}{1326} = \frac{1}{221}$, a little bit less than $0.5\%$.}

\begin{example} In a game of Texas Hold'em, compute the probability that you are dealt a pair.
\end{example}
\soln{There are $13$ possible ranks for a pair, and $C(4, 2) = 6$ pairs of each rank, so the answer is $\frac{6 \times 13}{1326} = \frac{1}{17}$.}

\begin{example} You are playing Texas Hold'em against five opponents, and you are dealt a pair of kings. You have the best hand at the
table unless someone else has a pair of aces. Compute the probability that one of your opponents has a pair of aces.
\end{example}
{\bf Approximate solution.} There are fifty cards left in the deck, excluding your two kings. The probability that any {\itshape specific} 
one of your opponents has pocket aces is $\frac{C(4, 2)}{C(50, 2)} = \frac{6}{1225}$, or about $1$ in $200$. (This much is exact.)

These probabilities are not independent: if one player has pocket aces, the others are less likely to. Nevertheless, we get a very nearly correct answer if 
we assume they are independent. The probability that any specific player does {\itshape not} have pocket aces is $1 -  \frac{6}{1225} = \frac{1219}{1225}$.
If these probabilities are independent, the probability that all five opponents have something other than pocket aces is 
$\Big( \frac{1219}{1225} \Big)^5$. So the probability that at least one of your opponents has pocket aces is
\[
1 - \bigg( \frac{1219}{1225} \bigg)^5 = 0.0242510680\dots
\]

\medskip
{\bf Remark.} Here is a simpler approximate solution. Just multiply $\frac{6}{1225}$ by $5$, to get
\[
\frac{30}{1225} = 0.02448979591\dots
\]
This is almost exactly the same. Why is this? We can use the binomial theorem to see that
\[
1 - (1 - x)^5 = 5x - 10x^2 + 10x^3 - 5x^4 + x^5,
\]
and plug in $x = \frac{6}{1225}$. Since $x$ is very small, the $x^2$, etc. terms are {\bf very} small.

\begin{example} You are sitting in first position. Compute the probability that you receive a hand that you should raise,
according to Harrington's advice.
\end{example}
\soln{As before there are $1326$ hands, so we count the various hands that Harrington says are worth opening:
\begin{itemize}
\item 
A pair of aces through tens: Five ranks, and $6$ ways to make each pair, so a total of $5 \times 6 = 30$.
\item 
Ace-king: Four ways to choose the suit of the ace, and four ways to choose the suit of the king. $4 \times 4 = 16$.
\item
Ace-queen suited. ({\itshape Suited} means the cards are of the same suit. If your cards are suited, this helps you because
it increases the chances that you will make a flush.) Four ways to choose the suit, so just $4$.
\end{itemize}
None of these possibilities overlap, so the total number is $30 + 16 + 4 = 50$. The probability is $\frac{50}{1326}$.
}

This is less than $1$ in $25$! Harrington's strategy is much more conservative than that of most top players.

In the exercises, you will compute the probability of getting a hand worth opening in middle or late position.

\subsection{Poker Betting}

So far we have just considered probabilities. But the interesting part of the game comes when we combine this with a discussion of betting strategy.

Poker is played for {\it chips}, which may or may not represent money. In general there are two different formats. In a {\bf cash game}, you simply
try to win as many chips as you can. By contrast, a {\bf tournament} is played until one player has won all the chips. Before each hand players
have to put {\bf antes} or {\bf blind bets} into the pot, and in a tournament these keep going up and up to force the tournament to end eventually.

\medskip
{\bf Betting rounds.} In all variations of poker, a betting round works as follows. The first player (usually, but not always, the player left of the dealer)
opens the betting. She may {\bf check} (bet nothing) or bet any amount. The betting then proceeds around the table clockwise. If no one has bet yet,
the player may check or bet. If someone has bet, then the player may {\bf fold} (abandon her hand), {\bf call} (match the bet), or {\bf raise} (put in a larger bet).
The betting continues to go around the table until either everyone has checked, or everyone has called or folded to the last (largest) bet. Note that players
may raise an unlimited number of times, so betting can go around the table multiple times if many players keep raising.

In {\itshape no-limit} poker, a player may bet anything up to and including her entire stack of chips. Players are never allowed to bet more than however many
chips they have on the table. (You are not allowed to reach into your wallet and suddenly drop a stack of Benjamins.) Conversely, you can always
call a bet for your entire stack: if someone bets more chips than you have, you may go `all-in' and their effective bet is limited to the number of chips
you have. (There are `side pot' rules if one player is all-in and two other players want to keep raising each other; we won't consider them here.)

Typically there are multiple rounds of betting. If a player bets and everyone else folds, then that player wins the pot. (The `pot' consists of the blinds and antes
and all of the bets that have been made.)
Otherwise, everyone remaining at the end
compares their hands, and the best hand wins the pot.

\medskip
{\bf Blinds and antes}. A hand of poker never starts with an empty pot; there is always a little bit of money to be won from the beginning. This is assured
via blinds and antes. If {\bf antes} are used, then each player puts a fixed (small) amount of money into the pot at the beginning. If {\bf blinds}
are used, then the first two players in the first betting round make a `blind bet' before looking at their cards. For example, the first player might be
required to bet \$1 (the small blind) and the second player \$2 (the big blind). These count as their initial bets, except that if everyone calls or 
folds to the big blind, the round is not quite over; the big blind has the opportunity to raise if she wishes.

\subsection{Examples}
We now consider some examples of poker play and the mathematics behind your decision making.

\medskip
{\bf Example 1.} You are playing Texas Hold'em with one opponent (Alice). The current pot is 500 chips, and you and Alice each have 700 chips.
You have a hand of $5 \hs 4 \hs$, the flop comes $A \cs K \hs 10 \hs$. You check, and Alice responds by going 
all-in. Should you fold or call her bet?

\medskip
{\bf Analysis.} There are three steps to solving this problem. First, you estimate your winning probability depending on what cards come.
Since you don't know what your opponent has, this is a very inexact science (and indeed depends on your assessment of Alice's strategy).

The next two steps are mathematically more straightforward: the second step is to compute the probability of each possible outcome,
and the third is to determine whether the expected value of calling is positive or negative. Since the expected value of folding is always
zero (not counting whatever you have put into the pot already), this determines whether or not you should call.

\medskip
You guess that Alice probably has a good hand -- a pair of tens or higher. You estimate that you probably need to make a flush
to beat her. You make a flush if at least one heart comes in the turn and the river. You'd rather see {\itshape only} one heart,
because if two hearts come, Alice beats you if she has any heart higher than the $5 \hs$.
\begin{itemize}
\item If exactly one heart comes during the next two cards, then almost certainly you win. You only lose if Alice has two hearts,
one of them higher than a five, or if she makes some freak hand like a full house or four of a kind. (This can't be discounted
if a pair appears on the flop, but as it stands this looks pretty unlikely.)

We estimate your winning chances here as $90\%$. (Reasonable people might disagree!)

\item If two hearts come during the next two cards, you might win -- but Alice could easily have a heart higher than the $5 \hs$.
We estimate your chances of winning as $50\%$.

\item If no hearts come, then you are very unlikely to win. You could -- for example, if two fives, or two fours, or a five and a four,
come then you {\itshape might} win, but this is unlikely. We will simplify by rounding this probability down to zero.
\end{itemize}
There are $47$ cards you can't see, and nine of them are hearts. What is the probability that the next two are both hearts?
As we've seen before, this is
\[
\frac{9}{47} \cdot \frac{8}{46} \sim 0.033\dots
\]
This is quite low! It is substantially lower than $(1/4)^2$, simply because you can already see four of the hearts.

\medskip
Now, what is the probability that one, but not both, of the next two cards, is a heart? There are two ways to compute this,
and we will work out both.

{\itshape Method 1}. The probability that the first card is a heart and the second card is not a heart is
\[
\frac{9}{47} \cdot \frac{38}{46} \sim 0.158\dots
\]
The probability that the second card is a heart and the first card is not is the same. So the total probability is $\frac{342}{1081}$, or
approximately $0.316$.

{\itshape Method 2.} First, we compute the probability that neither card is a heart. This is 
\[
\frac{38}{47} \cdot \frac{37}{46}.
\]
So, the probability that exactly one card is a heart is
\[
1 - \frac{38}{47} \cdot \frac{37}{46} - \frac{9}{47} \cdot \frac{8}{46} = \frac{342}{1081}.
\]
It is very typical that there are multiple ways to work out problems like this! This offers you a great chance to check your work.

\medskip
So what's the probability you win? $0.9$ times the probability that exactly one heart comes, plus $0.5$ times the probability that two hearts come.
In other words,
\[
0.9 \times 0.316 + 0.5 \times 0.033 \sim 0.301,
\]
which for the sake of simplicity we will round off to $0.3$.

\medskip
Now, on to the expected value computation. If you call and win, then you win \$1,200: the \$500 previously in the pot, plus the \$700 that Alice bet.
If you call and lose, you lose \$700. Therefore the expected value of calling is
\[
0.3 \cdot 1200 + 0.7 \cdot (-700) = -130.
\]
It's negative, so you should fold here. 

But notice that it's close! So, for example, if the flop had come $A \hs 8 \hs 7 \cs$, then you should call. (Exercise: verify this as above!) Here you will
make a straight if a six comes. It is not so likely that a six will come, but a small probability is enough to swing your computation.

\medskip
{\bf Example 2.} The same situation, except imagine that you both have 1,000 chips remaining and that
Alice bets only 300 chips. What should you do?

\medskip
You could consider folding, calling, or now raising. Let us eliminate raising as a possibilty: if Alice is bluffing with something
like $Q \cs 7 \ds$, then you might get her to fold, even though she has a better hand. But this doesn't seem very likely.

Since you have the opportunity to bet again, let us now consider {\bf the next card only.}

\begin{itemize} 
\item
Suppose the next card is a heart, giving you a flush.
Then, you think it is more likely than not that you'll win,
so you want to bet. Moreover, since Alice might have one heart in her hand, you would really like her to fold --
and so if this happens, you will go all in.

It is difficult to estimate the probabilities of what happens next -- this depends on how you see Alice, how she sees you,
and what she's holding. As a rough estimate, let us say there is a 50-50 chance that she calls your all-in bet,
and if she calls there is a 75\% chance of you winning with your flush.
\item
Suppose the next card is not a heart. Then you don't want to bet, because you don't have anything. Let us say that
there is a 75\% chance that Alice goes all-in, in which case you should and will fold. (Check the math here!) 

If Alice instead checks (assume there is a 25\% chance of this), you both get to see one more card and bet again.
If it is a heart, assume that you both go all-in and that you win with $75\%$ probability. If it is not a heart, assume that
Alice goes all in and you fold.
\end{itemize}
These percentages are approximate -- once again we can't really expect to work exactly. But given the above, we can
enumerate all the possibilities, their probabilities, and how much you win or lose:
\begin{itemize}
\item Heart, she calls your all-in, you win: probability $\frac{9}{47} \times \frac{1}{2} \times \frac{3}{4} \sim 0.072$, you win
\$1500. (The initial \$500 pot, and her \$1000.)
\item Heart, she calls your all-in, you lose: probability $\frac{9}{47} \times \frac{1}{2} \times \frac{1}{4} \sim 0.024$, you lose
\$1000. (Your remaining \$1000.)
\item Heart, she folds: $\frac{9}{47} \times \frac{1}{2} \sim 0.096$, you win \$800. (The initial \$500 pot, plus the \$300 she invested
to make the first bet.)
\item Not a heart, she goes all-in: $\frac{38}{47} \times \frac{3}{4} \sim 0.606$, you lose \$300. (This is what you invested to call
her first bet, but you fold and so avoid losing any more.)
\item Not a heart, she checks, next card is a heart, you win: $\frac{38}{47} \times \frac{1}{4} \times \frac{9}{46} \times \frac{3}{4} \sim 0.030$.
You win \$1500.
\item Not a heart, she checks, next card is a heart, you lose: $\frac{38}{47} \times \frac{1}{4} \times \frac{9}{46} \times \frac{1}{4} \sim 0.010$.
You lose \$1000.
\item Not a heart, she checks, next card is not a heart, you fold: $\frac{38}{47} \times \frac{1}{4} \times \frac{37}{46} \sim 0.163$. You lose
\$300.
\end{itemize}
As is often the case in poker, it is more likely that you will lose than win, but the winning amounts are larger than the losing amounts.
Here there are two reasons for this: first of all, if she goes all-in on a bad card for you, then you can usually fold and cut your losses.
The second is that we're comparing against a baseline of folding, which we say has expected value zero. But if you bet, you can not
only get Alice to match your bets, but also keep your stake in the existing pot. 

The expected value of calling is
\[
.072 \times 1500 - .024 \times 1000 + .096 \times 800 - .606 \times 300
+ .040 \times 1500 - .013 \times 1000 - .163 \times 300 \sim -35.
\]
A close decision, but if we believe our assumptions, then it looks like it's wise to fold.

\medskip
{\bf Example 3.} You are the big blind (50 chips) at a full table, playing Texas Hold'em. The first player, who is known to be conservative,
raises to $200$ chips, and everyone else folds to you. You have a pair of threes, and if you call, both you and your opponent will have
3,000 more chips to bet with. Since you already have 50 chips in the pot, it costs you 150 chips to call.

Should you call or fold?

\medskip
To solve this problem we again have to make guesses about what we think will happen, which are still more inexact than the last
problem. This will set up another expected value problem.

Anyway, the first player is known to be conservative, so she probably has ace-king or a high pair or something like that. Let us assume
that {\itshape no three comes on the flop, you will not dare to bet.} Assume further that your opponent will, and you end up folding.

Since you have a pair of threes, you are hoping that a three comes on the flop. If so, you will almost certainly win. Let us assume that, if a three comes on the flop:
\begin{itemize}
\item
With 25\% probability, your opponent will fold immediately and you will win the current pot (of 425 chips: your bet, her bet, and 25 chips from the small blind).
\item
With 60\% probability, your opponent will bet somewhat aggressively, but eventually fold, and you win (on average) the current pot of 425 chips, plus
an additional 500 chips.
\item
With 10\% probability, your opponent will bet very aggressively. Both of you go all-in, and you win the pot of 425 chips plus all 3,000 of her remaining chips.
\item
With 5\% probability, your opponent gets a better hand than three threes, and both of you go all-in and you lose 3,000 of your remaining chips.
\end{itemize}
Let $\alpha$ be the probability of a three coming on the flop. Then, the expected value of calling (relative to folding) is
\[
-150 + \alpha \cdot \bigg( .25 \cdot 425 + .60 \cdot 925 + .10 \cdot 3425 + .05 \cdot (-3000) \bigg) 
= -150 + 853.75 \alpha.
\]
So we need to compute $\alpha$ to determine whether this is positive or negative. To illustrate our techniques, we will do this in two different ways.
In both cases we compute the probability that {\bf no} three comes on the flop, and then subtract this from $1$.

\smallskip
{\bf Solution 1.} The first card will not be a three with probability $\frac{48}{50}$: there are $50$ cards remaining, and $48$ of them are not threes.
If the first card is not a three, then the second card will not be a three with probability $\frac{47}{49}$, and the third card will not be a three
with probability $\frac{46}{48}$. The probability that at least one card is a three is therefore
\[
1 - \frac{48}{50} \cdot \frac{47}{49} \cdot \frac{46}{48} = .117\cdots
\]

Therefore, the expected value of calling is
\[
-150 + 835.75 \cdot .117 = -52.30.
\]
It is negative, so a call is more prudent. 
\medskip

{\bf Solution 2.} We compute in a different way the probability that none of the three cards in the flop is a three. There are
$C(50, 3)$ possible flops, and $C(48, 3)$ possible flops which don't contain a three. So this probability is 
$\frac{C(48, 3)}{C(50, 3)}$, which is the same as $ \frac{48}{50} \cdot \frac{47}{49} \cdot \frac{46}{48}$.

Some remarks:
\begin{itemize}
\item If you each had 10,000 remaining chips, then it {\bf would} make sense to call. (Redo the math to see why!!) This illustrates the principle that
long-shot bets are more profitable if you possibly stand to make a very large amount of money.
\item The above computations assumed that all 50 cards were equally probable. But, given what you know about your opponent,
you might assume that she doesn't have a three in her hand. In this case, the probability of getting a three on the flop goes up to
\[
1 - \frac{46}{48} \cdot \frac{45}{47} \cdot \frac{44}{46} = .122\cdots
\]
which is slightly higher.
\end{itemize}


\subsection{Exercises}
Thanks to the participants (credited by their screen names below) 
in the Two Plus Two Forums for suggesting poker hands which are treated here:
\urlc{http://forumserver.twoplustwo.com/32/beginners-questions/videos-online-illustrating-simple-mathematical-poker-concepts-1631031/}
\begin{enumerate}
\item Refer to Harrington's opening strategies for Texas Hold'em described above. 
If you are in middle position and everyone has folded before you,
compute the probability that you are dealt a hand which Harrington suggests raising.
 
 Now do the same for late position.
 
\item (Suggested by ArtyMcFly.) The following amusing clip shows a hand in a million-dollar Hold'em tournament with eight players,
where two players are each dealt a pair of aces. One of them makes a flush and wins.

\urlc{https://www.youtube.com/watch?v=aR52zv1GqBY}
\begin{enumerate}
\item Compute the probability that Drinan and Katz are each dealt a pair of aces. (No need to approximate; you can compute this
exactly.)
\item Compute the probability that any two of the eight players are each dealt a pair of aces.
\item Given that two players are dealt aces, these aces must be of different suits. Each player will win if at least four cards of one of
his two suits are dealt. (If four of this suit are dealt, then he will make a flush. If five of this suit are dealt, then both players will
have a flush, but only one of them will have an ace-high flush.)

The broadcast lists a probability of 2\% of this happening for each player. Compute this probability exactly.

(Note that the most common outcome is that no four cards of the same suit will be dealt, in which case the two players will
have equal hands and tie.)

\item Compute the probability of this whole sequence happening: two of the eight players are dealt a pair of aces, and one of them
makes a flush and wins. Please give both an exact answer and a decimal approximation.
\item Suppose these eight players play one hundred hands of poker. What is the probability that this crazy sequence of events
happens at least once?
\end{enumerate}
 
\item (Suggested by whosnext.) Here is another clip illustrating some serious good luck. (Or bad luck, depending on whose
perspective you consider!)

\urlc{https://www.youtube.com/watch?v=72uxvL8xJXQ}
Danny Nguyen is all-in with $A \ds 7 \ds$ against an opponent with $A \ss K \cs$. The flop
is $5 \hs K \hs 5 \ss$. After this, the next two cards must both be sevens for Nguyen to survive. Compute
the probability of this happening.

\item Consider a variant of poker where you are dealt four cards instead of five. So a `straight' consists of four consecutive cards, a `flush'
four of a suit.

By analogy with ordinary poker, determine what the possible hands are, and determine the probability of each. For each hand, give an exact answer for the probability
as well as a decimal approximation.

\item (This is Hand 4-3 from {\itshape Harrington on Hold'em, Volume 1}.)

Early in a poker tournament, with blinds \$5 and \$10, you are sitting third out of ten players 
in a no-limit Hold'em tournament with a stack of \$1,000. You are dealt $A \ds K \ss$.

The first two players fold, and you elect to raise to \$50. The next four players fold, and the eighth (next)
player, who has a stack of \$1,630, calls your bet. The total pot is \$115, and the remaining players fold.

The flop comes $J \ds 7 \cs 4 \hs$, and you act first. You choose to bet \$80. (This is a `continuation bet',
a kind of bluff. Since you expect that your opponent is somewhat likely to fold, this is considered good strategy.)

Your opponent raises to \$160. {\bf Do you fold, call, or raise the bet?}

You should analyze this hand as in the examples in the book and in lecture. As best as you can, estimate
your odds of having the best hand after the turn and the river, and carry out an appropriate expected value
computation.

{\itshape Note: There is no single right answer, so justify your assumptions. If you like, you may work with
one other person in the class and turn in a joint soution to this problem.}

\item (This is the optional bonus.) Watch part of the World Series of Poker clip in the text, or any other
poker tournament which is publicly available. (With your solution, please let me know where I can find video to
watch the hand myself.) Find a decision made by one of the players similar to the situation in the text or the
previous problem, and either explain or critique the play. Your solution should involve 
probability and expected value computations somewhere!

\end{enumerate}

\section{Inference}

Your instructor decides to conduct a simple experiment. He pulls out a coin and is curious to see how many consecutive heads he will flip.
He starts flipping -- and lo and behold he flips a long sequence of consecutive heads! Six, seven, eight, nine, ten .... What are the odds of
consecutive heads? $\big( \frac12 \big)^{10} = \frac{1}{1024}$. Pretty unlikely!

\medskip

He continues flipping. Eleven, twelve, thirteen, fourteen, ... the probabilities get smaller and smaller. But eventually it occurs to you that there is 
an alternative, and indeed more likely, explanation: {\itshape You cannot see the coins}, and so {\itshape perhaps your instructor was just lying to you.}

\medskip
What happened here? After the first coin, or after the second, you probably didn't suspect any dishonesty -- after all, it is not so unlikely to flip one or
two heads. He {\itshape could have been} lying, but you probably didn't suspect that. But while the probability of umpteen consecutive heads goes down and
down, the probability that he was lying from the beginning doesn't, and eventually the latter becomes more plausible.

\medskip
This is an example of {\itshape Bayesian inference}, which we will explore from a mathematical point of view. But even if you don't know the mathematics
yet, you already make similar inferences all the time. For example, suppose that a politician makes a claim you find surprising.\footnote{More specifically, this claim
should concern a {\itshape matter of fact}, which can be independently verified to be true or false. For example, a politican might claim that crime levels have been rising or falling,
that the moon landing was faked, or that Godzilla was recently sighted in eastern Siberia. Even if such claims cannot be confirmed or denied with 100\% accuracy,
the point is that they are objectively true or false. This is different than offering an opinion or speculation. For example, a politician might claim that if we airlift ten million teddy bears into North Korea,
they will overthrow their dictator and become a democracy. We cannot say this is true or false without trying it. Similarly, a politician might say that Americans are the kindest 
people in the world. Unless you are prepared to objectively measure `kindness', this is a subjective matter of opinion.} Then, informally you will assess the probability that the
claim is true. In doing so, you will take into account two factors: (1) how likely you believed this claim might have been true, before the politician made it; (2) your assessment
of the honesty of the politician in question. 

\medskip
And finally we can look for examples from game shows. Here is a clip of {\itshape Let's Make a Deal}:

\begin{important}
\urlc{https://www.youtube.com/watch?v=-vRty_kkfgw}
\end{important}

What would you do?

\subsection{Conditional Probability}

\begin{important}
\begin{definition}
Let $A$ and $B$ be events in a sample space $S$. If $P(A) \neq 0$, then the {\bf conditional probability of 
$B$ given $A$}, written $P(B | A)$, is
\[
P(B|A) = \frac{ P(A \cap B) }{ P(A) }.
\]
\end{definition}
\end{important}
Here the symbol $\cap$ means {\bf intersection} -- so $A \cap B$ is the set of outcomes that are in both $A$ and $B$.
In informal language, $P(A \cap B)$ is the probability that both $A$ and $B$ occur. We also sometimes omit the word
`conditional', and just say `the probability of $B$ given $A$'.

\begin{example}
You flip two coins. Compute the probability that you flip at least two heads, given that you flip at least one head.
\end{example}
\soln{We could give a quicker solution, but let's write out everything explicitly for clarity's sake. The sample space is
\[
S = \{ HH, HT, TH, TT \},
\]
with all outcomes equally likely. Call $A$ the event that we flip at least one head. We have
\[
A = \{ HH, HT, TH \}.
\]
Write $B$ for the event that we flip two heads. We have
\[
B = \{ HH \}.
\]
We also have
\[
A \cap B = B = \{ HH \},
\]
because $B$ is a subset of $A$. ({\itshape Warning: In many examples $B$ will not be a subset of $A$.})

So,
\[
P(B|A) = \frac{ P(A \cap B) }{ P(A) } = \frac{ \frac14}{ \frac34} = \frac13.
\]
}
Note that we could have simplified the arithmetic in the last step. We have
$P(A \cap B) = \frac{ N(A \cap B) }{ N(S) }$, and also
$P(A) = \frac{ N(A) }{ N(S) }$, so that
\[
P(B|A) = \frac{ P(A \cap B) }{ P(A) } = \frac{ \frac{ N(A \cap B) }{ N(S) } }{\frac{ N(A) }{ N(S) } } = \frac{ N(A \cap B) } { N(A) }.
\]
This is an alternative formula which we can use whenever it's easier. (It holds always, not just for this example.)
 
The next example concerns the Price Is Right game {\bf One Away.}

\begin{important}
\urlc{https://www.youtube.com/watch?v=V6gCNW5wFIY}
\end{important}

\newgame{One Away -- The Price Is Right}{
The contestant is shown a car and a five digit price for the car. Each digit in the price is off by one -- too low or too high.
She then guesses the price of the car, one digit at a time.

If her guess is correct, she wins the car. Otherwise, if at least one digit is correct, she is told how many digits she has right
and can make corrections as she sees fit. 
}

\begin{example}
Suppose that all the digits are random. Compute the probability that her first guess is correct, given that
she has at least one number right.
\end{example}
\soln{
This is easy: Let $A$ be the event that she has at least one number right, so $P(A) = \frac{31}{32}$, an
let $B$ be the event that she has all five right, with $P(B) = P(B \cap A) = \frac{1}{32}$. We have
\[
P(B|A) = \frac{P(B)}{P(A)} = \frac{1}{31}.
\]
}

Her winning chances have gone from $1$ in $32$ to $1$ in $31$, {\itshape if} you assume the digits are random.
In reality, we can be pretty sure the first digit is indeed a $2$. (If this is not clear to you, please take a friend
with you next time you go car shopping.) If the {\itshape other} digits are random, her winning chances have
gone from $1$ in $16$ to ...... $1$ in $16$.

She is then told she has at least two numbers right.


\begin{example}
Suppose that all the digits are random. Compute the probability that her first guess is correct, given that
she has at least two numbers right.
\end{example}
\soln{
Now writing $A$ for the event that she has at least two numbers right, we have $P(A) = \frac{26}{32}$ and so
$P(B|A) = \frac{1}{26}$.

How did we compute $26$? Well, the number of ways to get {\itshape exactly} two numbers right is
$C(5, 2)$: choose which two numbers. And so, the number of ways to get at least two numbers right is
\[
C(5, 2) + C(5, 3) + C(5, 4) + C(5, 5) = 10 + 10 + 5 + 1.
\]
}

Now here's what no contestant on this show realizes: she should be {\bf ecstatic} that she {\bf doesn't}
have two numbers right. Let us go back to assuming the first digit is definitely a $2$, and the others are random.
Her initial winning chances are $1$ in $16$, and once we know she has at least two numbers right they
go up to $1$ in $15$. Whoopty doo. But, if she {\itshape doesn't} have at least two numbers right, then
{\itshape keep the initial $2$ and change everything else!!}

\begin{example}
You roll two dice. What is the probability that the sum of the numbers showing face up is $8$, given that
both of the numbers are even?
\end{example}
Note that, in contrast to the previous examples, neither event is implied by the other. We could roll an $8$
without either of the numbers being even, and we could roll two even numbers whose sum isn't $8$.

\soln{
Writing $S$ for the sample space, it has $36$ elements as we have seen before. Write $A$
for the event that the numbers are both even, and $B$ for the event that the total is eight. Then we have
\[
A = \{ 22, 24, 26, 42, 44, 46, 62, 64, 66 \},
\]
\[
B = \{ 26, 35, 44, 53, 62 \},
\] 
\[
A \cap B = \{ 26, 44, 62 \}.
\]
Then (using the alternate version of our formula) we have
\[
P(B|A) = \frac{N(A \cap B)}{N(A)} = \frac{3}{9} = \frac{1}{3}.
\]
}
In conclusion, if we know that both numbers are even, this makes it more likely that they will sum to eight.
This is true even though we removed some possibilities like $3 + 5$.

\subsection{The Monty Hall Problem}

We now consider the most famous mathematical problem coming from a game show: the {\itshape Monty Hall Problem}.
Although the problem was based on {\itshape Let's Make a Deal}, the problem was made up and never actually
happened on the show. But it's a good problem, so we'll consider it anyway.

\begin{important}
{\bf The Monty Hall Problem.} Monty Hall, on Let's Make a Deal, shows you three doors. Behind one door is a car,
behind the others, goats. You pick a door, say No. 1, 
and the host, who knows what's behind the doors, opens another door, say No. 3, 
which has a goat. He then says to you, ``Do you want to switch to door No. 2?'' 

Is it to your advantage to switch your choice?
\end{important}

\medskip
Let's assume that the contestant prefers the car to a goat.\footnote{But see \url{https://xkcd.com/1282/}.} We will make the following further assumptions:
\begin{itemize}
\item Initially, the car is equally likely to be behind any of the three doors.
\item After you choose a door, the host will randomly pick one of the other doors with a goat and open that one.

More specifically: If you choose a door with a goat, then exactly one of the other two doors will have a goat and the host will
show it to you. If you choose the door with the car, then both of the other doors will have goats and the host will pick one of them
at random and show it to you.
\end{itemize}

So, given that you choose Door 1, let's compute the sample space of all possible outcomes:
\begin{itemize}
\item The car is behind Door 2 (probability $\frac{1}{3}$). Monty shows you Door 3.
\item The car is behind Door 3 (probability $\frac{1}{3}$). Monty shows you Door 2.
\item The car is behind Door 1, and Monty shows you Door 2. (Probability $\frac{1}{3} \times \frac12 = \frac16$). 
\item The car is behind Door 1, and Monty shows you Door 3. (Probability $\frac{1}{3} \times \frac12 = \frac16$). 
\end{itemize}

Let $B$ be the event that the car is behind Door 2 (so $P(B) = \frac13$), and let $A$ be the event that Monty shows you Door 3.
We want to compute $P(B|A)$, the probability that the car is behind Door 2, given that Monty showed you Door 3.

We have
\[
P(B | A) = \frac{ P(A \cap B) } {P (A) }.
\]
The probability $P(A \cap B)$ is $\frac13$, the same as $P(B)$. As we saw before, if the car is behind Door 2, Monty will always show you
Door 3.

The probability $P(A)$ is $\frac 12$, the sum of the two probabilities above in which Monty shows you $\frac12$. If the car
is behind Door 2, Monty will always show you Door 3, and if the car is behind Door 1 then Monty {\itshape might} show you Door 3.

So
\[
P(B | A) = \frac{ \frac13}{ \frac 12} = \frac 23.
\]
{\bf Given that Monty showed you Door 3, there is now a $\frac23$ probability the car is behind Door 2. You should switch.}

\medskip
Many people find this counterintuitive, and so we will consider some alternative fomulations.

\medskip
{\bf Monty Hall with a Million Doors.} Instead of three doors, Monty shows you one million, and randomly you choose door
816,280. Monty then opens 999,998 of the remaining doors -- all but your door and Door 161,255. He offers you the opportunity to
switch.

You really feel like you should take it, don't you?

\medskip
{\bf Bertrand's Box} is a closely related paradox. There are three boxes -- one with two gold coins, one with two silver coins,
and one with one of each. You don't know which box is which. You randomly choose one of the boxes and one of the coins
in it, and it turns out that your coin is gold. What is the probability that the other coin in the box is also gold? Answer: Two
thirds. (Work it out!)

\medskip
{\bf The Prisoner Paradox} was posed by Martin Gardner in 1959, and is equivalent to the Monty Hall problem. Here it is,
in Gardner's original formulation.\footnote{ 
Gardner, Martin (October 1959). {\itshape Mathematical Games: Problems involving questions of probability and ambiguity}. Scientific American. 201 (4): 174--182.
Available online: \url{http://www.nature.com/scientificamerican/journal/v201/n4/pdf/scientificamerican1059-174.pdf}.}

\begin{quote}
A wonderfully confusing little problem involving three prisoners and a warden, even more difficult to state unambiguously, is now making the rounds. Three men-A, B and C-were in separate cells under sentence of death when the governor decided to pardon one of them. He wrote their names on three slips of paper, shook the slips in a hat, drew out one of them and telephoned the warden, requesting that the name of the lucky man be kept secret for several days. Rumor of this reached prisoner A. When the warden made his morning rounds, A tried to persuade the warden to tell him who had been pardoned. The warden refused.
`Then tell me,' said A, `the name of one of the others who will be executed. If B is to be pardoned, give me C's name. If C is to be pardoned, give me B's name. And if I'm to be pardoned, flip a coin to decide whether to name B or C.'

Three prisoners, A, B and C, are in separate cells and sentenced to death. The governor has selected one of them at random to be pardoned. The warden knows which one is pardoned, but is not allowed to tell. Prisoner A begs the warden to let him know the identity of one of the others who is going to be executed. `If B is to be pardoned, give me C's name. If C is to be pardoned, give me B's name. And if I'm to be pardoned, flip a coin to decide whether to name B or C.'

`But if you see me flip the coin,' replied the wary warden, `you'll know that you're the one pardoned. And if you see that I don't flip a coin, you'll know it's either you or the person I don't name.'

`Then don't tell me now,' said A.
`Tell me tomorrow morning.'

The warden, who knew nothing about
probability theory, thought it over that night and decided that if he followed the procedure suggested by A, it would give A no help whatever in estimating his survival chances. So next morning he told A that B was going to be executed.

After the warden left, A smiled to himself at the warden's stupidity. There were now only two equally probable elements in what mathematicians like to call the `sample space' of the problem. Either C would be pardoned or himself, so by all the laws of conditional probability, his chances of survival had gone up from $1/3$ to $1/2$.

The warden did not know that A could communicate with C, in an adjacent cell, by tapping in code on a water pipe. This A proceeded to do, explaining to C exactly what he had said to the warden and what the warden had said to him. C was equally overjoyed with the news because he figured, by the same reasoning used by A, that his own survival chances had also risen to $1/2$.

Did the two men reason correctly? If not, how should each calculate his chances of being pardoned? An analysis of this bewildering problem will be given next month.
\end{quote}


\subsection{Bayes' Theorem}

Suppose you go caving: you explore all sorts of beautiful underground caverns, and have a fabulous time.
But afterwards you are alarmed to hear that some cavers catch the disease of {\itshape cavitosis}.\footnote{This is
completely made up.} The disease can be treated, and so you decide to be tested to see if you have caught the
disease.

You learn the following:
\begin{itemize}
\item 
One in a thousand cavers develop cavitosis, and so your {\itshape a priori} probability of having cavitosis is 
$\frac{1}{1000}$.
\item
The test is not completely reliable, and 
has a {\itshape false positive} rate of $1\%$. This means that if you don't have the disease, and you
get tested for it, then with probability $\frac{99}{100}$ you will be told that you don't have the disease and
with probability $\frac{1}{100}$ you will be told (incorrectly) that you do have it.
\item
The test has a {\itshape false negative} rate of $3\%$. This means that if you {\itshape do} have the disease, and you
get tested for it, then with probability $\frac{97}{100}$ you will test positive\footnote{`Positive' does not mean
`good'; it means that you do have whatever condition was being tested for.} and with 
probability $\frac{3}{100}$ you will test negative.
\end{itemize}

There are four probabilities we can compute:
\begin{itemize}
\item
The probability that we have the disease, and test positive for it, is
\[
\frac{1}{1000} \times \frac{97}{100} = \frac{97}{100000} = 0.097\%.
\]
The probability that we do have the disease, but test negative for it, is
\[
\frac{1}{1000} \times \frac{3}{100} = \frac{3}{100000} = 0.003\%.
\]
The probability that we don't have the disease, but test positive for it, is
\[
\frac{999}{1000} \times \frac{1}{100} = \frac{999}{100000} = 0.999\%.
\]
The probability that we don't have the disease, and test negative for it, is
\[
\frac{999}{1000} \times \frac{99}{100} = \frac{98901}{100000} = 98.901\%.
\]
\end{itemize}

\begin{example}
You get tested for cavitosis, and to your horror the test comes back positive. Compute the probability
that you do indeed have cavitosis.
\end{example}
\soln{
The probability that you have cavitosis, and test positive for it, is $\frac{97}{100000}$.

The probability that you test positive for cavitosis is the sum of the two relevant probabilities above:
\[
\frac{97}{100000} + \frac{999}{100000} = \frac{1096}{100000}.
\]
Therefore, the probability that you have cavitosis, given that you have tested positive for it, is
\[
\frac{97}{1096} = 8.85\cdots\%.
\]
}
In other words, you should be concerned, and if your doctor prescribes antibiotics then you should take them,
but {\itshape it is still more likely than not that you don't have the disease}. In particular, it is much more likely
that the test resulted in a {\itshape false positive}.

A false positive rate of $1$ in $100$ sounds pretty good, but {\itshape out of those who test positive} 
this results in a $11$ in $12$ failure rate of the test!

\medskip
We now state a theorem that could have been used to derive this result more quickly.

\begin{important}
\begin{theorem}[Bayes' Theorem] 
Suppose that $A$ and $B$ are any two events. Then we have
\[
P(A|B) = \frac{ P(B|A) P(A) }{ P(B) }.
\]
\end{theorem}
\end{important}
It is easy to understand why this is true. The left hand side is given by
\[
P(A|B) = \frac{ P(A \cap B)}{P(B)},
\]
and the right side is equal to
\[
\frac{ \frac{P(B \cap A)}{P(A)} P(A)}{P(B)},
\]
which we immediately see is the same thing.

\begin{example} Here is an example borrowed from Wikipedia. Suppose that 
the probability that any one person has cancer is 1 in 100.

Now suppose a 65 year old goes to the doctor to see if she has cancer. 
She knows that 0.2\% of all people are age 65, and of those who test positive
for cancer, 0.5\% are age 65.\footnote{Note that the above probabilities are all
things one could probably look up in books or on the internet!}
\end{example}

{\bf Solution.} 
We can apply Bayes's Theorem to compute the probability that she has cancer. Let
$A$ be the event that she has cancer, and $B$ be the event that she is 65 years old.\footnote{What is it
mean to talk about $P(B)$ here? After all, if we've said she's 65 years old, then the probability of her
being 65 is 100\%, right?

The proper interpretations of these probabilities are as proportions of the population at large. This is the
context in which our 0.2\% and 0.5\% estimations make sense.} Then, we have
\[
P(A) = 0.01, \ \ \ P(B) = 0.002, \ \ \ P(B | A) = 0.005,
\]
\[
P(A|B) = \frac{ P(B|A) P(A) }{ P(B) } = \frac{ 0.005 \times 0.01 }{ 0.002} = 0.025.
\]
So there is a 2.5\% chance she (a random 65-year-old) has cancer.

\medskip
Note that we can rewrite Bayes's rule slightly as 
\[
P(A|B) = P(A) \times \frac{ P(B|A) }{ P(B) },
\]
which allows us to to interpret Bayes' theorem more natiurally. Here $P(A)$ is the {\itshape prior} (or {\itshape base})
probability of having cancer, i.e., the probability that a person about whom you don't know anything
has cancer. The ratio $\frac{ P(B|A) }{ P(B) }$, then, tells you how much more (or less) likely knowing $B$
makes $A$. For example, in the example above we have
\[
\frac{ P(B|A) }{ P(B) } = \frac{0.005}{0.002} = 2.5.
\]
So we can say that {\itshape being 65 years old makes it 2.5 times as likely that you will have cancer}.


\begin{example} We apply this to the cavitosis example above. Let $A$ be the event that we have cavitosis, and let $B$ be the event that we
test positive for it. We are interested in computing $P(A|B)$ -- the probability of having the disease, given a positive test.
\end{example}

{\bf Solution.}
By Bayes's theorem, we have
\[
P(A|B) = \frac{P(B|A) P(A) }{ P(B) } = \frac{ 0.97 \times \frac{1}{1000}}{???}
\]
Here we need to compute $P(B)$. It isn't given to us, and we need to compute it in the same way we did above.
There are two ways we could test positive for cavitosis: either we have the disease and tested positive for it,
or we don't but got a false positive.

In other words, we have
\[
P(B) = P(B|A) P(A) + P(B|\neg A) P(\neg A).
\]
Here the symbol $\neg$ means {\itshape not}. The above formula enumerates the two ways in which we might have
tested positive for cavitosis.

These probabilities we all computed above, and we have
\[
P(B) = P(B|A) P(A) + P(B|\neg A) P(\neg A) = 0.97  \times \frac{1}{1000} + 0.01 \times \frac{999}{1000} = .01096.
\]
So we have
\[
P(A|B) = \frac{P(B|A) P(A) }{ P(B) } = \frac{ 0.97 \times \frac{1}{1000}}{.01096} = \frac{97}{1096} = 0.0885\dots
\]

\medskip
Note that {\itshape this is the same way we computed it before}, only introducing notation and the formalism of Bayes' Theorem.
The theorem hopefully helps us better understand the {\itshape principle} of the computation we did before.

Note that it is interesting to write Bayes's theorem in the form
\[
P(A|B) = \frac{P(B|A) P(A) }{  P(B|A) P(A) + P(B|\neg A) P(\neg A) }.
\]
In the denominator we enumerate, and compute the probabilities of, the two ways in which B can happen, and the numerator shows
just the one that involves A being true. (Sometimes we have to enumerate more than two possibilities in the denominator.)

\bigskip
{\itshape Elections and polling.} As this chapter was being written, the U.S. 2016 Presidential election was underway\footnote{and we refer to it henceforth in the present}.
The candidates are Hillary Clinton and Donald Trump, and the race is receiving an {\itshape extraordinary} amount of attention. 

A popular site is {\bf FiveThirtyEight}, started by Nate Silver:
\urlc{http://fivethirtyeight.com}
Silver (and others writing for the site) have created a mathematical model which uses polling data to track the outcome of the election, and Bayesian inference
is at the heart of how Silver's model works.

For example, consider the following (hypothetical) situation. Imagine that Clinton and Trump are tied on the eve of the first debate,
and at the first debate Clinton promises to give every American a free puppy. How will Americans react to this? Maybe they think puppies are adorable
and now are eager to vote for Clinton. Maybe they dread cleaning up all the dog poop, and are thus more inclined to vote for Trump. If you're Nate Silver, you don't try to figure this out -- you just pay
attention to the polls.

Polls have a {\itshape margin of error}. Suppose you poll $1000$ people, among a population which is divided 50-50. What's the probability at least
$520$ express support for Clinton?\footnote{In a sense, you know how to do this already. It is the same as the probability of flipping a coin a thousand times,
and getting 520 or more heads, so
\[
\frac{C(1000, 520) + C(1000, 521) + C(1000, 522) + \cdots}{2^{1000}}.
\]
But that's a mess to compute, and we're interested in approximate (and, as it turns out, extremely close) models.} Roughly speaking, the margin of error
is about $\frac{1}{\sqrt{n}}$, where $n$ is the number of people polled. So, if you poll $1000$ people, your margin of error is about $\frac{1}{\sqrt{1000}} = 0.0316\cdots = 3.16\%$.
The true error could be higher -- by some freak accident, you could reach a thousand Trump supporters in a row, just like you {\itshape could} flip a coin and get
a thousand consecutive tails. The margin of error represents the range of outcomes you wouldn't be too surprised by. So, here in this 1,000 person poll,
you expect roughly 470 to 530 to express support for either candidate. If you get 550 Clinton supporters, that is pretty strong evidence that public opinion has
shifted. If you get 520 Clinton supporters, then that is {\itshape some} evidence that public opinion has
shifted. 

To give a flavor of how these computations look, consider the following simplified model of polling: we assume that there is a 60\% chance that a poll is accurate,
a 20\% chance that it is three points too high, and a 
a 20\% chance that it is three points too low.

Meanwhile, back to our debate and Clinton's puppy promise. 

\begin{example}
We'll say that there is a 30\% chance this lowered Clinton's support three points, 30\% chance that it
raised it three points, and 40\% that it didn't make a difference.

Suppose then that a post-puppy poll comes out showing Clinton at 53\%. What is the probability that her support actually increased?
\end{example}

{\bf Solution.} Let $A_1$, $A_2$, and $A_3$ be the events that Clinton is now at 53\%, 50\%, and 47\% respectively,
and $B$ be the event that she polled at 53\%. We want to compute
$P(A_1 | B)$, and we have
\[
P(A_1 | B) = \frac{ P(B | A_1) P(A_1) }{ P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + P(B | A_3) P(A_3)}.
\]
We know every quantity on the right side of this equation. We get
\[
P(A_1 | B) = \frac{ 0.6 \times 0.3 }{ 0.6 \times 0.3 + 0.2 \times 0.4 + 0 \times 0.3} = 
\frac{0.18}{0.26} = \frac{18}{26} = 69.2\dots\%.
\]
Note the zero in the denominator -- according to our model, polls can not be off six points so we know Clinton's support
didn't increase. The two possibilities are that Clinton's support genuinely went up, or that the polling was too high,
and our computation tells us that the first is a somewhat more likely outcome.

In real life the problem is (essentially) {\itshape continuous} rather than {\itshape discrete}: the poll could
have been 51.2\%, or 50.4\%, or 47.7\%, or .... and there is no theoretical limit to how much it can be off.
To learn more about how to adjust for this probability, I recommend a course in statistics.

\subsection{Monty Hall Revisited}
We now return to further discussion of the Monty Hall problem. Our first order of business is
to give a solution using Bayes' Theorem.

Again suppose that you have chosen Door 1, and Monty opens Door 3 (which contains a goat) and offers
you the opportunity to switch. Should you switch?

Let $B$ be the event that the car is behind Door 2, and let $A$ be the event that Monty shows you Door 3.
The probability that switching will pay off is $P(B | A)$ --
the probability that the car is behind Door 2, given that Monty showed you Door 3.

We have\footnote{Note that the roles of $A$ and $B$ are switched here.}
\[
P(B | A) = \frac{ P(A | B) P(B)}{P(A)}.
\]
In this formula, we have:
\begin{itemize}
\item
$P(A | B)$ is $1$. We're assuming here that Monty shows you a door, other than the one you picked initially,
which does not have the car. If the car is behind Door 2, then this must be Door 3.
\item $P(B)$ is $\frac{1}{3}$ -- this is the {\itshape initial} probability the car was behind door $2$. This is
one in three for all the doors.
\item $P(A)$ is $\frac{1}{2}$ -- if Monty's behavior is random, and we don't know where the car is, then Monty
is equally likely to show you either of the two doors you didn't pick.
\end{itemize}
So we have
\[
P(B | A) = \frac{ P(A | B) P(B)}{P(A)} = \frac{ 1 \times \frac{1}{3} }{ \frac12} = \frac23.
\]

This is perhaps more illuminating in the form
\[
P(B | A) = P(B) \times \frac{ P(A | B)}{P(A)}.
\]
Here $P(A)$ is $\frac{1}{2}$. We don't know which of the two remaining doors Monty will show us. But {\itshape if}
the car is behind Door 2, then this {\itshape raises} the probability that he'll show you Door 3. Since this is
what you in fact observed, it makes it more likely that the car was behind Door 2.

\medskip
To help further understand this, now let $B$ be the event that the car is behind Door 1. This is less likely
than Door 2, and we can see why our previous computation changes:
\begin{itemize}
\item $P(B)$ is still $\frac{1}{3}$ -- this is the {\itshape initial} probability the car was behind door $1$. This is
one in three for all the doors.
\item $P(A)$ is still $\frac{1}{2}$ -- if Monty's behavior is random, and we don't know where the car is, then Monty
is equally likely to show you either of the two doors you didn't pick.
\item
$P(A | B)$ is now $\frac12$ instead of $1$. Since the car is assumed to be behind Door 1, Monty chooses one of
the two remaining doors at random. He is not forced to show you Door 3.
\end{itemize}

In other words, the {\itshape prior} probabilities of being behind Door 1 or Door 2 were each $\frac13$.
The equation is: did the fact that Monty showed you Door 3 make either of these more likely? It did {\itshape not}
make Door 1 more likely, but it {\itshape did} make Door 2 more likely because this {\itshape would have forced
Monty to show you Door 3}, i.e., it made the {\itshape sequence of events you actually observed more likely}.

\bigskip {\bf Just how does Monty behave?}
If you think you've {\itshape finally} understood the Monty Hall problem, now I'm going to confuse you.

\begin{important} 
{\bf The Monty Hall Problem -- Zonk!} Monty shows you three doors, behind one of which is a car.
You pick Door \#1, and Monty shows you Door \#3, behind which is -- the car!

You lose. Zonk.
\end{important}

This violates the {\itshape assumptions} we made about Monty's behavior. But are we so sure they were correct?

Jeffrey Rosenthal introduces\footnote{Problem statements produced verbatim from his
{\itshape Monty Hall, Monty Fall, Monty Crawl}, Math Horizons, September 2008.
Also available here: \url{http://probability.ca/jeff/writing/montyfall.pdf}} two variations of the Monty Hall problem.
The first is the

\medskip
{\bf The Monty Fall Problem:} 
In this variant, once you have selected one of the three doors, the host slips on a banana peel and accidentally pushes open another door, 
which just {\itshape happens} not to contain the car. {\itshape Now} what are the probabilities that you will win the car if you stick 
with your original selection, versus if you switch to the remaining door?

\medskip
{\bf The Monty Crawl Problem:} 
As in the original problem, once you have selected one of the three doors, the host then reveals one non-selected door which does not contain the car. 
However, the host is very tired, and {\itshape crawls} from his position (near Door \#1) to the door he is to open. In particular, if he has a choice of doors to 
open (i.e., if your original selection happened to be correct), then he opens the {\itshape smallest number} available door. 
(For example, if you selected Door \#1 and the car was indeed behind Door \#1, then the host would always open Door \#2, never Door \#3.) 
What are the probabilities that you will win the car if you stick versus if you switch?

\medskip
The Monty Crawl Problem is easy: By assumption, Monty {\itshape definitely} would have opened Door \#2 if it didn't contain the car.
Since he skipped by it, you deduce that it contains the car.

For the Monty Fall Problem, again let $B$ be the event that the car is behind Door 2, and let $A$ be the event that Monty shows you Door 3,
{\itshape and reveals a goat.} The probability that switching will pay off is $P(B | A)$ --
the probability that the car is behind Door 2, given that Monty showed you Door 3.

We have still
\[
P(B | A) = \frac{ P(A | B) P(B)}{P(A)},
\]
and now
\begin{itemize}
\item $P(B)$ is $\frac13$, same as it ever was.
\item $P(A|B)$ is $\frac13$: Monty trips and opens one of the doors. Door 3 is as likely as the other two, so the probability is $\frac13$.
\item $P(A)$ is now $\frac29$: The probabilities that Monty shows you Door 3, and that it contains a goat, are now independent. Their
product is $\frac13 \times \frac23 = \frac29$.
\end{itemize}
So the conditional probability is
\[
P(B | A) = \frac{ \frac13 \times \frac13}{\frac29} = \frac12.
\]
This should be intuitive. There is nothing deliberate about Monty's decision, nothing to inform you that Door 1 or Door 2 has become
more likely. But do not in this case that the probability of {\itshape Door 1} containing the car has {\itshape gone up} (also to $\frac12$).
Here, there was some possibility that Monty's slip and fall might have revealed the car. The fact that no car appeared meant that the
odds of your {\itshape existing} choice being correct went {\itshape up}.

\medskip
And, finally, 
\smallskip
{\bf The Monty Hell Problem.}\footnote{Taken from the Wikipedia page. See also the very enligthening article
by John Tierney in the {\itshape New York Times}, July 21, 1991. Available here:
\url{http://www.nytimes.com/1991/07/21/us/behind-monty-hall-s-doors-puzzle-debate-and-answer.html}}
Monty doesn't want to give you a car. So he makes up his mind to do the following: if you initially 
pick a door with a goat, then show you the goat. You lose. Zonk. But if you pick the door with the car, then Monty
will do everything he can to get you to switch.

\medskip
In this case, needless to say, if Monty asks you to switch {\itshape then you definitely shouldn't}. So the moral is that {\itshape
the solution to the problem hinges on your assumptions about his behavior.}

\subsection{Exercises}

The following problems emphasize Bayes' theorem and what it describes about conditional probability. {\itshape Instructions}:
For each problem, before you work out the details, guess the answer and write down your guess. Then, after you get the answer,
compare this to your guess and briefly describe what your competition tells you.

\begin{enumerate}
\item Consider the simplified polling example from before. One poll (after Clinton's promise of puppies for all) showed Clinton with
53\% support, and we computed that this reflects Clinton's actual level of support with 69.2\% probability, and that the candidates are
tied with probability 30.8\%.

Suppose now that a second poll comes in, again showing Clinton with 53\% support. Now compute the probability that this is her true level
of support.

Finally, suppose that a third poll comes in, this time showing the candidates tied. Now compute the probability that this is her true level
of support.

When FiveThirtyEight adjusts each candidate's probability of winning, this is what it is doing!

\item Recall the experiment we conducted in class: I flipped a coin repeatedly, and it came up heads every time! Eventually it occurred
to you that the experiment was rigged -- I was ignoring the actual result of the coin flip and just telling you that it was heads every time.

Assume there is a prior probability of 95\% that I was conducting the experiment honestly, and a 5\% chance that I was cheating and would always
say it comes up heads. For each $n = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10$, after you have seen $n$ heads, compute the probability that the experiment
is rigged.

(Hint: around $n = 4$ or $5$, you should definitely suspect it but be unsure. By $n = 10$, there is a small possibility I am telling the truth, but
at this point you should be reasonably confident that I'm lying.)

\item You decide to conduct your own experiment: You pull a coin out of your pocket, keep flipping it, and it keeps coming up heads over and over!

This time, since you produced the coin yourself, 
you estimate the prior probability as one in a million that by some freak chance this coin had heads on both sides. Now estimate the probability
of this, after $n = 0, 5, 10, 15, 20, 25, 30$ flips.

\item As a Gamecock football fan, in August you look at Clemson's team and you wonder if they are incompetent. You guess that
with 50\% probability they are and with 50\% probability they aren't.

Clemson's first game of the season is against Podunk State. If they are incompetent, they have a 20\% chance of winning; otherwise, they
have a 70\% chance of winning.

Suppose that Clemson loses its game. How do you revise your estimates as to the probability that Clemson is incompetent?

\item \st{In a variation of Let's Make A Deal, Monty shows you {\itshape four} doors, one of which contains a car. You pick Door 1,
and then Monty chooses at random one of the other doors which does not contain a car (let's say Door 4) and shows you that it contains
a goat instead. 

The prior probabilities of each door containing the car were $\frac{1}{4}$. What are the revised probabilities now, and should you switch?}

\item Here is a clip of the game Barker's Markers (from The Price Is Right):
\urlc{https://www.youtube.com/watch?v=XlTaZlk3mz8}

\begin{enumerate}
\item
Assume that the contestant has no idea how much the items cost and guesses randomly.
Also assume that the producers choose randomly two of his correct guesses and reveal them.

Explain why he has a winning probability of $\frac34$ if he switches. (Use Bayes' theorem!)

\item
Watch several clips of this game (also known as `Make Your Mark' during the Drew Carey era). Determine,
as best as you can, the extent to which these assumptions are accurate. In the clips you watch, do you think
the contestant should indeed switch?
\end{enumerate}


\item \st{Consider a further variation of Let's Make a Deal with four doors, this time {\itshape two} of which contain cars. You pick Door 1,
and this time Monty opens a different door which contains a {\itshape car}. (If only one of the remaining doors contained a car, he shows you that one;
if two did, he chooses one of them at random.)

Compute now the probabilities that each of the doors contains a car. Should you switch?}

\item Compose, and solve, your own problem that involves Bayes's theorem, similar to the above. (Bonus points for problems which
are particularly interesting, especially realistic, or are drawn from any game show.)

\item {\bf Term project.} Send me, {\bf by e-mail}, a rough indication of what you would like to do for your term project. You are free to ask me questions,
or to suggest more than one idea if you would like advice on which is the most feasible.

Unless you prefer to work alone, please let me know whom you plan to work with (if you know). If you don't know, I'm happy to match people who
have indicated similar interests.

\end{enumerate}

\subsection{More Exercises}

\begin{enumerate}
\item The election continues, and in a following debate Trump counters Clinton's promises by promising a free kitten to every American family.
You initially estimate that with 40\% probability Trump now has a 50\% level of support, and with 30\% probability Trump now has a 52\% or 48\% level of support.

In this problem we consider a different polling model. With 60\% probability a poll will be accurate; with 15\% probability it will be too high by 2 points; with
5\% probability it will be too high by 4 points; with 15\% probability it will be too low by 2 points; with
5\% probability it will be too low by 4 points.

Four polls come in and show the following levels of respective support for Trump: 50\%, 52\%, 48\%, 54\%. After each poll, compute the probability that
Trump's support is respectively at 48\%, 50\%, or 52\%.

\item
You sit down for a game of poker with a player you have never met before. Suppose that, when first to act, an {\itshape aggressive} player will raise
70\% of the time and fold 30\% of the time; a {\itshape conservative} player will raise
40\% of the time and fold 60\% of the time.

You estimate initially that there is a 50\% chance that your opponent is aggressive. The first three times that she is first to act, she elects to raise.

After each raise, compute the revised probability that your opponent is aggressive.

\item
You are a child worried about monsters under your bed. Suppose that your knowledge of Bayesian probability outstrips your common sense, so you decide
to use Bayes' theorem to assauge your worries.

You initally estimate that with 30\% probability there are monsters under your bed, and with 70\% probability
there aren't. 

Periodically, you ask your father to look under your bed. If there are monsters under your bed, then with 90\% probability they will hide and your father won't see anything.
With 10\% probability your father will indeed see the monsters (and presumably be eaten by them).

\begin{enumerate}
\item
Suppose you ask your father to look, and on three consecutive nights he doesn't see any monsters. What do you now estimate as the probability that there are monsters under your bed?
\item
How many times must you ask your father to look before you get this probability under 1\%?
\end{enumerate}

\item You speculate that your math professor may be a zombie. At the beginning of the class, you estimate that with 20\% probability he is a zombie.

If he is not a zombie, then each class he will give a normal lecture. If he is a zombie, then with 80\% probability he will give his lecture as normal,
and with 20\% probability he will instead devour your brains.

Twenty lectures pass, and your professor has still not consumed your brains. With what probability do you now estimate that your professor is a zombie?

\item In a variation of Let's Make A Deal, Monty shows you {\itshape four} doors, one of which contains a car. You pick Door 1,
and then Monty chooses at random one of the other doors which does not contain a car (let's say Door 4) and shows you that it contains
a goat instead. 

The prior probabilities of each door containing the car were $\frac{1}{4}$. What are the revised probabilities now, and should you switch?

\item Here is a clip of the game Barker's Markers (from The Price Is Right):
\urlc{https://www.youtube.com/watch?v=XlTaZlk3mz8}

\begin{enumerate}
\item
Assume that the contestant has no idea how much the items cost and guesses randomly.
Also assume that the producers choose randomly two of his correct guesses and reveal them.

Explain why he has a winning probability of $\frac34$ if he switches. (Use Bayes' theorem!)

\item
Watch several clips of this game (also known as `Make Your Mark' during the Drew Carey era). Determine,
as best as you can, the extent to which these assumptions are accurate. In the clips you watch, do you think
the contestant should indeed switch?
\end{enumerate}


\item Consider a further variation of Let's Make a Deal with four doors, this time {\itshape two} of which contain cars. You pick Door 1,
and this time Monty opens a different door which contains a {\itshape car}. (If only one of the remaining doors contained a car, he shows you that one;
if two did, he chooses one of them at random.)

Compute now the probabilities that each of the doors contains a car. Should you switch?

\item Compose, and solve, your own problem that involves Bayes's theorem, similar to the above. (Bonus points for problems which
are particularly interesting, especially realistic, or are drawn from any game show.)

\end{enumerate}

\section{Competition}

\subsection{Introduction}

The following clip is from the final round of the British Game Show {\itshape Golden Balls}.

%\urlc{https://www.youtube.com/watch?v=yM38mRHY150}

\urlc{https://www.youtube.com/watch?v=tYYSu6PkyDs}

\newgame{Golden Balls (Final Round)}{
Two players play for a fixed jackpot, the amount of which was determined in earlier rounds. They each have two balls,
labeled `split' and `steal'. They are given some time to discuss their strategies with each other. Then, they each secretly 
choose one of the balls and their choices are revealed to each other.

If both choose the `split' ball, they split the jackpot. If one chooses the `split' ball, and the other `steal', the player choosing
`steal' gets the entire jackpot. If both players choose `steal', they walk away with nothing.
}

In the video, the players are competing for a prize of $8,200$ pounds (roughly \$11,000 in US currency). We can summarize
the game in a two-by-two grid that describes the possible choices for you and for your opponent, and the outcome
of each choice.

\begin{center}
\begin{game}{2}{2}[\textbf{Opponent}][\textbf{You}]
           & Share & Steal \\
  Share & 5500        & 11000        \\
  Steal & 0        & 0        
\end{game}
\end{center}

Now, assuming that you don't care about your opponent one way or another, and want only to maximize your own expected
payoff, your optimal strategy is clear. If your opponent steals, it doesn't matter whether you steal or share. If your opponent
splits, then you do better if you steal. {\itshape Therefore, it is clear that you should always choose the steal ball.}

Of course, your opponent will reason in exactly the same way. Therefore, your opponent will deduce that she should choose
the steal ball, {\itshape and therefore with optimal strategy both of you will choose `steal' and win nothing.}

Here is an amusing video of the same game being played by two other contestants:

\urlc{https://www.youtube.com/watch?v=S0qjK3TWZE8}

You can also find other videos of this game on YouTube, many of which will do somewhat less to affirm your faith in humanity.

\medskip
{\itshape Setup and notation.} This is an example of a two-player {\itshape strategic game}, of the type studied in the subject known as {\itshape game theory}.
Although this entire course is about games, mathematical `game theory' refers to this sort of analysis: you have a game involving two or more players, and
you have distilled the analysis down to the point where you know what will happen depending on your, and your opponents', choice of strategy.

A {\itshape strategic game} consists of the following inputs:
\begin{itemize}
\item
Two or more players. Here we will only look at two player games.
\item
For each player, two or more choices of strategy. (The players choose their strategies independently and simultaneously.)
For example, in the Golden Balls example, each can choose `Share' or `Steal'.
Typically, the strategies are the same for each player, but this doesn't have to be the case.
\item
A {\itshape payoff matrix}, such as the one above, describing the payoff to each player.
\end{itemize}

In the above, we listed only the payoff to the {\itshape first} player. To be more precise, we list the payoffs to both players as follows:
\begin{center}
\begin{game}{2}{2}[\textbf{Opponent}][\textbf{You}]
           & Share & Steal \\
  Share & 5500, 5500        & 11000, 0        \\
  Steal & 0, 11000        & 0        
\end{game}
\end{center}

This is an example of a {\itshape non-zero-sum game}. Many games (and especially those that we describe colloquially as
`games') are {\itshape zero-sum} in the sense that what is good for one player is equally bad for the other player. For example,
if one team wins the World Series, then the other team loses. In most board games, there is one winner and everyone else loses.\footnote{
This is not true of all board games. For example check out {\itshape Republic of Rome}, described here -- \url{https://boardgamegeek.com/boardgame/1513/republic-rome} --
where there can be at most one winner, but where it is possible for all players to lose.}

\subsection{Examples of Strategic Games}

We now give some common examples of strategic games and discuss their optimal strategy.

\bigskip \noindent
{\bf Example 1. The Prisoner's Dilemma.} This is essentially the same as the `Golden Balls' game discussed above, and is perhaps
the most familiar example of a strategic game.
One formulation\footnote{Taken from the Wikipedia article.} of the Prisoner's Dilemma is as follows.

\medskip
Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge. They hope to get both sentenced to a year in prison on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to: betray the other by testifying that the other committed the crime, or to cooperate with the other by remaining silent. The offer is:
\begin{itemize}
\item
If A and B each betray the other, each of them serves 2 years in prison.
\item
If A betrays B but B remains silent, A will be set free and B will serve 3 years in prison (and vice versa).
\item
If A and B both remain silent, both of them will only serve 1 year in prison (on the lesser charge).
\end{itemize}

The payoff matrix for this game as follows:
\begin{center}
\begin{game}{2}{2}[B][A]
           & Betray & Silent \\
  Betray & -2, -2      &  -3, 0        \\
  Silent & 0, -3        & -1, -1        
\end{game}
\end{center}

Mathematically speaking, the game is trivial. Each prisoner should betray the other. This invites a very serious paradox,
as well as questions about whether the model is realistic. (If you betray your accomplice and he remains silent, you had
better get the hell out of town as soon as you are released from jail.) It is also interesting if the game is played multiple
times consecutively. But we won't pursue these questions here.

\bigskip \noindent
{\bf Example 2. Rock, Paper, Scissors.} This is a familiar game and we can describe its payoff matrix immediately.
\begin{center}
\begin{game}{3}{3}[B][A]
           & Rock & Paper & Scissors \\
  Rock & 0, 0      & 1, -1 & -1, 1       \\
  Paper & -1, 1        & 0, 0   & 1, -1  \\
Scissors & 1, -1        & -1, 1   & 0, 0      
\end{game}
\end{center}

This is a stupid game of pure luck, but don't tell these people:
\urlc{https://www.youtube.com/watch?v=nGYqSqf0yCY}

Note that this is also an example of a {\itshape zero-sum game}: a win for you is equivalent to a loss for your opponent.

If you try to get too clever, then your opponent can outwit you. Best to assume your opponent is the smartest person
on earth. Therefore, just play a random strategy: play rock, paper, scissors randomly with probability $\frac13$ chance each.
Then, on average, you can't win, but you can't lose either.

\bigskip \noindent
{\bf Example 3: Chicken.} The game of {\bf Chicken} is illustrated by this clip\footnote{Thanks to Kevin Bartkovich, who
taught me this subject and who used this very clip for this example.} from the movie {\itshape Rebel Without a Cause}.

\urlc{https://www.youtube.com/watch?v=u7hZ9jKrwvo}

\medskip
Two teenagers challenge each other, in front of all of their friends, to the following contest. They start far apart and drive
their cars at maximum speed towards each other. If one swerves and the other one does not, then the driver who swerves is
the `chicken' and loses face among his friends, while the other enjoys increased social status.

If neither swerves, the resulting accident kills them both.

\medskip

The payoff matrix for this game might be described as follows:
\begin{center}
\begin{game}{2}{2}[B][A]
           & Swerve & Straight \\
  Swerve & 0, 0      &  1, -1        \\
  Straight & -1, 1        & -100, -100        
\end{game}
\end{center}

\bigskip \noindent
{\bf Example 4: Final Jeopardy (Simplified).}

Here is a clip of the final round from the game show {\bf Jeopardy}:

\urlc{https://www.youtube.com/watch?v=p-jFBEozxWk}

And here is a particularly dramatic one:

\urlc{https://www.youtube.com/watch?v=8MwVgf2AzcQ}

\newgame{Jeopardy -- Final Jeopardy}{Three players come into the final round with various
amounts of money. They are shown a category and write down a dollar amount (anything up
to their total) that they wish to wager.

After they record their wagers, they are asked a trivia question. They gain or lose the amount
of their wager, depending on whether their answer was correct. Only the top finisher gets to
keep their money.}

This is a complicated game to analyze in full detail, so we consider the following toy model of it:

{\itshape Two players, A and B, start with 3 and 2 coins respectively. Each chooses to wager none, some, or all of her coins,
and then flips a coin. If the coin comes up heads, she gains her wager; tails, she loses it.

How should each player play to maximize her chances of winning (counting a tie as half a win)?}

We can compute the payoff matrix for the game as follows:
\begin{center}
\begin{game}{3}{4}[B][A]
           & 3 & 2 & 1 & 0 \\
  2 & $\frac58$ & $\frac68$ & $\frac58$ & $\frac48$        \\
  1 & $\frac48$ & $\frac58$ & $\frac68$ & $\frac68$        \\
  0 & $\frac48$ & $\frac48$ & $\frac68$ & $1$        
\end{game}
\end{center}
Here only the payoffs for A are listed; the payoffs for B are the negatives of these. (Since we stipulated that
each player wants only to win, the game is a zero-sum game: your opponent's gain is your loss.)

This takes a little bit of work to compute. For each combination of wagers there are four possibilities:
A and B both gain, A and B both lose, only A gains, only B gains. We add $\frac14$ for each such scenario
in which A wins, and $\frac18$ for each such scenario in which they are tied.

We can see immediately that A should never wager all three coins: she always does at least as well, and depending on 
B's strategy possibly better, by instead wagering two coins. 

But between the other strategies it is not obvious what the players should choose: your best strategy depends on
your opponent's best strategy, in a somewhat complicated way. We will come back to this later.


\subsection{Nash Equilibrum}
These games are very interesting if done {\itshape repeatedly} or there are additional factors (such as altruism) involved. We will assume that
the game is played {\itshape once} and each player plays exclusively for his or her own self-interest.

The obvious question for a game is: {\bf what is the optimal strategy}? We cannot hope to define that precisely, so we define a mathematically
rigorous, more precise notion that captures some of what we mean by `optimal'. We will imagine that our opponent is a genius, or perhaps a mindreader, 
and 
focus on playing defense: making sure that our opponent can't outplay us.


We will allow each player to choose a {\bf mixed} (i.e., random) {\bf strategy}. For example, if you are playing Spilt or Steal you might choose the
non-random strategy of always playing Steal. But if you are playing Rock, Paper, Scissors and you always make the same play then you can be exploited
by a sufficiently clever opponent. If we are playing defensively, we should choose rock, paper, and scissors each with $\frac13$ probability.


\begin{important}
\begin{definition}
By a {\bf mixed strategy} we mean an assignment of a probability (between $0$ and $1$, inclusive) to each possible strategy. 
\end{definition}

\begin{definition} Suppose you and your opponent each choose a (mixed) strategy for a game. Then these strategies form a {\bf Nash equilibrum}\footnote{Named
after John Forbes Nash, as depicted in the movie {\itshape A Beautiful Mind}.} if: your current strategy is optimal against her current strategy, and her current strategy
is optimal against your current strategy.
\end{definition}
\end{important}
In other words, given your strategy, your opponent can't do any better than her current strategy, and vice versa. (Being at a Nash equilibrium doesn't mean that you
couldn't both do better by both adjusting your strategy.)

\medskip
{\bf Example.} Suppose you play Split or Steal. Then a Nash equilibrium is both players choosing to steal with probability $1$. If your opponent always steals,
then it doesn't matter what you do, so actually {\itshape every} strategy is optimal for you. You can't do any worse (or better) than always stealing.

\medskip
{\bf Example.} Suppose you play Rock, Paper, Scissors. Then a Nash equilibrium is a mixed strategy: both players choosing rock, paper, and scissors with probability
$\frac{1}{3}$ each. If you do this, then your opponent's strategy doesn't matter: the game is, on average, a draw.

Note that no other choices of strategy form a Nash equilibrium. Why is this? Suppose that you choose a different strategy. Then the probabilities will be uneven,
so let's say your strategy is biased towards Rock. Then your opponent should switch immediately to playing Paper all the time. But then you should switch to
playing Scissors all the time! And so your opponent should switch to Rock all the time. And so on, ad infinitum. The game is not at equilibrium.

\medskip
{\bf Example.} Here is the game of {\itshape Stag Hunt}. You and another player have to choose to either cooperate and hunt a stag, or to hunt a rabbit on your own.
You can catch a rabbit by yourself, but you need the other player's cooperation to successfully hunt the stag. The payoff matrix is as follows.


\begin{center}
\begin{game}{2}{2}[They][You]
           & Stag & Rabbit \\
  Stag & 2, 2      &  1, 0        \\
  Rabbit & 0, 1        & 1, 1       
\end{game}
\end{center}
Obviously, you both should cooperate to hunt the stag. This choice is a Nash equilibrium: if your opponent has decided to cooperate, then you want to cooperate with her too.

However, both players hunting the rabbit is also a Nash equilibrium. If your opponent has decided to hunt the rabbit instead of the stag, then that is important, but you
have to do the same thing (or go without food entirely).

\medskip
{\bf Example.} You and another player place a coin heads or tails, each invisibly to the other. You have no chance to communicate beforehand.
If you both make the same choice, then you get a reward; otherwise, nothing happens.

The payoff matrix for this game is as follows.

\begin{center}
\begin{game}{2}{2}[They][You]
           & Heads & Tails \\
  Heads & 1, 1      &  0, 0        \\
  Tails & 0, 0        & 1, 1       
\end{game}
\end{center}
The game has two Nash equilibria: you both pick heads, or you both pick tails. This is somewhat similar to the Stag Hunt, but here it is not obvious which you should pick.

\medskip
Now we come back to Chicken. We describe a less morbid game along the same lines -- sort of like Share or Steal, but with a bit more bite. 
A referee asks you to choose (independently) whether to be nice or mean. If you are both nice, the referee gives you each a dollar. If the other player
is nice and you are mean, then the referee gives you her dollar too. But if you are both mean, the referee takes five dollars from both of you.

All of this is described by the following payoff matrix.

\begin{center}
\begin{game}{2}{2}[B][A]
           & Nice & Mean \\
  Nice & 1, 1      &  2, 0        \\
  Mean & 0, 2        & -5, -5        
\end{game}
\end{center}
Note that `both nice' is not a Nash equilibrium. If the other player is always nice, you should be mean to take her money. Similarly, `both mean'
is also not a Nash equilibrium. If your opponent is determined to always be mean no matter what, then you should give in and be nice, so that you avoid losing anything.

However, it {\itshape is} a Nash equilibrium for you to always be nice and your opponent to always be mean, or vice versa. If your opponent is always mean, 
then you stave off the damage by always being nice. Conversely, if you decide to always play nice, then -- in the dog-eat-dog world of theoretical game theory,
there is no reason for your opponent not to exploit you.

Are there any Nash equilibria in the middle -- where you each play a mixed strategy? Suppose your opponent plays a mixed strategy where she is nice with probability
$\alpha$, and mean with probability $1 - \alpha$.

Then, the expected value of you being nice is
\[
1 \cdot \alpha + 0 \cdot (1 - \alpha) = \alpha,
\]
and the expected value of you being mean is
\[
2 \cdot \alpha + (-5) \cdot (1 - \alpha) = 7 \alpha - 5.
\]
If $\alpha > 7 \alpha - 5$, then you should always be nice; if $\alpha < 7 \alpha - 5$, then you should always be mean.

The interesting case is when these are equal, i.e., when $\alpha = 7 \alpha - 5$. (Doing the algebra, this is the same as saying $6 \alpha = 5$,
or $\alpha = \frac56$.) If your opponent is nice with probability $\frac56$, then every strategy has the same payoff: the extra dollar you get from
being mean exactly balances the occasional big loss. So you may as well match her strategy, and this is also a Nash equilibrium. 

Indeed, in some sense it is the best one. With this mixed strategy, the probabilities of each combination are
\begin{center}
\begin{game}{2}{2}[B][A]
           & Nice & Mean \\
  Nice & $\frac{25}{36}$      &  $\frac{5}{36}$         \\
  Mean & $\frac{5}{36}$        & $\frac{1}{36}$      
\end{game}
\end{center}
and the expected payoff of the game is
\[
\frac{25}{36} \cdot 1 + \frac{5}{36} \cdot 2 + \frac{5}{36} \cdot 0 + \frac{1}{36} \cdot (-5) = \frac{30}{36} = \frac{5}{6}
\]
to each player. It is not as good as if you both play `Always Nice', but with this choice of strategy you do pretty well, {\itshape and}
you ensure that you cannot be exploited.

\medskip
{\itshape Final Jeopardy}. We finally return to our Final Jeopardy model: A has 3 coins, and B starts with two. Each places a wager
and either wins or loses that many coins with 50-50 probability, and wants to finish with more coins than their opponent. (A tie counts as half a win.)

\begin{center}
\begin{game}{3}{3}[B][A]
           & 2 & 1 & 0 \\
  2 & $\frac68$ & $\frac58$ & $\frac48$        \\
  1  & $\frac58$ & $\frac68$ & $\frac68$        \\
  0 & $\frac48$ & $\frac68$ & $1$        
\end{game}
\end{center}
Recall that we didn't include the possibility that A wager all three coins because it was {\itshape dominated} by the choice of wagering two
coins. Wagering two is at least as good in all situations, and in some situations better.

For simplicity we multiply all the payoffs by $8$ and subtract $4$. This results in a nicer-looking payoff matrix, without changing the essence of the problem.
\begin{center}
\begin{game}{3}{3}[B][A]
           & 2 & 1 & 0 \\
  2 & 2 & 1 & 0       \\
  1  & 1 & 2 & 2       \\
  0 & 0 & 2 & 4        
\end{game}
\end{center}

We want to find a Nash equilibrium. We can simplify this problem by realizing that {\itshape $B$ should never wager $1$.} Why?
The payoffs are respectively 1, 2, 2 {\itshape to A} depending on A's strategy. If she is considering doing this, she should flip a coin
and wager either $0$ or $2$ with a 50-50 chance each. This results in an average payoff of 1, 1.5, and 2 which is better for B.

So we can further simplify the matrix:
\begin{center}
\begin{game}{2}{3}[B][A]
           & 2 & 1 & 0 \\
  2 & 2 & 1 & 0       \\
  0 & 0 & 2 & 4        
\end{game}
\end{center}
We can go one step further. For player A, it is not harmful to wager $1$, but she can just as well wager $0$ or $2$ with 50-50 probability.
The average payoffs are identical. So we can eliminate this strategy from A's choices as well to obtain:
\begin{center}
\begin{game}{2}{2}[B][A]
           & 2 & 0 \\
  2 & 2 & 0       \\
  0 & 0 & 4        
\end{game}
\end{center}
Finally, we can find the Nash equilibrium! We want a mixed strategy. So suppose B plays $2$ with probability $p$ and
$0$ with probability $1 - p$. Then the payoff to A from playing $2$ is
\[
2p + 0(1 - p) = 2p,
\]
and the payoff to A from playing $0$ is
\[
0p + 4(1 - p) = 4(1 -p).
\]
In a mixed strategy Nash equilibrium, A will be indifferent to these two strategies, so we must have
$2p = 4 - 4p$ -- solving this yields $p = 2/3$. So $B$ should play $2$ with probability $\frac23$ and $0$ with
probability $\frac13$.

Conversely, suppose A plays $2$ with probability $q$ and
$0$ with probability $1 - q$. Then the payoff to A (i.e. the negative payoff to B) from playing $2$ is
\[
2q + 0(1 - q) = 2q,
\]
and the payoff to A from playing $0$ is
\[
0q + 4(1 - q) = 4(1 -q).
\]
So, similarly, A should play $2$ with probability $\frac23$ and $0$ with
probability $\frac13$.

\subsection{Exercises}

\begin{enumerate}
\item
Consider our original game of Chicken:
\begin{center}
\begin{game}{2}{2}[B][A]
           & Swerve & Straight \\
  Swerve & 0, 0      &  1, -1        \\
  Straight & -1, 1        & -100, -100        
\end{game}
\end{center}
Compute the Nash equilibrium for this game (it will be a mixed strategy for both players) and the expected payoff.

\item
You and a friend were planning to meet tonight, but your cell phones are both dead and you cannot communicate. You were
either going to meet at the park and go jogging, or go to the movie theater and watch a movie.
Since you can't communicate, each of you decides to go to either the park or the theater and hope that the other decides the same.

You would slightly prefer jogging to the movies, and your friend would slightly prefer a movie to jogging. But the top priority for each
of you is meeting up, and you would rather do the other's preferred activity together than your preferred activity by yourself.

Describe this scenario as a strategic game and come up with a suitable payoff matrix. Compute at least one Nash equilibrium 
corresponding to your payoff matrix.

\item
Consider a game like rock-paper-scissors, but only with rock and paper. If you both show the same thing, then you win a prize, and if you show
different things, then your opponent wins the same prize. In addition, if you both show rock, then you get an additional bonus prize. (This does not
come at your opponent's expense, so although this outcome is better for you than if you both show paper, it is not worse for your opponent.)

Describe this scenario as a strategic game and come up with a suitable payoff matrix. Compute at least one Nash equilibrium 
corresponding to your payoff matrix.
\end{enumerate}

\section{Backwards Induction}

{\bf Example. A Money Division Game.}
Consider the following game. You and another player play for a pot of \$100. You go first, and you can propose any division
of the money between the two of you. The other player may then either accept your division, or flip a coin. If she elects to flip and flips heads,
then she gets the entire pot; if she flips tails, then neither of you gets anything.

The key to analyzing this game is {\itshape backwards induction}: figure out what your counterpart's optimal strategy is, and then base your strategy on that.
The expected value (to her) of a coin flip is \$50.00, and therefore you should offer more than that to ensure that she will take your offer. For example,
if you must divide the pot into integer amounts, you should offer to give her \$51.00 and keep \$49.00. This is better than a coin flip, so -- if she is playing rationally --
she will accept your offer.

\medskip
Now consider a three player version of the same. The three of you play for a pot of \$100. You propose any division of the money between
the three of you. The second player either accepts it, or proposes an alternative division of the money. In the latter case, the third player either
accepts that or flips a coin.

By what we have just determined, if the second player proposes a division, she should propose to keep \$49.00 and give \$51.00 to the third player.
Obviously you want to avoid this outcome since you will get nothing, So you should make a proposal with more than \$49.00 for the second player.
The best option is to propose \$50.00 for yourself and \$50.00 for the second player. The second player should then accept this deal.

\bigskip
{\bf Another Prisoner Example.} You and one other person share a prison cell. You are both very intelligent, and you have exactly the same motives.

One day the jailer comes and paints a mark on each of your foreheads -- either red or blue. You don't have any idea what color your mark is, but you can see your cellmate's -- it is red. He can also see yours.

The jailor informs you both that either of you may guess the color of your mark. If you guess right, you will be set free, but if you guess wrong, you will be executed.
You would very much like to be set free, but you even more don't want to be executed, so neither of you is willing to guess unless you are certain.

Finally, the jailor then tells you: `At least one of you has a red mark'. After a few moments, you raise your hand and inform the jailor -- correctly -- that your forehead has a red mark, and you are set free.
{\itshape How did you know?}

\medskip
The solution is to consider the problem from your cellmate's perspective. Suppose instead that you had a blue mark. Then your cellmate would see your blue mark. Since he knew
that at least one of you had a red mark, he could deduce that it must be him. So he would have immediately guessed that his own mark was red.

{\itshape He did not do so; therefore your mark is not blue.} So you can guess with confidence that it is red.

\subsection{The Big Wheel}

Part of {\itshape The Price Is Right} consists of spinning the famous {\bf big wheel}. It is played twice each show.
Most of the stand-alone
clips on Youtube feature something unusual happening, so we refer to 13:00 or 30:00 of the following clip.

\urlc{https://www.youtube.com/watch?v=qOWfz7ZN6PE}

\newgame{The Big Wheel -- Price Is Right}{
The {\bf Big Wheel} consists of twenty numbers -- 5 through 100 (i.e. five cents through a dollar), in increments of five. Three players compete,
and the player who spins the closest to a dollar without going over advances to the Showcase Showdown.

The players spin in order. Each player spins once, and then either keeps the result or elects to spin a second time
and add the two results. If the result is higher than \$1.00, the player is eliminated immediately. The winner is the player
who spins the highest without going over. (If two or more players tie, they advance to a tiebreaker.)

In addition, players earn a bonus if they spin exactly a dollar -- but we will ignore this.
}

The natural question is: how should each of the players play? To be more specific, when they are faced with the decision to
spin again or not, should they spin again?

In some ways this is like the strategic games dicussed in the previous chapter. But it has one very important difference:
{\itshape the players play sequentially rather than simultaneously}. 

We will assume that {\itshape all players understand their best possible strategy and will play it}. With that in mind, we solve
this puzzle by {\itshape backwards induction}: we start with the last player (who has by far the easiest decision) and work backwards.

\subsubsection{Player 3} This is very easy, and on the show you will even observe that Barker and Carey don't ask the
contestants what they want to do.

If you spin more than the two previous contestants (or if they busted by spinning more than \$1.00), then you win and
obviously you should not spin again. Conversely, if you spin less than one of the two previous contestants, then you lose if
you don't spin again, and you might win if you spin again, so obviously you should spin again.

You could tie. If you and one other player are tied with more than 50 cents, then you have 50-50 odds of winning a tie breaker
and less than that of not busting, so you should accept the tie and proceed to the tie-breaker. Conversely, if you and one other
player are tied with less than 50 cents, you should spin again. (If you are at exactly 50 cents, then it is a tossup.)

Similarly, if you are tied with both other players, you should spin again if the tie is 65 cents or less, and accept the tie at 70 cents
or greater.

\subsubsection{Player 2 -- Example} This is much more subtle, but once we are done we will understand how to work out
Player 1's strategy. We begin with a specific example. Assume that the first player spun 60 cents. Obviously if you spin less than
60 on your first spin, you must go again. Suppose you spin 65 cents on your first spin. Should you spin again, or hold?
To answer this question we will compute the winning probability in either outcome.

Suppose first that you hold at 65 cents. Then the third player will spin again. If she spins 65 cents or greater, she will hold,
where if she spins 60 cents or less she will spin again.

The probability that the third player will win without a tiebreaker is
\[
\frac{7}{20} + \frac{12}{20} \cdot \frac{7}{20} = \frac{14}{25} = 0.56.
\]
The first figure is the probability that she will win on the first spin; the second is the probability she will take a second spin times
the probability she will win on that spin. (Note that, no matter what she spins on the first spin, if it is less than 65 cents
there are exactly seven outcomes with which she will
win on the second spin, and so seven numbers that will put her
between 70 cents and one dollar. Other numbers will leave her too high or too low.)

The probability that the third player will force a tiebreaker is
\[
\frac{1}{20} + \frac{12}{20} \cdot \frac{1}{20} = \frac{2}{25} = 0.08.
\]
So her winning chances are $0.56 + \frac{1}{2} \cdot 0.08 = 0.6$ -- so yours are $0.4$, or 40 percent.

\medskip
Now we consider the option of spinning again. Here we can take a shortcut, and notice that in this case your odds are
less than 35\%: there is a 35\% probability that you will not bust, and if you don't bust then your opponent still has some odds
to beat you. So, although we could compute this probability we don't need to.

\medskip
So, in conclusion, if the first player spins 60 cents on her first spin, and you spin 65 cents on your second spin, you should keep
it and not attempt to spin again. Clearly this is still more true if you spin more than 65 cents on your spin, and if you spin less than
60 cents then you should go again.

What if you tie the first player? If you hold, the probability of a third player victory without a tiebreaker is
\[
\frac{8}{20} + \frac{12}{20} \cdot \frac{8}{20} = \frac{16}{25} = 0.64.
\]
Note that if the third player also spins 60 percent, then she will choose to spin again {\itshape as we analyzed previously!} 
(This is the whole 
idea of backwards induction -- we're figured out in advance how the third player will respond to any action, 
so we don't need to think about it again.) So, she will only tie if she spins 60 cents total on both spins. The probability of this
outcome is
\[
\frac{11}{20} \cdot \frac{1}{20} = 0.0275,
\]
the probability of spinning less than 60 cents on the first spin, and then on the second spin 

So, in conclusion, with probability $0.3325$ you will finish in a two-way tie, and with probability $0.0275$ you will finish in a three-way
tie. Your winning probability is therefore
\[
\frac{1}{2} \cdot 0.3325 + \frac13 \cdot 0.0275 = .175\dots
\]
{\bf Not very good.}

Suppose then you spin again. This looks pretty good, right? With probability $0.4$ you will improve your score and not be stuck in a tie
with the first player. 

With probability 0.05 each, you will improve your score to $60 + 5n$ for each of $n = 1, 2, 3, 4, 5, 6, 7, 8$. Your opponent will
spin again only if she doesn't match your score. In each case, her winning probability without a tiebreaker is
\[
\frac{8 - n}{20} + \frac{n + 11}{20} \cdot \frac{8 - n}{20} = \frac{(8 - n)(n + 31)}{400}.
\]
Her tying probability is
\[
\frac{1}{20} + \frac{n + 11}{20} \cdot \frac{1}{20} = \frac{n + 31}{400}.
\]
So her total winning probability is
\[
\frac{(8 - n)(n + 31)}{400} +  \frac{n + 31}{800}
= 
\frac{-2n^2 - 45n + 527}{800}.
\]
That probably looks very strange. For each $n$ from $1$ to $8$, this is
(to three decimal places)
\[
0.6, 0.536, 0.468, .393, .315, .231, .142, .048,
\]
and so your winning probability is
\[
0.4, 0.464, 0.534, .607, .685, .769, .858, .952.
\]
To compute your winning probability, multiply each of these numbers by 
0.05 and add the results. (Equivalently, take their average and multiply by
0.4 -- your probability of not busting.)

We get a winning probability of $0.26$. It's {\bf not very good} -- you have
a probability of only 40\% of not busting and even then you could lose 

\section{Special Topics}

In this section we treat some unusual mathematical topics which come up in game shows. (During the course,
we covered these just before the midterm; it was intended that students see these topics but not be expected to master them.)

\subsection{Divisibility Tests}
The following clip illustrates the Price Is Right game of {\bf Hit Me}.

\newgame{Hit Me (The Price Is Right)}{ The contestant plays a game of blackjack against the dealer, where the objective is to get a total
of 21 without going over. The dealer plays as in ordinary blackjack: it deals two cards at random, and if it has a total of 16 or less it keeps
drawing cards until it is over 16.  

The contestant is shown six prizes along with six prices, each of which is $n \times$ the actual price of the item. Behind each prize
is a card worth $n$. The contestant chooses prices one at a time and her hand is made up of these cards.

One of the prices will always be exactly right (so $n = 1$, and the card is ace, which in blackjack you may count as eleven), and one of them
will be ten times the right price. If the contestant picks these two prizes first, she gets a blackjack (21) and wins no matter what the dealer
has. Otherwise, she still has some opportunities to win.
}

In the clip, the contestant is shown the following prizes and prices: some kind of joint cream for \$5.58;  toothpaste
for \$14.37; some fragrance for \$64.90; a six-pack of juice for (???? -- poor camera work); 
some calcium supplements for \$76.79;  and some denture adhesive for \$27.12. 

We now ask which cards these prizes might hide. And for this we review the {\itshape divisibility tests} from number theory:
\begin{itemize}
\item A number is divisible by $2$ iff its last digit is.
\item A number is divisible by $3$ iff the sum of its digits is.
\item A number is divisible by $4$ iff its last two digits are.
\item A number is divisible by $5$ iff its last digit is.
\item A number is divisible by $6$ iff it is divisible by both $2$ and $3$.
\item There are divisibility tests for $7$, but it is probably easier to just try dividing by $7$ in your head.
\item A number is divisible by $8$ iff its last three digits are.
\item A number is divisible by $9$ iff the sum of its digits is.
\item A number is divisible by $10$ iff its last digit is $0$.
\end{itemize}
No, `iff' is not a typo. The word {\bf iff} is mathematical short-hand for {\bf if and only if}, describing a {\bf necessary
and sufficient condition}. For example, if the sum of a number's digits is divisible by $3$, then the number is divisible by $3$.
If the sum of a number's digits is {\itshape not} divisible by $3$, the number is {\itshape not} divisible by $3$.

In mathematics we are always very happy when we have necessary and sufficient conditions. Sometimes we have only
one or the other. For example, if a number ends in the digit $6$, then we know it is divisible by $2$, but vice versa.
Conversely, if we want to test if a number is divisible by $8$, we can just apply the divisibility test for $4$. If the number
is indeed divisible by $4$ then we have more work to do, but if it's not then it can't be divisible by $8$ either.

So we can use these to figure out what prices are divisible by what.
\begin{itemize}
\item
$558$ is divisible by $2$, $3$, $6$, and $9$. Pretty obviously the joint cream is not 62 cents, but it could well
be \$2.79 and so this one is a little bit tricky to guess.

So, the card could be any of the ace, two, three, six, or nine, with the ace or the two more likely.
\item
$1437$ is divisible by $3$ (only). It looks like the toothpaste is \$4.79. 
\item
$6490$ is divisible by $2$, $5$, and $10$. If the price of the juice does not end in a ten, and we know that one
of the cards is a ten (which it always is), then we know this has to be the ten.
\item
We have no idea what card the juice hides, because the camera operator is incompetent.
\item
$7679$ is divisible by $7$ (only). The cheap way to see this is to eliminate $3$ and $9$ immediately;
it's not even, it's not divisible by $5$, so $7$ is the only thing that's left unless we believe that the supplements
cost this much money. 

We can also notice that $7700$ is divisible by $7$; now subtract $21$.
\item
$2712$ is divisible by $2$, $3$, $4$, $6$, and $8$. It's difficult to guess the actual price.
\end{itemize}
We can see if that if you are willing to do some arithmetic in your head, you can do quite well in this game!

\subsection{Bonkers, Gray Codes, and Mathematical Induction}

Here is a clip of the Price Is Right game of {\bf Bonkers}:

\begin{important}
\urlc{https://www.youtube.com/watch?v=3EqBci6OQNg}
\end{important}

\newgame{Bonkers (The Price Is Right)}{
The contestant is shown a prize whose price is four digits. She is then shown a board with a
four digit price for the item. Each digit is wrong, and there are spaces to put paddles above and below
each digit.

She must guess whether each digit is too high or too low, by placing paddles in the appropriate location and hitting
a button (after which she gets the prize if her guess is right, and buzzed if it is wrong). She has thirty seconds and may
guess as many times as she is physically able to.
}

She does win the prize, but she only gets off four guesses and wins at the last second. Her strategy leaves much to be improved upon.
Here is a contestant who puts on a much better show:

\begin{important}
\urlc{https://www.youtube.com/watch?v=iZzBu5K_aBA}
\end{important}

We can ask: what's the optimal strategy? Is there an efficient, and easily remembered, algorithm to go through every possibility?

The best we can possibly do is to move $19$ paddles. You make some starting guess (you need to move $4$ paddles for this),
and then there are $15$ more possible guesses. (The total number of possibilities is $2^4 = 16$.) We will achieve this, and more.

\begin{theorem} Suppose you play a game of Bonkers with $n$ digits ($n = 1, 2, 3, 4, ...$), and the $n$ paddles are arranged in any
guess. Then it is possible to cycle through all remaining $2^n - 1$ guesses by moving only one paddle at a time -- so $2^n - 1$ paddle moves.
\end{theorem}

We are interested in the case $n = 4$ -- but it is actually {\bf {\itshape easier}} to prove this for {\bf all $n$ at the same time!} The algorithm is beautifully 
simple:

\begin{itemize}
\item Step 1. Go through all possibilities for the first $n - 1$ paddles. This requires $2^{n - 1} - 1$ moves.
\item Step 2. Move the last paddle. This requires $1$ move.
\item Step 3. Again go through all possibilities for the first $n - 1$ paddles. This requires $2^{n - 1} - 1$ moves.
\end{itemize}
The total number of moves required is
\[
(2^{n - 1} - 1) + 1 + (2^{n - 1} - 1) = 2 \cdot 2^{n - 1} - 1 + 1 - 1 = 2^n - 1.
\]
So in other words, {\bf if} we can solve Bonkers with one paddle, we can solve it with two. {\bf If} we can solve Bonkers with two paddles,
we can solve it with three. {\bf If} we can solve Bonkers with three paddles,
we can solve it with four. And so on, forever. The solution with one paddle is trivial ($2^1 - 1 = 1$, and we simply move the paddle
from one slot to the other), but this is the building block that sets off a chain reaction, allowing us to solve Bonkers for any number of paddles.
So if the price was twenty digits, we could win the game within 1,048,595 moves -- twenty to fix the paddles initially, and
$2^20 - 1 = 1048575$ to iterate through the remaining guesses.

This process of reasoning is known as {\bf induction} by mathematicians, and {\bf recursion} by computer programmers. In each case it is an extraordinarily
powerful tool.

So let's see how it works in practice:
\begin{itemize}
\item Bonkers with one paddle: Move the following paddles in order: $1$. If our starting position is T (T for top and B for bottom), then this results
in the following sequence of positions: T, B.
\item Bonkers with two paddles: Move the following paddles in order: $1, 2, 1$. If our starting position is TT,  
then this results in the following sequence of positions: TT, BT, BB, TB.
\item Bonkers with three paddles: Move the following paddles in order: $1, 2, 1, 3, 1, 2, 1$. If our starting position is TTT,  
then this results in the following sequence of positions: TTT, BTT, BBT, TBT, TBB, BBB, BTB, TTB.
\item Bonkers with four paddles: Move the following paddles in order: $1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1$. If our starting position is TTT,  
then this results in the following sequence of positions: TTTT, BTTT, BBTT, TBTT, TBBT, BBBT, BTBT, TTBT, 
TTBB, BTBB, BBBB, TBBB, TBTB, BBTB, BTTB, TTTB.
\end{itemize}
We can see the recursive structure of our solutions more clearly in the instructions than in the resulting sequence of paddles. For example,
if we write (sequence for 3) for $1, 2, 1, 3, 1, 2, 1$, then the last sequence for $4$ is (sequence for 3), 4, (sequence for 3). Similarly if we
call that whole thing (sequence for 4), the sequence for 5 is (sequence for 4), 5, (sequence for 4).

The same pattern can be seen in the tick lengths on many rulers!

\medskip
These sequences of T's and B's are known as {\bf binary Gray codes} and have applications in electrical engineering.

\section{Project Ideas}
Part of the course requirements is a term project: study a game or game show in depth, write a paper analyzing it, and give a presentation in class.

Here are some ideas. Of course, feel free to come up with your own.
\begin{itemize}
\item {\bf Deal or No Deal}: This is an easy to understand game from the contestants' point of view. What about the producers? How does the bank determine its
offers? 

Watch a bunch of episodes of the show and write down what happens. Attempt to determine some sort of formula that predicts what the bank will offer.

\item {\bf Press Your Luck}: One interesting project would be to investigate the patterns behind the show, just as Michael Larson did. Watch old YouTube
videos, and hit freeze frame a lot! Try to describe the patterns, and see if you could win \$100,000 too.

\item {\bf Switcheroo}: Here is a fascinating, and deep Price Is Right game:
\urlc{https://www.youtube.com/watch?v=nvSMVAuGpAE}
Try to figure out the optimal strategy. You will have to assume that the contestant has {\itshape some} idea how much the small prizes
cost, but very imperfect information.

\item {\bf Race Game}: A somewhat easier Price is Right game. Here is a clip:
\urlc{https://www.youtube.com/watch?v=CkqZkqeNyKU}
You might try to figure out the best strategy, assuming the contestant has no idea how much the prizes cost. 

This is somewhat similar to the game {\itshape Mastermind} (see the Wikipedia page). But don't neglect the fact that some
prizes are closer to the lever than others!

\item {\bf Poker}: If you enjoyed the poker discussion, you might want to dig deeper. I recommend
reading at least the first of Harrington's books, watching some poker tournaments online, and then trying to analyze
what happened.

\end{itemize}
%\section{Index of Game Descriptions and Video Links}

%(Just started -- to include everyrhing.)

%\showraes

%\section{Game Descriptions, Video Links, and Important Principles}
%{\repeat@at@end}

\end{document}
\begin{thebibliography}{10000}

%\bibitem{baker} A. Baker,
%\emph{Linear forms in the logarithms of algebraic numbers. I},
%Mathematika \textbf{13} (1966), 204--216.

\end{document}